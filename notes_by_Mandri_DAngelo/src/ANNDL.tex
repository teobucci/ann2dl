%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside,italian]{book}
\usepackage[LGR,T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{color}
\usepackage{float}
\usepackage{calc}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\DeclareRobustCommand{\greektext}{%
  \fontencoding{LGR}\selectfont\def\encodingdefault{LGR}}
\DeclareRobustCommand{\textgreek}[1]{\leavevmode{\greektext #1}}
\ProvideTextCommand{\~}{LGR}[1]{\char126#1}

%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}

\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}[chapter]
\providecommand{\algorithmname}{Algoritmo}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{algorithm,algpseudocode}

\usepackage[]{hyperref}

\makeatother

\usepackage{babel}
\usepackage{listings}
\renewcommand{\lstlistingname}{Listato}

\begin{document}
\begin{titlepage}    
	\begin{center}
       
		\vspace*{1cm}
		\Huge
        \textbf{Articial Neural Networks and Deep Learning}
                          
		
		\vspace{1.5cm}
		
		\Large
        
		\textbf{Antonino Elia Mandri \\ Stefano D'Angelo}
		
                           		
		
		\vspace{1.5cm}      
       		
		\includegraphics[width=0.8\textwidth]{polimi.png}
		\vspace{1.5cm}
                          
		Anno Accademico 2020/2021                
	\end{center} 
\end{titlepage}

\tableofcontents{}

\chapter{Introduzione}

\section{Apprendimento automatico}

Il \textbf{machine learning} è una branca dell'intelligenza artificiale
che raccoglie un insieme di metodi quali: statistica computazionale,
riconoscimento di pattern, reti neurali artificiali, filtraggio adattivo,
teoria dei sistemi dinamici, elaborazione delle immagini, data mining,
algoritmi adattivi, ecc; che utilizza metodi statistici per migliorare
progressivamente la performance di un algoritmo nell'identificare
pattern nei dati. Nell'ambito dell'informatica, l'apprendimento automatico
è una variante alla programmazione tradizionale nella quale si predispone
in una macchina l'abilità di apprendere qualcosa dai dati in maniera
autonoma, senza ricevere istruzioni esplicite a riguardo. 

\noindent Immaginiamo di possedere un insieme di una certa esperienza
E, per esempio dei dati, che chiameremo $D=x_{1},x_{2},...,x_{N}$,
definiamo quindi i seguenti paradigmi di apprendimento:
\begin{itemize}
\item apprendimento supervisionato (\textbf{supervised learning}): in cui
al modello vengono forniti degli output desiderati $t_{1},t_{2},...,t_{N}$
e l'obiettivo è quello di estrarre una regola generale che associ
l'input $D$ all'output corretto;
\item apprendimento non supervisionato (\textbf{unsupervised learning}):
è una tecnica di apprendimento automatico che consiste nel fornire
al sistema informatico una serie di input (esperienza del sistema),
$D$ nel nostro caso, che egli riclassificherà ed organizzerà sulla
base di caratteristiche comuni per cercare di effettuare ragionamenti
e previsioni sugli input successivi; 
\item L'apprendimento per rinforzo (\textbf{reinforcement learning}): è
una tecnica di apprendimento automatico che punta ad attuare sistemi
in grado di apprendere ed adattarsi alle mutazioni dell'ambiente in
cui sono immersi, attraverso la distribuzione di una \textquotedbl ricompensa\textquotedbl{}
detta rinforzo che consiste nella valutazione delle loro prestazioni.
Il modello produce una serie di azioni $a_{1},a_{2},...,a_{N}$ che
interagiscono con l'ambiente e ricevendo una serie di ricompense $r_{1},r_{2},...,r_{N}$
impara a produrre azioni che massimizzino le ricompense nel lungo
periodo.
\end{itemize}
Una definizione di cosa sia il machine learning fu data da Mitchell
nel 97: 

\textquotedbl\textit{A computer program is said to learn from experience
E with respect to some class of task T and a performance measure P,
if its performance at tasks in T, as measured by P, improves because
of experience E}\textquotedbl{}

\section{Deep learning}

Senza addentrarci nel complesso e variegato mondo del machine learning,
sappiamo in linea generale che si tratta di algoritmi che fanno largo
uso della statistica. Funzionano bene su un'ampia varietà di problemi.
Tuttavia, tali algortimi, non sono riusciti a risolvere i problemi
centrali dell'IA, come il riconoscimento del linguaggio o il riconoscimento
di oggetti e altri ancora. Ciò avviene sonstanzialmente a causa dell'alta
dimensionalità dei dati da trattare. I meccanismi usati dal machine
learning per generalizzare risultano insufficienti per apprendere
complesse funzioni multidimensionali. Il \textbf{Deep Learning}, la
cui traduzione letterale significa apprendimento profondo, è una sottocategoria
del Machine Learning e indica quella branca dell\textquoteright Intelligenza
Artificiale che fa riferimento agli algoritmi ispirati alla struttura
e alla funzione del cervello chiamate \textbf{reti neurali artificiali}.
Le architetture di Deep Learning (reti neurali artificiali) sono per
esempio state applicate nella computer vision, nel riconoscimento
automatico della lingua parlata, nell\textquoteright elaborazione
del linguaggio naturale, nel riconoscimento audio e nella bioinformatica.
Potremmo definire il Deep Learning come un sistema che sfrutta una
classe di algoritmi di apprendimento automatico che: 
\begin{itemize}
\item usano vari livelli di unità non lineari a cascata per svolgere compiti
di \textit{estrazione di caratteristiche e di trasformazione}. Ciascun
livello successivo utilizza l\textquoteright uscita del livello precedente
come input. Gli algoritmi possono essere sia di tipo supervisionato
sia non supervisionato e le applicazioni includono l\textquoteright analisi
di pattern (apprendimento non supervisionato) e classificazione (apprendimento
supervisionato);
\item apprendono multipli livelli di rappresentazione che corrispondono
a differenti livelli di astrazione; questi livelli formano una gerarchia
di concetti.
\end{itemize}
La differenza principale consiste in come vengono estratte le caretteristiche
utili alla risoluzione del problema. Tali caratteristiche vengono
estratte manualmente nel machine learning , mentre vengono estratte
autonomamente negli algoritmi di deep learning.

\begin{figure}[H]
\begin{centering}
\includegraphics{ml_vs_dl}
\par\end{centering}
\caption{Differenze tra machine learning e deep learning. I riquadri evidenziati
indicano le parti apprese dai dati}

\end{figure}


\section{Percettrone}

Nell'apprendimento automatico, il \textbf{percettrone} è un tipo di
classificatore binario che mappa i suoi \textbf{ingressi} $\boldsymbol{x}$
(un vettore di tipo reale) in un valore di output $\boldsymbol{f\left(x\right)}$
(uno scalare di tipo reale) calcolato con 
\begin{equation}
\boldsymbol{f(x)}=\chi(\left\langle \boldsymbol{w},\boldsymbol{x}\right\rangle +b)
\end{equation}
dove $\boldsymbol{w}$ è un vettore di \textbf{pesi} con valori reali,
l'operatore $\left\langle \cdot,\cdot\right\rangle $ è il prodotto
scalare (che calcola una somma pesata degli input), \textit{b}\textbf{
}è il \textbf{bias}, un termine costante che non dipende da alcun
valore in input e $\chi(y)$ è la funzione di output. Le scelte più
comuni per la funzione $\chi(y)$ sono:
\begin{enumerate}
\item $\chi(y)=sign(y)$
\item $\chi(y)=y\Theta(y)$
\item $\chi(y)=y$
\end{enumerate}
dove $\Theta(y)$ è la \textbf{funzione di Heaviside}.

\begin{figure}[h]
\begin{centering}
\includegraphics[scale=0.85]{percettrone}
\par\end{centering}
\caption{percettrone}
\end{figure}

\begin{equation}
h_{j}\left(\boldsymbol{x}|\boldsymbol{w},b\right)=h_{j}\left(\sum_{i=1}^{I}w_{i}\cdot x_{i}-b\right)=h_{j}\left(\sum_{i=0}^{I}w_{i}\cdot x_{i}\right)=h_{j}\left(\boldsymbol{w}^{T}\boldsymbol{x}\right)
\end{equation}
Non tutti i problemi di classificazione sono affrontabili con strumenti
lineari come il percettrone. Sorgono spontanee alcune domande, come
inizializziamo e modifichiamo il vettore di pesi $\boldsymbol{w}$
del percettrone? Quale funzione di attivazione scegliamo? 

\subsection{Apprendimento Hebbiano}
\begin{quotation}
<<\textit{The strength of a synapse increases according to the simultaneous
activation of the relative input and the desired target}>> (Donald
Hebb, The Organization of Behavior, 1949). 
\end{quotation}
La \textbf{regola di Hebb} è la seguente: l'efficacia di una particolare
sinapsi cambia se e solo se c'è un'intensa attività simultanea dei
due neuroni, con un'alta trasmissione di input nella sinapsi in questione.
L'\textbf{apprendimento Hebbiano} può essere riassunto come segue:
\begin{equation}
\begin{cases}
w_{i}^{k+1}=w_{i}^{k}+\Delta w_{i}^{k}\\
\Delta w_{i}^{k}=\eta\cdot x_{i}^{k}\cdot t^{k}
\end{cases}
\end{equation}

dove:
\begin{itemize}
\item $\eta$: rateo di apprendimento;
\item $x_{i}^{k}$: l'i-esimo input al tempo $k$;
\item $t^{k}$: l'output desiderato al tempo $k$.
\end{itemize}
L'inizializzazione dei pesi parte con dei valori casuali. La soluzione
può non esistere e se esiste non essere unica, ma tutte ugualmente
corrette. Questo algoritmo può non convergere alla soluzione per due
motivi:
\begin{enumerate}
\item La soluzione non esiste;
\item $\eta$ è troppo grande, continuiamo a modificare i pesi con passo
elevato, viceversa un valore di $\eta$ troppo piccolo aumenta sensibilmente
il tempo di convergenza.\marginpar{Cercare esempio}
\end{enumerate}

\subsection{Feed Forward Neural Networks}

Una \textbf{rete neurale feed-forward} (\textquotedbl rete neurale
con flusso in avanti\textquotedbl ) o rete feed-forward è una rete
neurale artificiale dove le connessioni tra le unità non formano cicli,
differenziandosi dalle reti neurali ricorrenti. Questo tipo di rete
neurale fu la prima e più semplice tra quelle messe a punto. In questa
rete neurale le informazioni si muovono solo in una direzione, avanti,
rispetto a nodi d'ingresso, attraverso nodi nascosti (se esistenti)
fino ai nodi d'uscita. Nella rete non ci sono cicli. Le reti feed-forward
non hanno memoria di input avvenuti in tempi precedenti, per cui l'output
è determinato solamente dall'attuale input. 

\subsubsection{Percettrone a singolo strato}

La più semplice rete feed-forward è il \textbf{percettrone a singolo
strato} (SLP dall'inglese \textbf{single layer perceptron}), utilizzato
verso la fine degli anni '60. Un SLP è costituito da uno strato in
ingresso, seguito direttamente dall'uscita. Ogni unità di ingresso
è collegata ad ogni unità di uscita. In pratica questo tipo di rete
neurale ha un solo strato che effettua l'elaborazione dei dati, e
non presenta nodi nascosti, da cui il nome. Gli SLP sono molto limitati
a causa del piccolo numero di connessioni e dell'assenza di gerarchia
nelle caratteristiche che la rete può estrarre dai dati (questo significa
che è capace di combinare i dati in ingresso una sola volta). Famosa
fu la dimostrazione, che un SLP non è in grado di rappresentare la
funzione XOR.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{XOR_percettrone}
\par\end{centering}
\caption{Problema XOR percettrone}
\end{figure}


\subsubsection{Percettrone multistrato}

Il \textbf{Percettrone multistrato} (in acronimo MLP dall'inglese
Multilayer perceptron) è un modello di rete neurale artificiale che
mappa insiemi di dati in ingresso in un insieme di dati in uscita
appropriati. È fatta di strati multipli di nodi in un grafo diretto,
con ogni strato completamente connesso al successivo. Eccetto che
per i nodi in ingresso, ogni nodo è un neurone (elemento elaborante)
con una funzione di attivazione non lineare. Il Percettrone multistrato
usa una tecnica di apprendimento supervisionato chiamata backpropagation
per l'allenamento della rete. La MLP è una modifica del Percettrone
lineare standard e può distinguere i dati che non sono separabili
linearmente.

Prima di addentrasi in metodologie di progettazioni delle reti neurali
è utile introdurre alcuni concetti.

\subsection{Funzioni di attivazione}

Vediamo brevemente diversi tipi di funzioni di attivazione:

\textbf{ReLU. }The Rectified Linear Unit è una funzione di attivazione
definita come la parte positiva del suo argomento:\textbf{ 
\begin{equation}
f(x)=x^{+}=ReLU(x)=max(0,x)
\end{equation}
}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{Grafico_relu}
\par\end{centering}
\caption{grafico ReLU}

\end{figure}

dove $x$ è l'input a un neurone.\textbf{ }

\textbf{Sigmoid. }La funzione sigmoidea è una funzione matematica
che produce una curva sigmoide; una curva avente un andamento ad \textquotedbl S\textquotedbl .
Spesso, la funzione sigmoide si riferisce ad uno speciale caso di
funzione logistica mostrata a destra e definita dalla formula:
\begin{equation}
f(x)=\frac{1}{1+e^{-x}}
\end{equation}

Generalmente, una funzione sigmoidea è una funzione continua e derivabile,
che ha una derivata prima non negativa e dotata di un minimo locale
ed un massimo locale. Le funzioni sigmoidee sono spesso usate nelle
reti neurali per introdurre la non linearità nel modello e/o per assicurarsi
che determinati segnali rimangano all'interno di specifici intervalli.
Un motivo per la relativa popolarità nelle reti neurali è perché la
funzione sigmoidea soddisfa questa proprietà:
\begin{equation}
\frac{d}{dx}sig(x)=sig(x)(1-sig(x))
\end{equation}
Questa relazione polinomiale semplice fra la derivata e la funzione
stessa è, dal punto di vista informatico, semplice da implementare.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{grafico_sig}
\par\end{centering}
\caption{grafico sigmoide}
\end{figure}

\textbf{Tangente iperbolica: }
\begin{equation}
tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
\end{equation}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{tangente-iperbolica}
\par\end{centering}
\caption{grafico tanh}

\end{figure}
Queste e altre funzioni verranno esaminate più attentamente dopo aver
introdotto tecniche di ottimizzazione delle reti neurali. 

\paragraph{Softmax }

In matematica, una funzione softmax, o funzione esponenziale normalizzata,
è una generalizzazione di una \textbf{funzione logistica} che comprime
un vettore $\boldsymbol{z}\in\mathbb{R}^{k}$ di valori reali arbitrari
in un vettore ${\displaystyle \sigma(\mathbf{z})}$ di valori reali
compresi in un intervallo ${\displaystyle (0,1)}$ la cui somma è
${\displaystyle 1}$. La funzione è data da:

\[
{\displaystyle \sigma:\mathbb{R}^{K}\to\left\{ \boldsymbol{z}\in\mathbb{R}^{K}|z_{i}>0,\sum_{i=1}^{K}z_{i}=1\right\} }
\]

\[
{\displaystyle \sigma(\mathbf{z})_{j}={\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}}}
\]


\chapter{Reti Neurali}

L'utilità dei modelli di rete neurale sta nel fatto che queste possono
essere usate per comprendere una funzione utilizzando solo le osservazioni
sui dati. Ciò è particolarmente utile nelle applicazioni in cui la
complessità dei dati o la difficoltà di elaborazione rende la progettazione
di una tale funzione impraticabile con i normali procedimenti di analisi
manuale.

I compiti in cui le reti neurali sono applicate possono essere classificate
nelle seguenti categorie:
\begin{itemize}
\item funzioni di approssimazione, o di \textbf{regressione}, tra cui la
previsione di serie temporali e la modellazione;
\item \textbf{classificazione}, compresa la struttura e la sequenza di generici
riconoscimenti, l'individuazione delle novità ed il processo decisionale;
\item l'elaborazione dei dati, compreso il \textquotedbl\textbf{filtraggio}\textquotedbl{}
(eliminazione del rumore), il clustering, separazione di segnali e
compressione.
\end{itemize}
Le aree di applicazione includono i sistemi di controllo (controllo
di veicoli, controllo di processi), simulatori di giochi e processi
decisionali (backgammon, scacchi), riconoscimento di pattern (sistemi
radar, identificazione di volti, riconoscimento di oggetti, ecc),
riconoscimenti di sequenze (riconoscimento di gesti, riconoscimento
vocale, OCR), diagnosi medica, applicazioni finanziarie, data mining,
filtri spam per e-mail.

\section*{Pregi}

Le reti neurali per come sono costruite lavorano in parallelo e sono
quindi in grado di trattare molti dati. Si tratta in sostanza di un
sofisticato sistema di tipo statistico dotato di una buona immunità
al rumore; se alcune unità del sistema dovessero funzionare male,
la rete nel suo complesso avrebbe delle riduzioni di prestazioni ma
difficilmente andrebbe incontro ad un blocco del sistema. I software
di ultima generazione dedicati alle reti neurali richiedono comunque
buone conoscenze statistiche; il grado di apparente utilizzabilità
immediata non deve trarre in inganno, pur permettendo all'utente di
effettuare subito previsioni o classificazioni, seppure con i limiti
del caso. Da un punto di vista industriale, risultano efficaci quando
si dispone di dati storici che possono essere trattati con gli algoritmi
neurali. Ciò è di interesse per la produzione perché permette di estrarre
dati e modelli senza effettuare ulteriori prove e sperimentazioni.

\section*{Difetti}

I modelli prodotti dalle reti neurali, anche se molto efficienti,
non sono spiegabili in linguaggio simbolico umano: i risultati vanno
accettati \textquotedbl così come sono\textquotedbl , da cui anche
la definizione inglese delle reti neurali come \textquotedbl black
box\textquotedbl : in altre parole, a differenza di un sistema algoritmico,
dove si può esaminare passo-passo il percorso che dall'input genera
l'output, una rete neurale è in grado di generare un risultato valido,
o comunque con una alta probabilità di essere accettabile, ma non
è possibile spiegare come e perché tale risultato sia stato generato.
Come per qualsiasi algoritmo di modellazione, anche le reti neurali
sono efficienti solo se le variabili predittive sono scelte con cura.

Non sono in grado di trattare in modo efficiente variabili di tipo
categorico (per esempio, il nome della città) con molti valori diversi.
Necessitano di una fase di addestramento del sistema che fissi i pesi
dei singoli neuroni e questa fase può richiedere molto tempo, se il
numero dei record e delle variabili analizzate è molto grande. Non
esistono teoremi o modelli che permettano di definire la rete ottima,
quindi la riuscita di una rete dipende molto dall'esperienza del creatore.

\section{Teorema di approssimazione universale}

Nella teoria matematica delle reti neurali artificiali, il \textbf{teorema
di approssimazione universale} afferma che una feed-forward network
con un singolo strato nascosto e contenente un numero finito di neuroni
può approssimare una qualsiasi funzione misurabile secondo Lebesgue,
con qualsiasi grado di accuratezza, su un sotto insieme compatto di
$\mathbb{R}^{n}$ sotto deboli ipotesi sulla funzione di attivazione
dei neuroni. Il teorema non dice nulla su algoritmi di apprendimento
da utilizzare. Sebbene una rete feed-forward con un singolo strato
nascosto sia un approssimatore universale, l'ampiezza di queste reti
deve essere esponenzialmente grande. Nel 2017 Lu et al. \cite{key-1}
dimostrarono una variante del teorema per reti feed-forward con ampiezza
limitata. In particolare provarono che una rete di ampiezza $n+4$
con funzione di attivazione ReLU può approssimare una qualsiasi funzione
integrabile secondo Lebesgue definita su uno spazio $n-dimensionale$
rispetto alla norma $L_{1}$ se è permesso alla rete di crescere in
profondità (quindi non più a singolo strato nascosto). Provarono anche
la limitata potenza espressiva se l'ampiezza della rete è minore o
uguale a $n$. Nessuna funzione integrabile secondo Lebesgue ad eccezione
di quelle definite su insiemi a misura nulla può essere approssimata
da una rete con ampiezza $n$ e funzione di attivazione ReLU. La formulazione
originale del teorema non fa assunzioni che la funzione di attivazione
sia ReLU ma solo che sia continua, limitata e non costante. I due
teoremi sono formalmente enunciati nel modo seguente:

\paragraph*{Ampiezza illimitata:}

Sia $\varphi:\mathbb{R}\rightarrow\mathbb{R}$ una funzione continua,
limitata e non costante (chiamata \textit{funzione di attivazione}).
Sia $I_{m}$ l'ipercubo unitario $[0,1]^{m}$. Sia $C(I_{m})$ lo
spazio delle funzioni a valori reali definite su $I_{m}$. Dato un
$\varepsilon>0$ e una qualsiasi funzione $f\in C(I_{m})$, esiste
allora un intero $N\in\mathbb{N}$, costanti reali $v_{i},b_{i}\in\mathbb{R}$
e vettori reali $\boldsymbol{w}_{i}\in\mathbb{R}^{m}$ per $i=1,...,N$
tale che possiamo definire:
\begin{equation}
F(\boldsymbol{x})=\sum_{i=1}^{N}v_{i}\varphi(\boldsymbol{w}_{i}^{T}\boldsymbol{x}+b_{i})
\end{equation}
come realizzazione approssimativa della funzione $f$ per cui vale:
\begin{equation}
|F(\boldsymbol{x})-f(\boldsymbol{x})|<\varepsilon
\end{equation}
per ogni $x\in\mathbb{R}^{m}.$ In altre parole, funzioni della forma
$F(\boldsymbol{x})$ sono dense in $C(I_{m})$. Questo vale ancora
se si sostituisce a $I_{m}$ un qualsiasi sotto insieme compatto di
$\mathbb{R}^{m}.$

\paragraph{Ampiezza limitata: }

Per ogni funzione integrabile secondo Lebesgue $f:\mathbb{\mathbb{R}}^{n}\rightarrow\mathbb{R}$
e ogni $\varepsilon>0$, esiste una rete fully-connected ReLU $A$
con ampiezza $d_{m}\leq n+4$ tale che la funzione $F_{A}$ rappresentata
da tale rete soddisfa:
\begin{equation}
\int_{\mathbb{R}^{n}}|F_{A}(\boldsymbol{x})-f(\boldsymbol{x})|d\boldsymbol{x}<\varepsilon.
\end{equation}


\section{Regressione}

L'analisi della \textbf{regressione} è una tecnica usata per analizzare
una serie di dati che consistono in una variabile dipendente e una
o più variabili indipendenti. Lo scopo è stimare un'eventuale relazione
funzionale esistente tra la variabile dipendente e le variabili indipendenti.
La variabile dipendente nell'equazione di regressione è una funzione
delle variabili indipendenti più un termine d'errore. Quest'ultimo
è una variabile casuale e rappresenta una variazione non controllabile
e imprevedibile nella variabile dipendente. I parametri sono stimati
in modo da descrivere al meglio i dati. Il metodo più comunemente
utilizzato per ottenere le migliori stime è il metodo dei \textquotedbl\textbf{minimi
quadrati}\textquotedbl{} (OLS), ma sono utilizzati anche altri metodi.

\subsection{Variabili casuali}

In matematica, e in particolare nella teoria della probabilità, una
\textbf{variabile casuale} (detta anche \textbf{variabile aleatoria}
o \textbf{variabile stocastica}) è una variabile che può assumere
valori diversi in dipendenza da qualche fenomeno aleatorio. Le variabili
casuali sono per noi molto importanti perchè l'obiettivo di una rete
neurale per regressione è quello di approssimare una funzione target
$t$ incognita avendo a disposizioni $N$ osservazioni (coppie input-output
della funzione $t$). Anche per la classificazioni di immagini è possibile
ricondursi a variabili aleatorie.

\section{Classificazione}

La \textbf{classificazione statistica} è quell'attività che si serve
di un algoritmo statistico al fine di individuare una rappresentazione
di alcune caratteristiche di un'entità da classificare (oggetto o
nozione), associandole una etichetta classificatoria. La classificazione
si divide in due tipologie:
\begin{enumerate}
\item \textbf{Classificazione in due classi} $\{\Omega_{0},\varOmega_{1}\}$
in cui generalmente si utilizza come funzione di attivazione nel layer
di output (singolo neurone):
\begin{enumerate}
\item $\tanh$ codificando le classi nel modo seguente $\{\Omega_{0}=-1,\varOmega_{1}=1\}$.
Ovviamente il codomino della funzione tanh varia con continuità nell'intervallo
(-1,1), quindi si prende come valore quello meno distante.
\item Sigmoid codificando le classi nel modo seguente $\{\Omega_{0}=0,\varOmega_{1}=1\}$.
Analoga considerazione a riguardo del codominio fatta per tanh.
\end{enumerate}
\item \textbf{Classificazione in più di due classi} $\{\Omega_{0},\varOmega_{1},...,\varOmega_{n}\}$,
il layer di output ha tanti neuroni quante sono le classi. Un possibile
approcio è codificare le classi usando la codifica onehot dove la
classe i-esima viene rappresentata da un vettore di dimensione $n$
in cui ogni componente è 0 ad esclusione dell'i-esimo elemento che
vale 1. I neuroni dello strato di output useranno softmax come funzione
di attivazione.
\end{enumerate}

\section{Maximum Likelihood Estimation (stimatore di massima verosimiglianza)}

Supponiamo di avere $N$ campioni di una variabile aleatoria di cui
conosciamo la distribuzione di probabilità ma non conosciamo tutti
i parametri di tale distribuzione, \textbf{MLE} si pone l'obiettivo
di trovare i parametri tali per cui è massima la probabilità che gli
$N$ campioni appartengano alla distribuzione di probabilità con i
parametri così trovati. Dato $\boldsymbol{\theta}=(\theta_{1},\theta_{2},...,\theta_{p})^{T}$
un vettore di parametri, cerchiamo $\boldsymbol{\theta}_{MLE}:$
\begin{itemize}
\item Scriviamo la probabilità condizionata $L=P(\boldsymbol{x}|\boldsymbol{\theta})$
con $\boldsymbol{x}$ vettore di input;
\item opzionalmente, se agevola i calcoli, calcoliamo $l=\log(P(\boldsymbol{x}|\boldsymbol{\theta}))$;
\item cerchiamo il massimo rispetto a $\boldsymbol{\theta}$ con gli strumenti
dell'analisi matematica:
\begin{itemize}
\item $\nabla_{\boldsymbol{\theta}}(L)$ o $\nabla_{\boldsymbol{\theta}}(l)=0$
\item Controlliamo che il valore di $\boldsymbol{\theta}_{MLE}$ sia un
massimo (tramite hessiana o altro)
\end{itemize}
\end{itemize}
Questa è la risoluzione analitica, non sempre possibile o conveniente.
In ogni caso cerchiamo quel valore di $\boldsymbol{\theta}$ che massimizza
la probabilità, quindi in generale si può affrontare il problema con
altri metodi quali:
\begin{itemize}
\item tecniche di ottimizzazione: per esempio moltiplicatori di Lagrange;
\item tecniche numeriche: per esempio la discesa del gradiente, approfondita
largamente in seguito;
\end{itemize}
Vediamo un esempio classico della probabilità:

supponiamo di avere $N$ indipendenti e identicamente distribuiti
(i.i.d.) campioni di numeri reali provenienti da una distribuzione
gaussiana di varianza $\sigma^{2}$ nota e media $\mu$ incognita:

\begin{equation}
\boldsymbol{x}=x_{1},x_{2},...,x_{n}\sim N(\mu,\sigma^{2})
\end{equation}
 con 
\begin{equation}
N(\mu,\sigma^{2})=p(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
\end{equation}
calcoliamo la likelihood $L=P(\boldsymbol{x}|\boldsymbol{\theta})$:
\begin{equation}
L=p(x_{1},x_{2},...,x_{n}|\mu,\sigma^{2})=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}}
\end{equation}
passiamo al logaritmo:
\begin{equation}
l=\log(\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}})=\sum_{i=1}^{N}\log(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}})
\end{equation}

\begin{equation}
=N\cdot\log(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)^{2}
\end{equation}
deriviamo rispetto a $\mu$ che è il parametro che vogliamo stimare:
\begin{equation}
\frac{\partial l(\boldsymbol{x}|\mu)}{\partial\mu}=\frac{\partial}{\partial\mu}(N\cdot\log(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)^{2})
\end{equation}

\begin{equation}
=\frac{1}{\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)
\end{equation}
ora uguagliamo a zero la derivata parziale:
\begin{equation}
\frac{1}{\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)=0
\end{equation}

\begin{equation}
\sum_{i=1}^{N}(x_{i}-\mu)=0
\end{equation}

\begin{equation}
\sum_{i=1}^{N}x_{i}=\sum_{i=1}^{N}\mu
\end{equation}
\begin{equation}
\mu_{MLE}=\frac{1}{N}\sum_{i=1}^{N}x_{i}.
\end{equation}
Vediamo un esempio pratico di come applicare lo stimatore di massima
verosomiglianza con una generica rete neurale per la regressione.

\subsection{MLE per regressione}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{neural_network_for_regression}
\par\end{centering}
\caption{Rete neurale per regressione}
\end{figure}
Il problema della regressione può essere esposto in maniera semplice
e intuitiva, sacrificando la formalità, come dati una serie di punti
(coppie input-output della funione incognita) campionati sperimentalmente
cerchiamo una qualche funzione che a parità di input fornisca l'output
più \textquotedbl vicino\textquotedbl{} possibile. Osserviamo che non
cerchiamo una funzione che passi esattamente dai punti campionati
(quello è il compito dell'interpolazione) ma che minimizzi la differenza
con i tutti i campioni. Nell'esempio in fig. 2.1 abbiamo una funzione
incognita $t_{n}$ che vogliamo stimare tramite la funzione calcolata
dalla rete neurale $g(x_{n}|w).$ Poichè dobbiamo tenere conto che
la nostra approssimazione presenterà degli errori e non avendo alcuna
informazione aggiuntiva sulla funzione $t_{n}$ modelliziamo questo
errore come un rumore aggiunto alla funzione $g(x_{n}|w).$ Per semplicità
dei calcoli ipotiziamo che il rumore abbia varianza nota.

\begin{equation}
t_{n}=g(x_{n}|w)+\epsilon_{n}
\end{equation}
\begin{equation}
\epsilon_{n}\sim N(0,\sigma^{2})
\end{equation}
è allora intuitivo osservare che in ogni punto $t_{n}$ ha distribuzione
di probabilità gaussiana di media $g(x_{n}|w)$ e varianza $\sigma^{2}$
\begin{equation}
\Rightarrow t_{n}\sim N(g(x_{n}|w),\sigma^{2}).
\end{equation}
Come nell'esempio teorico dobbiamo stimare la media di una distribuzione
normale.
\begin{equation}
p(t|g(x|w),\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t-g(x|w))^{2}}{2\sigma^{2}}}
\end{equation}
scriviamo la likelihood per l'insieme di osservazioni $L(w)=p(\boldsymbol{t}|g(\boldsymbol{x}|w),\sigma^{2})$
\begin{equation}
L(w)=p(t_{1},t_{2},...,t_{N}|g(x|w),\sigma^{2})=\prod_{i=1}^{N}p(t_{i}|g(x_{i}|w),\sigma^{2})
\end{equation}
\begin{equation}
=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}}
\end{equation}
cerchiamo i pesi $w$ che massimizzano la likelihood $L(w)$
\begin{equation}
argmax_{w}L(w)=argmax_{w}(\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}})
\end{equation}
\begin{equation}
=argmax_{w}(\sum_{i=1}^{N}\log(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}})
\end{equation}
\begin{equation}
=argmax_{w}(N\cdot\log(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2})
\end{equation}
\begin{equation}
=argmin_{w}(\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2}).
\end{equation}
Siamo giunti quindi a trovare tramite lo stimatore di massima verosomiglianza
la \textbf{funzione di errore} da minimizzare che useremo per allenare
la nostra rete e quindi trovare i pesi $w$ ottimali per la regressione.
Infatti $\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2}$ è definita SSE (\textit{Sum
of Squared Errors}) ed è alla base della tecnica di ottimizzazione
e regressione nota come \textbf{metodo dei minimi quadrati. }In maniera
analoga è possibile procedere anche nel caso la varianza sia incognita.\textbf{}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{sse}
\par\end{centering}
\textbf{\caption{SSE}
}
\end{figure}
\textbf{}

\subsection{MLE per classificazione binaria}

Usiamo MLE per determinare la \textbf{loss function} adatta al problema
di classificazione binaria, la rete mostrata in figura utilizza la
funzione sigmoide nell'ultimo neurone di conseguenza l'output varierà
con continuità tra 0 e 1.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{NN_classification}
\par\end{centering}
\caption{Rete neurale per classificazione binaria}
\end{figure}
Adottiamo ancora un approcio probabilistico. Idealmente, in un problema
di classificazione binaria, noi vorremmo che ad ogni input $x_{n}$
corrisponda un solo numero $t_{n}\in\{0,1\}$. Una distribuzione di
probabilità su due soli valori 0 e 1 è la distribuzione di Bernoulli. 

Una variabile aleatoria discreta $X$ ha distribuzione di Bernoulli
${\displaystyle \mathcal{B}(p)}$ di parametro ${\displaystyle p\in[0,1]}$
se e solo se:

\[
P(X=1)=p
\]
\[
P(X=0)=1-p
\]
La conoscenza del parametro $p$ deriva da assunzioni sul fenomeno
aleatorio in esame. Ad esempio nel lancio di una moneta non truccata
sappiamo che $p$ è uguale a 0,5 dato che assumiamo che le leggi fisiche
che governano il moto di una moneta non hanno preferenze tra testa
o croce. Nel nostro caso quello che cerchiamo è una probabilità a
posteriori del valore assunto da $t_{n}$ sapendo l'input $x_{n}.$
L'idea è utilizzare come parametro della distribuzione la conoscenza
acquisita dalla rete analizzando l'input. 
\[
t_{n}\sim B(g(x_{n}|w))
\]
Quello che vogliamo fare è massimizzare il numero di classificazioni
corrette su tutto l'input. I nostri campioni osservati durante il
training sono indipendenti ed identicamente distribuiti:
\[
p(t_{n}|g(x_{n}|w))=g(x_{n}|w)^{t_{n}}(1-g(x_{n}|w))^{1-t_{n}}
\]
Scriviamo la likelihood congiunta:

\[
L(w)=p(t_{1},t_{2},...,t_{n}|g(x|w))=\prod_{n=1}^{N}p(t_{n}|g(x_{n}|w))=
\]
\[
\prod_{n=1}^{N}g(x_{n}|w)^{t_{n}}(1-g(x_{n}|w))^{1-t_{n}}
\]
Massimizzando rispetto a w:
\[
argmax_{w}(\prod_{n=1}^{N}g(x_{n}|w)^{t_{n}}(1-g(x_{n}|w))^{1-t_{n}})
\]
Passiamo al logaritmo

\[
argmax_{w}(\sum_{n=1}^{N}t_{n}log(g(x_{n}|w))+(i-t_{n})log(1-g(x_{n}|w)))
\]
Massimizzare quest'ultima formula è equivalente a minimizzare:
\[
argmin_{w}(-\sum_{n=1}^{N}t_{n}log(g(x_{n}|w))+(i-t_{n})log(1-g(x_{n}|w)))
\]
In questo modo siamo giunti ad avere una funzione di errore da minimizzare
per la classificazione binaria. Tale funzione di errore prende il
nome di Cross-entropy.

\[
E(w)=\sum_{n=1}^{N}t_{n}log(g(x_{n}|w))+(i-t_{n})log(1-g(x_{n}|w))
\]


\section{Ottimizzazione}

Fino ad ora abbiamo solo accennato a come impostare i pesi di una
rete neurale (\textit{hebbian learning}) ma non siamo mai entrati
nello specifico. Ricordiamo brevemente come è composta una rete neurale
e come valutiamo la sua bontà.
\begin{itemize}
\item Una rete è formata da uno o più percettroni, disposti in vari strati
di ampiezza variabile. Per il momento ci limitiamo alle reti Fully
Connected (FC) in cui ogni neurone è collegato a tutti i neuroni dello
strato successivo, ad eccezione ovviamente dello strato di output.
Ogni neurone esegue la somma pesata del proprio vettore di input e
propaga in uscita il valore della funzione di attivazione (ReLU, sigmoide
etc...) applicata alla somma di input.
\item Ogni rete deve essere validata tramite una error function (o loss
function) che permetta di quantificare la bontà della rete e quindi
avere un responso sulla validità dei pesi impostati.
\end{itemize}
Tale \textit{error function} che ricordiamo essere funzione della
rete e quindi dei suoi pesi $w$, deve essere differenziabile (o quasi)
rispetto a $w.$ L'idea generale è quella di sfruttare gli strumenti
del calcolo infinitesimale per trovare il minimo della funzione di
errore. Nel caso la funzione di errore fosse differenziabile e convessa
sappiamo dalla teoria del calcolo che certamente esiste un minimo
assoluto e sappiamo ricavarlo analiticamente. Sfortunatamente nel
caso generale possiamo richiedere alla funzione di errore solo la
continuità e la quasi differenziabilità, inoltre già una rete neurale
di medio bassa complessità presenta al suo interno migliaia o milioni
di pesi che rendono praticamente impossibile trovare una soluzione
analitica. Quello che si riesce agilmente a fare è calcolare numericamente
il \textbf{gradiente} (con un bassissimo tasso di errore) che ricordiamo
fornisce la direzione di massima crescità della funzione. 

\subsection{Discesa del gradiente}

\paragraph{Gradiente}

Nel calcolo differenziale vettoriale, il gradiente di una funzione
a valori reali (ovvero di un campo scalare) è una funzione vettoriale.
Il gradiente di una funzione è spesso definito come il vettore che
ha come componenti le derivate parziali della funzione, anche se questo
vale solo se si utilizzano coordinate cartesiane ortonormali. In generale,
il gradiente di una funzione ${\displaystyle f}$, denotato con ${\displaystyle \nabla f},$
(il simbolo ${\displaystyle \nabla}$ si legge nabla), è definito
in ciascun punto dalla seguente relazione: per un qualunque vettore
$\boldsymbol{v}$, il prodotto scalare $\nabla f\cdot\boldsymbol{v}$
dà il valore della derivata direzionale di $f$ rispetto a $\boldsymbol{v}$
(sse $f$ è differenziabile). Nel caso unidimensionale il gradiente
corrisponde alla derivata della funzione e indica la pendenza, quindi
il tasso di variazione della funzione, e il verso in cui la funzione
cresce (nel caso unidimensionale la direzione del vettore è determinata
e unica, il segno invece determina il verso, positiva la funzione
cresce a destra, negativa la funzione cresce a sinistra). La derivata
è definita formalmente come:
\begin{equation}
\frac{df(x)}{dx}=\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}
\end{equation}
mentre chiamiamo derivata parziale rispetto a $x_{i}$ di una generica
funzione reale $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ derivabile:
\begin{equation}
\frac{\partial f(\boldsymbol{x})}{x_{i}}=\lim_{h\rightarrow0}\frac{f(x_{1},...,x_{i}+h,...,x_{n})}{h}
\end{equation}
con $i\in\{1,...,n\}$. Il gradiente è quindi un vettore le cui componenti
sono tutte le derivate parziali rispetto agli assi di una funzione:
\begin{equation}
\nabla f(\boldsymbol{x})=(\frac{\partial f(\boldsymbol{x})}{x_{1}},\frac{\partial f(\boldsymbol{x})}{x_{2}},...,\frac{\partial f(\boldsymbol{x})}{x_{n}})
\end{equation}
ogni componente indica quanto e in che verso la funzione cresce, né
consegue che un vettore così definito individui la direzione e verso
in cui la funzione ha crescita massima, inoltre il modulo indica di
quanto cresce la funzione in questa direzione. 

\textbf{Considerazioni pratiche. }Notiamo che la formulazione matematica
del gradiente è definito come il limite del rapporto incrementale
con l'incremento $h$ che tende a zero, ovviamente tale operazione
non può essere svolta direttamente da un calcolatore ma viene svolta
tramite le tecniche del calcolo numerico, in prima approssimazione
è sufficiente calcolare il rapporto con un incremento molto piccolo
come ad esempio $1e-5$ inoltre spesso funziona meglio (soprattutto
in prossimità di punti angolosi, in cui formalmente la derivata non
è definita) la \textit{derivata numerica simmetrica:
\begin{equation}
\frac{f(x+h)-f(x-h)}{2h}
\end{equation}
}

\paragraph{Idea generale}

In ottimizzazione e analisi numerica il metodo di discesa del gradiente
(detto anche metodo del gradiente, metodo steepest descent o metodo
di discesa più ripida) è una tecnica che consente di determinare i
punti di massimo e minimo di una funzione di più variabili. Si voglia
risolvere il seguente problema di ottimizzazione non vincolata nello
spazio $n-$dimensionale $\mathbb{R}^{n}$
\begin{equation}
minimizzare\,f(\boldsymbol{x}),\qquad x\in\mathbb{R}^{n}.
\end{equation}
La tecnica di \textbf{discesa del gradiente} si basa sul fatto che,
per una data funzione $f(\boldsymbol{x})$, la direzione di massima
discesa in un assegnato punto $\boldsymbol{x}$ corrisponde a quella
determinata dall'opposto del suo gradiente in quel punto $\boldsymbol{p}_{k}=-\nabla f(\boldsymbol{x}).$
Questa scelta per la direzione del gradiente garantisce che la soluzione
tenda ad un punto di minimo di $f$. Il metodo del gradiente prevede
dunque di partire da una soluzione iniziale ${\displaystyle \mathbf{x}_{0}}$
scelta arbitrariamente e di procedere iterativamente aggiornandola
come
\begin{equation}
\boldsymbol{x}_{k+1}=\boldsymbol{x}_{k}+\eta_{k}\boldsymbol{p}_{k}
\end{equation}
dove $\eta_{k}\in\mathbb{R}^{+}$ corrisponde alla lunghezza del passo
di discesa, la cui scelta diventa cruciale nel determinare la velocità
con cui l'algoritmo convergerà alla soluzione richiesta. Si parla
di metodo stazionario nel caso in cui si scelga un passo ${\displaystyle \eta_{k}={\bar{\eta}}}$
costante per ogni ${\displaystyle k}$, viceversa il metodo si definisce
dinamico. In quest'ultimo caso una scelta conveniente, ma computazionalmente
più onerosa rispetto a un metodo stazionario, consiste nell'ottimizzare,
una volta determinata la direzione di discesa ${\displaystyle \mathbf{p}_{k}}$,
la funzione di una variabile ${\displaystyle {f}_{k}(\eta_{k}):=f(\mathbf{x}_{k}+\eta_{k}\mathbf{p}_{k})}$
in maniera analitica o in maniera approssimata. Si noti che, a seconda
della scelta del passo di discesa, l'algoritmo potrà convergere a
uno qualsiasi dei minimi della funzione ${\displaystyle f}$, sia
esso locale o globale.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{discesa_unidimesionale}
\par\end{centering}
\caption{Discesa gradiente 1-D}

\end{figure}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{350px-Gradient_descent\lyxdot svg}
\par\end{centering}
\caption{Discesa gradiente 2-D}

\end{figure}


\subsection*{Algoritmo generale}

Lo schema generale per l'ottimizzazione di una funzione $f(\boldsymbol{x})$
mediante metodo del gradiente è il seguente:

\begin{algorithm}[H]
$k=0$

while $\nabla f(\boldsymbol{x})\neq0$

\hspace*{1cm}calcolare la direzione di discesa $\boldsymbol{p}_{k}=-\nabla f(\boldsymbol{x})$

\hspace*{1cm}calcolare il passo di discesa $\eta_{k}$

\hspace*{1cm}$\boldsymbol{x}_{k+1}=\boldsymbol{x}_{k}+\eta_{k}\boldsymbol{p}_{k}$

\hspace*{1cm}$k=k+1$

end.

\caption{Discesa del gradiente}
\end{algorithm}


\subsection{Algoritmi di ottimizzazioni della discesa del gradiente}

Nelle librerie di apprendimento (es: keras) esistono varie implementazioni
di algoritmi per ottimizzare la discesa del gradiente. Questi algoritmi
sono generalmente usati come black-box, in questa sezione forniremo
una panoramica e le intuizioni dietro ad essi. Una prima differenziazione
della discesa del gradiente è sulla quantità di dati usati per i calcoli.
In questa sezione ci riferiamo alla funzione di costo (loss function,
error function etc) da minimizzare come $J(\boldsymbol{x}|\boldsymbol{\theta}):\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
funzione di $\boldsymbol{x}\in\mathbb{R}^{n}$ parametrizzata da $\boldsymbol{\theta}\in\mathbb{R}^{d}.$
Per brevità, visto che minimiziamo rispetto a $\boldsymbol{\theta}$
indichiamo la funzione di costo semplicemente come $J(\boldsymbol{\theta})$.

\paragraph{Batch gradient descent}

Batch gradient descent calcola il gradiente rispetto ai parametri
$\boldsymbol{\theta}$ della funzione di costo sull'intero insieme
di dati di allenamento (training dataset) ed esegue la media aritmetica:
\begin{equation}
\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}-\eta\cdot\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})
\end{equation}
osserviamo che per eseguire un singolo aggiornamento dei parametri
(epoch) dobbiamo calcolare il gradiente su tutto il dataset, questo
rende l'algoritmo estremamente lento e intrattabile nel caso in cui
la dimensione del dataset è maggiore del quantitativo di memoria del
calcolatore. Inoltre non permette l'aggiornamento online del modello
aggiungendo o modificando gli esempi nel training set durante la fase
di allenamento. Batch gradient descent converge sicuramente al minimo
globale se la funzione da ottimizzare è convessa (caso ottimo, altamente
improbabile nella realtà) altrimenti converge ad un minimo locale
o ad un punto di sella (quest'ultimo non voluto). In codice python:

\begin{lstlisting}
for i in range(nb_epochs):
  params_grad = gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
\end{lstlisting}
In formule il gradiente è calcolato come:

\begin{equation}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{x}|\boldsymbol{\theta})=(\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{1}},\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{2}},...,\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{d}})
\end{equation}
dove:
\begin{equation}
\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{i}}=\frac{1}{N}\sum_{n=1}^{N}\frac{\partial J(x_{n}|\boldsymbol{\theta})}{\partial\theta_{i}}
\end{equation}


\paragraph{Stochastic gradient descent}

(SGD) al contrario aggiorna i parametri per ogni esempio di allenamento
$x_{i}$ e il relativo output $y_{i}$:
\begin{equation}
\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}-\eta\cdot\nabla_{\boldsymbol{\theta}}J(x_{i},y_{i}|\boldsymbol{\theta}).
\end{equation}
L'idea alla base di SGD è che batch gradient descent esegue calcoli
ridondanti su interi dataset molto grandi, come ricalcolare il gradiente
per esempi simili prima di aggiornare i pesi. SGD elimina questa ridondanza
aggiornando i pesi ad ogni computazione del gradiente, questo tuttavia
porta ad avere un' alta varianza del gradiente, con conseguenti fluttuazioni
nella loss function da minimizzare. 
\begin{figure}[H]
\begin{centering}
\includegraphics{Stogra}
\par\end{centering}
\caption{Fluttuazioni SGD}
\end{figure}
Tuttavia è stato dimostrato che decrementando lentamente e gradualmente
il learnig rate, SGD mostra le stesse caratteristiche di convergenza
di batch gradient descent, convergendo certamente ad un minimo locale
o gloabale per una funzione non convessa o convessa rispettivamente.
\begin{lstlisting}
for i in range(nb_epochs):
  for example in data:
	params_grad = gradient(loss_function, data, params)
	params = params - learning_rate * params_grad
\end{lstlisting}
In formule:
\begin{equation}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{x}|\boldsymbol{\theta})=(\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{1}},\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{2}},...,\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{d}})
\end{equation}
dove:
\begin{equation}
\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{i}}\approx\frac{\partial J(x_{n}|\boldsymbol{\theta})}{\partial\theta_{i}}
\end{equation}


\paragraph{Mini-batch gradient descent}

prende il meglio dei due metodi aggiornando i pesi per ogni mini batch
di dimensione $n$ esempi di training:
\begin{equation}
\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}-\eta\cdot\nabla_{\boldsymbol{\theta}}J(x_{i:i+n},y_{i:i+n}|\boldsymbol{\theta}).
\end{equation}
In questo modo risulta ridotta la varianza del gradiente e quindi
degli aggiornamenti dei pesi con la conseguenza di una maggiore stabilità
della convergenza e permette di velocizzare il calcolo rispetto a
SGD sfruttando le librerie ottimizzate del calcolo matriciale.
\begin{lstlisting}
for i in range(nb_epochs):
  for batch in get_batches(data, batch_size=n):
	params_grad = gradient(loss_function, batch, params)
	params = params - learning_rate * params_grad
\end{lstlisting}
In formule:
\begin{equation}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{x}|\boldsymbol{\theta})=(\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{1}},\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{2}},...,\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{d}})
\end{equation}
dove:

\begin{equation}
\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{i}}\approx\frac{1}{M}\sum_{n\in minibatch}\frac{\partial J(x_{n}|\boldsymbol{\theta})}{\partial\theta_{i}}
\end{equation}
Minibatch è un sottoinsieme del training set di cardinalità $M$.

\paragraph{Sfide. }

Mini-batch gradient descent non garantisce ancora ottime proprietà
di convergenza, ma offre delle sfide che devono essere affrontate:
\begin{itemize}
\item Scegliere l'adatto \textbf{learning rate} (l.r.) può essere difficile.
Un l.r. troppo piccolo porta ad una convergenza dolorosamente lenta,
mentre un l.r. troppo alto causa fluttuazioni intorno ad un minimo
della loss function o addirittura la divergenza.
\item Programmare diversi l.r. durante le fasi di allenamento seguendo schemi
predefiniti oppure adattandolo ai risulati intermedi dell'allenamento
(\textbf{learning rate adattivo}).
\item Usare differenti l.r. contemporaneamente. Se i dati sono sparsi e
le features si presentano con differenti frequenze potremmo considerare
di utilizzare valori di l.r. elevati quando si presentano le features
più rare.
\item Uscire dai minimi sub ottimi e punti di sella.
\end{itemize}
Ora vediamo una rapida carrellata degli algoritmi più noti.

\subsubsection{Momentum}

SGD ha problemi a navigare attraverso i \textquotedbl burroni\textquotedbl ,
per esempio zone in cui la superficie della funzione curva molto più
rapidamente rispetto alle altre, situazione molto comune vicino ai
minimi locali. In questo scenario, SGD oscilla lungo le pendenze del
burrone e avanza lentamente nella direzione del minimo. 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{SGD_momentum}
\par\end{centering}
\caption{Momento}
\end{figure}
\textbf{Momentum} è un metodo che aiuta ad accelerare SGD nella direzione
rilevante e riduce le oscillazioni come mostrato in Figura 2.6b. Per
fare ciò introduce un termine $\gamma$ nell'aggiornamento del vettore
direzione come mostrato:
\begin{equation}
\begin{cases}
\boldsymbol{v}_{t}=\gamma\boldsymbol{v}_{t-1}+\eta\cdot\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})\\
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\boldsymbol{v}_{t}
\end{cases}
\end{equation}
Il termine $\gamma$ comunemente usato è 0.9 o un valore simile. Essendo
la direzione seguita al tempo t dipendente dal tempo t-1 e quindi
ricorsivamente da tutti i tempi precedenti, permette di mantenere
una sorta di memoria, inoltre è facile convincersi dalla formula che
i cambi di direzioni isolati verranno attenuati mentre la direzione
principale, cioè quella che più volte ricorre verrà incrementata.
Un'analogia utile a comprendere è immaginare una palla che rotola
lungo un sentiero di montagna, la palla tende ad accelerare nella
direzione di discesa ma subisce anche altre accelerazioni isolate
dovute a parziali intralci nel percorso, come ad esempio un sasso
che la fa deviare momentaneamente verso destra, momentum si occupa
di smorzare tali deviazioni e incrementare nella direzione di discesa.

\subsubsection{Nesterov accelerated gradient (NAG).}

Questo metodo è un evoluzione di momentum, tornando all'analogia con
la palla che rotola giù da un pendio, l'idea è quella di guardarsi
attorno prima di calcolare la prossima direzione e cercare di aggirare
preventivamente ostacoli e avvallamenti. Notiamo che in momentum calcoliamo
preventivamente il vettore $\boldsymbol{\theta}_{t-1}-\gamma\boldsymbol{v}_{t-1}$
che fornisce un' approssimazione della prossima posizione dei parametri
(i parametri sono raggruppati in un vettore, possono quindi essere
visti come punti nello spazio). Allora possiamo effettivamente guardarci
attorno calcolando il gradiente non nella posizione attuale dei parametri
ma rispetto all'approssimazione futura.

\begin{equation}
\begin{cases}
\boldsymbol{v}_{t}=\gamma\boldsymbol{v}_{t-1}+\eta\cdot\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}_{t-1}-\gamma\boldsymbol{v}_{t-1})\\
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\boldsymbol{v}_{t}
\end{cases}
\end{equation}
Ancora, $\gamma$ comunemente usato è 0.9 o un valore simile.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{NAG}
\par\end{centering}
\caption{NAG update}
\end{figure}
In figura 2.7 vediamo un confronto tra Momentum e NAG. Momentum prima
calcola il gradiente corrente (vettore blu piccolo) dopo esegue un
grande passo nella direzione accumulata nei passi precedenti (vettore
blu grande). NAG invece prima esegue un grande passo nella direzione
predetta (vettore grande marrone) poi calcola il gradiente e corregge
il passo (vettore rosso piccolo). Questo aggiornamento anticipato
ci impedisce di accelerare troppo e aumenta la reattività durante
l'allenamento. 

Adesso siamo capaci di adattare gli aggiornamenti dei pesi alla pendenza
della funzione di errore e di accelerare SGD. Il prossimo passo è
adattare i nostri aggiornamenti specificamente per ogni peso in modo
da eseguire aggiornamenti minori o maggiori a seconda dell'importanza
del peso.

\subsubsection{Adagrad }

Adatta il learning rate ai parametri, eseguendo aggiornamenti maggiori
ai pesi meno frequenti e aggiornamenti minori ai pesi più frequenti.
Per questa ragione è adatto nel caso in cui i dati siano sparsi .
Sia:

\begin{equation}
g_{t}=\nabla_{\theta_{t}}J(\theta_{t})
\end{equation}
inoltre definiamo:
\begin{equation}
G_{t}=\begin{bmatrix}\begin{array}{ccccc}
\sum_{j=0}^{t}(\frac{\partial j(\boldsymbol{\theta})}{\partial\theta_{1}})^{2} & 0 & \cdots &  & 0\\
0 & \ddots &  &  & \vdots\\
\vdots &  & \sum_{j=0}^{t}(\frac{\partial j(\boldsymbol{\theta})}{\partial\theta_{i}})^{2} &  & \vdots\\
 &  &  & \ddots & \vdots\\
0 & \cdots & \cdots & \cdots & \sum_{j=0}^{t}(\frac{\partial j(\boldsymbol{\theta})}{\partial\theta_{d}})^{2}
\end{array}\end{bmatrix}
\end{equation}
la matrice diagonale appartenente a $\mathbb{R}^{dxd}$ in cui ogni
elemento in posizione $i,i$ è la somma dei quadrati della derivata
parziale rispetto al parametro $i-esimo$ dal tempo 0 al tempo $t$.
La regola di aggiornamento diventa:
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\odot g_{t}
\end{equation}
dove $\odot$ è il prodotto elemento per elemento tra matrici. In
questo modo risulta evidente che il l.r. risulta inversamente proporzionale
agli aggiornamenti precedenti, più un peso viene aggiornato più il
termine corrispondente $G_{t,ii}$ cresce e di conseguenza il learning
rate decresce. Il termine $\epsilon$ è necessario per evitare divisioni
per zero (comunemente nell'ordine di $10^{-8}$). I ricercatori inoltre
hanno notato che senza l'operatore di radice quadrata l'algoritmo
funziona molto peggio, probabilmente dovuta a una rapida decadenza
dei coefficienti. Uno dei benefici di \textbf{Adagrad} è l'eliminazione
della necessità di modificare manualmente il learning rate. D'altra
parte introduce un' altra debolezza, anche estraendo la radice quadrata,
a denominatore sommiamo sempre quantità positive, questo alla lunga
porta il learning rate a valori infinitesimi, bloccando di fatto l'apprendimento.
Il prossimo algoritmo ha lo scopo di risolvere questo problema.

\subsubsection{Adadelta }

È un estensione di Adagrad che cerca di ridurre l'aggressività con
cui monotonicamente decresce il learning rate. Invece di accumulare
tutti i quadrati dei precedenti gradienti, \textbf{Adadelta} restringe
l'accumulo con una finestra di una fissata dimensione $w.$ Inoltre,
invece di salvare inefficientemente tutti i $w$ gradienti al quadrato
salva una media decadente dei precedenti gradienti. La media è definita
nel modo seguente:

\begin{equation}
E[g^{2}]_{0}=\boldsymbol{0}
\end{equation}

\begin{equation}
E[g^{2}]_{t}=\gamma E[g^{2}]_{t-1}+(1-\gamma)g_{t}^{2}
\end{equation}
con ancora:
\[
g_{t}=\nabla_{\theta_{t}}J(\theta_{t})
\]
e $\gamma$ un termine simile al momento (circa 0.9). Abbiamo affermato
precedentemente che Adadelta è l'evoluzione di Adagrad, richiamiamo
il vettore aggiornamento di Adagrad:
\begin{equation}
\Delta\theta_{t}=-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\odot g_{t}
\end{equation}
adesso sostituiamo semplicemente la matrice diagonale $G_{t}$ con
la media definita:

\begin{equation}
\Delta\theta_{t}=-\frac{\eta}{\sqrt{E[g^{2}]_{t}+\epsilon}}g_{t}
\end{equation}
notiamo che adesso a denominatore abbiamo un vettore e non una matrice.
Definiamo, per brevità di scrittura

\begin{equation}
RMS[g]_{t}=\sqrt{E[g^{2}]_{t}+\epsilon}
\end{equation}
dove RMS sta per root mean squared, allora
\begin{equation}
\Delta\theta_{t}=-\frac{\eta}{RMS[g]_{t}}g_{t}
\end{equation}
il prossimo passo consiste nel rendere anche il numeratore dipendente
in qualche modo dai parametri, definiamo quindi un'altra media, questa
volta una media degli aggiornamenti precedenti:
\begin{equation}
E[\Delta\theta^{2}]_{0}=\boldsymbol{0}
\end{equation}
\begin{equation}
E[\Delta\theta^{2}]_{t}=\gamma E[\Delta\theta^{2}]_{t-1}+(1-\gamma)\Delta\theta^{2}.
\end{equation}
La root mean squared degli aggiornamenti dei parametri è:

\begin{equation}
RMS[\Delta\theta]_{t}=\sqrt{E[\Delta\theta^{2}]_{t}+\epsilon}.
\end{equation}
Siccome all'aggiornamento al tempo $t$ non possiamo conoscere $RMS[\Delta\theta]_{t}$,
lo approssimiamo usando RMS al tempo precedente. Siamo giunti alla
formula finale di Adadelta:

\begin{equation}
\begin{cases}
\Delta\theta_{t}=-\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_{t}}g_{t}\\
\theta_{t+1}=\theta_{t}-\Delta\theta_{t}
\end{cases}.
\end{equation}


\subsubsection{RMSprop}

È molto simile ad Adadelta, infatti i due metodi sono stati sviluppati
indipendentemente nello stesso periodo e con lo scopo di risolvere
i problemi di Adagrad. L'aggiornamento dei pesi segue questi passi:

\begin{equation}
E[g^{2}]_{t}=\gamma E[g^{2}]_{t-1}+(1-\gamma)g_{t}^{2}
\end{equation}

\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E[g^{2}]_{t}+\epsilon}}g_{t}
\end{equation}
gli autori suggeriscono di usare $\gamma=0.9$ e $\eta=0.001.$

\subsubsection{Adam}

\textbf{Adaptive Moment Estimation} (\textbf{Adam}) è un altro metodo
per adattare il learning rate ad ogni parametro. In aggiunta alla
media decadente dei gradienti precedenti al quadrato $v_{t}$ come
Adadelta e RMSprop, Adam mantiene anche una media decadente dei gradienti
passati $m_{t}$ (NB non il quadrato):
\begin{equation}
m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}
\end{equation}

\begin{equation}
v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}.
\end{equation}
Notiamo che il termine $v_{t}$ è del tutto analogo a $E[g^{2}]_{t}$
di Adadelta e RMSprop. $m_{t}$ e $v_{t}$ sono stime del momento
primo (media) e momento secondo (varianza) dei gradienti rispettivamente,
da qui il nome del metodo. Entrambi i termini vengono inizializzati
a 0, gli autori hanno notato però che questa inizializzazione porta
un bias che fa tendere l'aggiornamento a 0, soprattutto con valori
di $\beta_{1},\beta_{2}$ vicini a 1. Per contrastare questo bias
introduciamo una correzione a entrambi i termini:

\begin{equation}
\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}
\end{equation}

\begin{equation}
\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}
\end{equation}
L'aggiornamento diventa allora:
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\hat{m}_{t}
\end{equation}
Gli autori propongono come valori di default: $\beta_{1}=0.9$, $\beta_{2}=0.999$
e $\epsilon=10^{-8}.$ Hanno mostrato empiricamente che Adam lavora
bene e con prestazioni simili a Adadelta e RMSProp.

\subsubsection{AdaMax}

In Adam calcoliamo $v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$,
notiamo che $v_{t}$ è direttamente proporzionale alla norma $l_{2}$
del gradiente e quindi l'aggiornamento è inversamente proporzionale
alla norma del gradiente. Possiamo generalizzare l'aggiornamento usando
$l_{p}$ norma.

\begin{equation}
v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})|g_{t}|^{p}
\end{equation}
\begin{equation}
=(1-\beta_{2})\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p}.
\end{equation}
La norma $l_{p}$ è numericamente instabile per grandi valori di $p$,
questo è il motivo per cui generalmente si usa $l_{1},l_{2}$. Tuttavia
la norma $l_{\infty}$ mostra ancora un comportamento molto stabile,
per questo gli autori di \textbf{AdaMax} la propongono. Definiamo
$u_{t}=\lim_{p\rightarrow\infty}(v_{t})^{1/p}$ allora:

\begin{equation}
u_{t}=\lim_{p\rightarrow\infty}(v_{t})^{1/p}=\lim_{p\rightarrow\infty}((1-\beta_{2})\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p})^{1/p}
\end{equation}

\begin{equation}
=\lim_{p\rightarrow\infty}(1-\beta_{2})^{1/p}(\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p})^{1/p}
\end{equation}

\begin{equation}
=\lim_{p\rightarrow\infty}(\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p})^{1/p}
\end{equation}

\begin{equation}
=max(\beta_{2}^{t-1}|g_{1}|,\beta_{2}^{t-2}|g_{2}|,...,\beta_{2}|g_{t-1}|,|g_{t}|)
\end{equation}
che può essere riscritta ricorsivamente come:
\begin{equation}
u_{t}=max(\beta_{2}u_{t-1},|g_{t}|).
\end{equation}
Al solito, $u_{0}=0.$ Sostituiamo nella formula di Adam $\sqrt{\hat{v}_{t}}+\epsilon$
con $u_{t}$, otteniamo così la regola di aggiornamento AdaMax:

\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{u_{t}}\hat{m}_{t}
\end{equation}
I valori consigliati di default sono $\eta=0.002$, $\beta_{1}=0.9$,
$\beta_{2}=0.999$.

\subsubsection{Nadam }

Come visto prima, Adam può essere visto come una combinazione di RMSprop
e Momentum: RMSprop contribuisce tramite il termine $v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$
e momentum tramite il termine $m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}$.
Abbiamo visto anche che Nesterov accelerated gradient (NAG) è superiore
a momentum. \textbf{Nadam} (\textbf{Nesterov-accelerated Adaptive
Moment Estimation}) combina così Adam e NAG. In NAG calcoliamo il
gradiente non nella posizione attuale ma nella posizione stimata a
priori, in cui arriveremo dopo l' aggiornamento.

\[
g_{t}=\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}_{t}-\gamma\boldsymbol{m}_{t-1})
\]

\[
m_{t}=\gamma m_{t-1}+\eta g_{t}
\]

\[
\theta_{t+1}=\theta_{t}-m_{t}
\]
Gli autori di Nadam propongono di modificare NAG in questo modo: piuttosto
che applicare il momento due volte, una volta per guardarsi attorno
nel calcolo del gradiente $g_{t}$ e una seconda volta nel calcolo
di $\theta_{t+1}$, usiamo il momento corrente $m_{t}$ per guardarci
attorno direttamente nell'aggiornamento dei pesi e non nel gradiente,
allora NAG modificato diventa:

\begin{equation}
g_{t}=\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}_{t})
\end{equation}
\begin{equation}
m_{t}=\gamma m_{t-1}+\eta g_{t}
\end{equation}

\begin{equation}
\theta_{t+1}=\theta_{t}-(\gamma m_{t}+\eta g_{t})
\end{equation}
Richiamiamo brevemente anche il metodo Adam:
\[
m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}
\]

\[
v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}
\]

\[
\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}
\]

\[
\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}
\]

\[
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\hat{m}_{t}
\]
Espandiamo l'ultima equazione:
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\frac{m_{t}}{1-\beta_{1}^{t}})
\end{equation}

\begin{equation}
=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\frac{\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}

\begin{equation}
=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\frac{\beta_{1}m_{t-1}}{1-\beta_{1}^{t}}+\frac{(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}
Notiamo che $\frac{m_{t-1}}{1-\beta_{1}^{t}}=\hat{m}_{t-1}$ sostituendo:

\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\beta_{1}\hat{m}_{t-1}+\frac{(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}
Notiamo che questa regola è ancora Adam, per giungere a Nadam dobbiamo
inserire NAG modificato in precedenza, notiamo che in NAG modificato
usiamo il momento corrente nell' aggiornamento e non il momento del
passo precedente, modifichiamo di conseguenza la formula di aggiornamento:

\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\beta_{1}\hat{m}_{t}+\frac{(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{confronto_discesa}
\par\end{centering}
\caption{Confronto algoritmi}
\end{figure}


\section{Backpropagation}

In questa sezione svilupperemo una comprensione intuitiva della \textbf{backpropagation}
che permette di calcolare agilmente il gradiente della funzione di
costo (error function o loss function) di una rete neurale, tramite
l'applicazione ricorsiva della regola di derivazione di funzioni composte,
nota anche come \textbf{regola della catena}.

\paragraph{Derivazione funzione composta.}

Consideriamo due funzioni reali di varibile reale (la regola è valida
in generale per tutte le funzioni differenziabili, anche a più variabili
con tutti gli adattamenti del caso) $f:\mathbb{R}\rightarrow\mathbb{R}$
e $g:\mathbb{R}\rightarrow\mathbb{R}$ e chiamiamole $y=g(x)$ e $z=f(y)$.
Sia poi $z=f(y)=f(g(x))$ la loro composizione. Allora vale:
\begin{equation}
\frac{dz}{dx}=\frac{dz}{dy}\cdot\frac{dy}{dx}
\end{equation}
(come se il differenziale $dy$ si semplificasse nella moltiplicazione,
con buona pace dei matematici). Sostituendo:

\begin{equation}
\frac{dz}{dx}=\frac{d}{dy}f(y)\cdot\frac{d}{dx}g(x)
\end{equation}
\begin{equation}
=f'(y)\cdot g'(x)
\end{equation}
\begin{equation}
=f'(g(x))\cdot g(x)
\end{equation}
In parole povere, la derivata della funzione composta $z=f(g(x))$
è data dalla derivata della funzione più esterna, con argomento invariato,
moltiplicata per la derivata della funzione più interna. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{Rete_per_regressione}
\par\end{centering}
\caption{Rete neurale multistrato}
\end{figure}
In Figura 2.10 è mostrata una rete neurale che computa la funzione
$g_{1}:\mathbb{R}^{I}\rightarrow\mathbb{R}$ parametrizzata da $w$.
La funzione da minimizzare è $E(w|t_{n},x_{n})$, notiamo che $E$
è funzione di $w$ e parametrizzata da $t_{n}$ e $x_{n}$. Usando
la regola della catena calcoliamo la derivata: 
\begin{equation}
\frac{\partial E(w)}{\partial w_{ji}}=-2\sum_{n=1}^{N}(t_{n}-g_{1}(x_{n},w))\cdot g'_{1}(x_{n},w)\cdot w_{1j}^{(2)}\cdot h'_{j}(\sum_{j=0}^{J}w_{1j}^{(1)}\cdot x_{i,n})\cdot x_{i}
\end{equation}
Infatti notiamo:
\begin{equation}
\frac{\partial E(w)}{\partial g_{1}(x_{n},w)}=-2\sum_{n=1}^{N}(t_{n}-g_{1}(x_{n},w))
\end{equation}

\begin{equation}
\frac{\partial g_{1}(x_{n},w)}{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}=g'_{1}(x_{n},w)
\end{equation}

\begin{equation}
\frac{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}{\partial h{}_{j}(\bullet)}=w_{1j}^{(2)}
\end{equation}

\begin{equation}
\frac{\partial h{}_{j}(\bullet)}{\partial w_{1j}^{(1)}\cdot x_{i,n}}=h'_{j}(\sum_{j=0}^{J}w_{1j}^{(1)}\cdot x_{i,n})
\end{equation}
\begin{equation}
\frac{\partial w_{1j}^{(1)}\cdot x_{i,n}}{\partial w_{1j}^{(1)}}=x_{i}
\end{equation}
Allora:
\begin{equation}
\frac{\partial E(w)}{\partial w_{ji}}=\frac{\partial E(w)}{\partial g_{1}(x_{n},w)}\cdot\frac{\partial g_{1}(x_{n},w)}{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}\cdot\frac{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}{\partial h{}_{j}(\bullet)}\cdot\frac{\partial h{}_{j}(\bullet)}{\partial w_{1j}^{(1)}\cdot x_{i,n}}\cdot\frac{\partial w_{1j}^{(1)}\cdot x_{i,n}}{\partial w_{1j}^{(1)}}.
\end{equation}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{backprop}
\par\end{centering}
\caption{Backpropagation}
\end{figure}
Dal punto di vista pratico, la regola della catena è molto utile,
permette facilmente di parallelizzare ed eseguire localmente il calcolo
del gradiente e l'aggiornamento dei pesi. Ogni neurone ha la propria
\textit{funzione di attivazione} con la propria derivata. Possiamo
calcolare le derivate singolarmente e propagare all'indietro semplicemente
tramite un prodotto. Per esempio nella rete in Figura 2.10 il neurone
di output calcola tutte le derivate parziali della sua funzione di
attivazione rispetto ai propri parametri di ingresso, il risultato
così ottenuto viene propagato allo strato precedente, in cui ogni
neurone ha già calcolato le proprie derivate rispetto ai suoi parametri
di input, le moltiplica con la derivata ricevuta dallo strato successivo
e propaga all'indietro e così via fino allo strato di input. Inoltre
quando un neurone esegue il prodotto tra le sue derivate e quella
ricevuta dallo strato successivo, è in possesso del gradiente della
funzione costo rispetto a tutti i suoi parametri e quindi procede
ad aggiornare i pesi secondo una delle strategie viste nella precedente
sezione. 

\subsection{Scomparsa del gradiente}

I problemi che si possono incontrare usando la backpropagation sono
molteplici e dipendono da vari fattori, come profondità della rete,
funzioni di attivazione scelte, inizializzazione dei pesi e altri.
Alcuni sono già stati accennati e trattati nei vari algoritmi di discesa
del gradiente visti. Il problema della \textbf{scomparsa del gradiente}
(in lingua inglese \textbf{vanishing gradient} problem) è un fenomeno
che crea difficoltà nell'addestramento delle reti neurali profonde
tramite retropropagazione dell'errore mediante discesa stocastica
del gradiente. Come visto, ogni parametro del modello riceve ad ogni
iterazione un aggiornamento proporzionale alla derivata parziale della
funzione di perdita rispetto al parametro stesso. Una delle principali
cause è la presenza di funzioni di attivazione non lineari classiche,
come la \textit{tangente iperbolica} o la \textit{funzione logistica},
che hanno gradiente a valori nell'intervallo ${\displaystyle (0,1)}$.
Poiché nell'algoritmo di retropropagazione i gradienti ai vari livelli
vengono moltiplicati tramite la regola della catena, il prodotto di
${\displaystyle n}$ numeri in ${\displaystyle (0,1)}$ decresce esponenzialmente
rispetto alla profondità ${\displaystyle n}$ della rete. Quando invece
il gradiente delle funzioni di attivazione può assumere valori elevati,
un problema analogo che può manifestarsi è quello dell'esplosione
del gradiente.

\section{Overfitting}

In statistica e in informatica, si parla di \textbf{overfitting} (in
italiano: adattamento eccessivo, sovradattamento) quando un modello
statistico molto complesso si adatta ai dati osservati (il campione)
perché ha un numero eccessivo di parametri rispetto al numero di osservazioni.
Un modello assurdo e sbagliato può adattarsi perfettamente se è abbastanza
complesso rispetto alla quantità di dati disponibili. Si sostiene
che l'overfitting sia una violazione del principio del Rasoio di Occam. 

\paragraph{Apprendimento automatico}

Il concetto di overfitting è molto importante anche nell'apprendimento
automatico e nel data mining. Di solito un algoritmo di apprendimento
viene allenato usando un certo insieme di esempi (il training set
appunto), ad esempio situazioni tipo di cui è già noto il risultato
che interessa prevedere (output). Si assume che l'algoritmo di apprendimento
(il learner) raggiungerà uno stato in cui sarà in grado di predire
gli output per tutti gli altri esempi che ancora non ha visionato,
cioè si assume che il modello di apprendimento sarà in grado di \textbf{generalizzare}.
Tuttavia, soprattutto nei casi in cui l'apprendimento è stato effettuato
troppo a lungo o dove c'era uno scarso numero di esempi di allenamento,
il modello potrebbe adattarsi a caratteristiche che sono specifiche
solo del training set, ma che non hanno riscontro nel resto dei casi;
perciò, in presenza di overfitting, le prestazioni (cioè la capacità
di adattarsi/prevedere) sui dati di allenamento aumenteranno, mentre
le prestazioni sui dati non visionati saranno peggiori.

\paragraph{Contromisure}

Sia nella statistica sia nel apprendimento automatico, per prevenire
ed evitare l'overfitting è necessario mettere in atto particolari
accorgimenti tecnici, come la convalidazione incrociata e l'arresto
anticipato, che indicano quando un ulteriore allenamento non porterebbe
a una migliore generalizzazione.

\paragraph{Cross validation }

La convalidazione incrociata è un accorgimento che non permette direttamente
di evitare l'overfitting ma ci permette di riconoscere quando avviene.
L'idea è molto semplice e quasi sempre applicabile, a patto di avere
un training set abbastanza grande, consiste nel rimuovere dal training
set un certo numero di esempi e spostarli nel validation set. La rete
si allena ancora normalmente sul training set, che risulterà ridotto,
successivamente ad ogni epoca vengono sottoposti gli esempi del validation
set che vengono processati dalla rete, quest' ultima durante la fase
di validazione non si allena (non aggiorna i pesi) e viene monitorata
la funzione di errore che in assenza di overfitting mostrerà un andamento
simile alla funzione di errore valutata durante la fase di allenamento,
nel momento in cui la rete inizierà ad adattarsi eccessivamente noteremo
che la funzione di errore valutata nel validation set iniziera ad
aumentare mentre quando la valutiamo nel training set continuerà a
decrescere.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{Overfitting_svg\lyxdot svg}
\par\end{centering}
\caption{Overfitting: La curva blu mostra l'andamento dell'errore nel classificare
i dati di training, mentre la curva rossa mostra l'errore nel classificare
i dati di test o validazione. Una situazione in cui il secondo aumenta
mentre il primo diminuisce è indice della possibile presenza di un
caso di overfitting. }
\end{figure}
Esistono varie implementazioni della cross validation.

\subsection{Holdout cross validation}

È il metodo più semplice per implementare la \textbf{validazione incrociata}.
Il data set viene diviso staticamente in due parti, \textbf{training
set} e \textbf{validation set} e i due sottoinsiemi vengono usati
come descritto prima.

\subsection{K-fold cross validation}

Il data set viene suddiviso in \textbf{k} sotto insiemi di egual dimensione
(escluso al più l'ultimo), il training del modello viene ripetuto
k volte ed ogni volta viene usato un sotto insieme come validation
set, ogni volta ripartendo dallo stato iniziale. Alla fine delle k
iterazioni i risultati delle validazioni vengono mediati per ottenere
il risultato finale. Il vantaggio di questo metodo è che importa meno
come i dati vengono divisi in quanto dopo k iterazioni tutto il dataset
è stato usato almeno una volta come validation set (meno varianza).
Lo svantaggio è l'enorme peso computazionale (training ripetuto k
volte) per questo è quasi mai usato.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{kfoldvalidation}
\par\end{centering}
\caption{K-fold cross validation: in giallo il sottoinsieme di validazione}
\end{figure}


\subsection{Leave-one-out cross validation}

È una variante di k-fold dove k viene posto uguale a N cardinalità
del dataset. Quindi la rete si allena su N-1 esempi e un solo esempio
è usato per la validazione.

\subsection{Iperparametri}

Fino ad ora abbiamo visto molti tasselli che compongono una rete neurale
ma non come combinarli assieme. Purtroppo ancora non è stata sviluppata
una metodologia di progettazione che possa fornire linee guida solide
per la creazione di una rete neurale per un determinato compito. Alcuni
strumenti matematici possono aiutarci ad esempio come abbiamo fatto
per trovare la funzione di errore per la regressione. Per altre grandezze
invece va fatto ricorso all'esperienza, ad altri risultati pubblicati
riguardanti reti che svolgono un compito simile e tanti, tantissimi
tentativi ed errori. Per valutare la bontà di un modello possiamo
usare l'\textbf{insieme di test} e ancora la cross validation. Quanti
e quali sono quindi gli \textbf{iperparametri} che il progettista
della rete deve scegliere? Molti, per esempio in una rete feedforward
bisogna definire:
\begin{itemize}
\item \textbf{numero dei neuroni} e loro disposizione in strati;
\item \textbf{funzione di attivazione} dei neuroni;
\item \textbf{funzione di errore};
\item algoritmo di \textbf{discesa del gradiente};
\end{itemize}
eventualmente anche:
\begin{itemize}
\item \textbf{dimensione batch};
\item \textbf{dimensione validation set} per la cross validation;
\item molti altri a seconda della complessità della rete.
\end{itemize}
Come scegliere questi iperparametri?
\begin{itemize}
\item usare tutte le conoscenze pregresse e ipotesi sulla distribuzione
dei dati da processare;
\item sfruttare le conoscienze acquisite da altri nella risoluzione di compiti
simili;
\item usare la creatività!
\end{itemize}
Anche per la scelta degli iperparametri del modello è di aiuto la
cross validation. Infatti possiamo usare (ed è altamente consigliato)
la validazione per monitorare l'andamento della loss function e determinare
la bontà del modello al variare degli iperparametri. È utile usare
i dati di validazione al posto di quelli di test per evitare che le
nostre scelte degli iperparametri siano condizionate e sovra adattate
al test set, perdendo generalità. Modifichiamo gli iperparametri in
base ai risultati sul validation set, se questi sono confermati anche
dal test set possiamo ragionevolmente affermare che il modello funzioni.
Se il test set non conferma allora abbiamo sovra adattato gli iperparametri
al validation set (una qualche forma di overfitting manuale). 

\subsection{Weight decay}

La regolarizzazione riguarda il vincolamento della \textquotedblleft libertà\textquotedblright{}
di un modello, sulla base di un\textquoteright assunzione a-priori
sul modello stesso, per ridurre l\textquoteright overfitting. Finora
abbiamo massimizzato la verosimiglianza dei dati

\begin{equation}
w_{MLE}=argmax_{w}\hphantom{}P(D|w)
\end{equation}

questo approccio è detto frequentista. Usando, invece, un approccio
\textbf{Bayesiano}, possiamo ridurre la libertà del modello. Usiamo,
cioè, una \textbf{Maximum A-Posteriori verosimiglianza}:

\begin{equation}
w_{MAP}=argmax_{w}P(w|D)\approx argmax_{w}P(D|w)\cdot P(w)
\end{equation}

dove $P(w)$ è la probabilità a-priori, che definisce lo spazio di
ricerca dell\textquoteright algoritmo. Si è osservato che piccoli
pesi migliorano la capacità di generalizzazione delle reti neurali.
Weight decay in pratica aggiunge una penalità proporzionale alla norma
L2 dei pesi alla loss function, con il fine di imprimere una direzione
al gradiente che riduca la norma dei pesi. Oltre all'evidenza empirica
in taluni casi weigt decay è supportato dalla teoria matematica usando
\textbf{Maximum A-Posteriori verosimiglianza}. Analiziamo il caso
già visto della regressione, ma utilizzando \textit{MAP} e supponiamo
che i pesi vengano inizializzati con una distribuzione $P(w)\approx N(0,\sigma_{w})$
che come vedremo più avanti è assolutamente ragionevole come ipotesi.
\[
argmax_{w}P(D|w)\cdot P(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}}*\prod_{q=1}^{Q}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(w_{q})^{2}}{2\sigma^{2}}}
\]

\[
=argmin_{w}(\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2})+\sum_{q=1}^{Q}\frac{(w_{q})^{2}}{2\sigma^{2}}
\]

\begin{equation}
=argmin_{w}(\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2})+\gamma\sum_{q=1}^{Q}(w_{q})^{2}
\end{equation}

Come scegliere il parametro gamma? Una possibile strategia è tramite
cross-validation.

\section{Altre tecniche di ottimizzazione}

\subsection{Shuffling e Curriculum Learning}

Generalmente allenare una rete fornendole gli esempi sempre nello
stesso ordine può portare ad un bias nell'algoritmo di ottimizzazione
e quindi all'overfitting. Di conseguenza una buona idea è fornire
gli esempi mescolati ad ogni epoca. In altri casi, in cui lo scopo
della rete è risolvere via via problemi sempre più complessi, allora
è meglio fornire gli esempi in ordine di complessità. Quest'ultimo
è il caso del \textbf{Curriculum learning}.

\subsection{Preprocessamento dei dati}

Ci sono tre operazioni principali che possono essere eseguite sui
dati di input prima di essere processati dalla rete (sia durante il
training che durante il test). Ricordiamo che per noi l'input è in
linea generale un vettore, quindi può essere visto come un punto nello
spazio e l'insieme dei dati, rappresentato da una matrice $X\in\mathbb{R}^{N\times D}$
dove $N$ è il numero dei dati, $D$ la loro dimensione.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{Dati_originali}
\par\end{centering}
\caption{Dati originali}

\end{figure}


\subsubsection{Sottrazione della media}

Questa operazione ha il compito di centrare la media dell'insieme
dei dati in zero. Ad ogni coordinata di tutti i vettori viene sottratta
la media rispettiva. In formule:

sia $\boldsymbol{x}_{i}$ l'i-esimo vettore appartente a $X$ di dimensione
$D$, $\boldsymbol{x}_{i}=(x_{i,1},...,x_{i,D}$)
\begin{equation}
\boldsymbol{y}_{i}=\boldsymbol{x}_{i}-\widehat{\boldsymbol{x}}
\end{equation}
dove
\begin{equation}
\widehat{\boldsymbol{x}}=(\frac{1}{N}\sum_{i=1}^{N}x_{i,1},\frac{1}{N}\sum_{i=1}^{N}x_{i,2},...,\frac{1}{N}\sum_{i=1}^{N}x_{i,D})
\end{equation}


\subsubsection{Normalizzazione}

Ci riferiamo alla \textbf{normalizzazione} dei dati non in senso geometrico
classico, che consiste nel dividire ogni vettore per la sua norma,
ma come l'operazione di riscalamento di ogni dimensione in modo tale
che ognuna di esse abbia circa la stessa scala. Per far ciò possiamo
operare in due modi:
\begin{enumerate}
\item Dividiamo ogni dimensione di ogni vettore per la deviazione standard
di quella dimensione (analogo a quanto fatto per la media). In formule:
\begin{equation}
\boldsymbol{y}_{i}=\frac{\boldsymbol{x}_{i}}{\boldsymbol{\sigma}}
\end{equation}
dove
\begin{equation}
\boldsymbol{\sigma}=(\sqrt{\frac{1}{N}\sum(x_{i,1}-\widehat{\boldsymbol{x}}_{1})^{2}},\sqrt{\frac{1}{N}\sum(x_{i,2}-\widehat{\boldsymbol{x}}_{2})^{2}},...,\sqrt{\frac{1}{N}\sum(x_{i,D}-\widehat{\boldsymbol{x}}_{D})^{2}})
\end{equation}
\item Dividiamo ogni dimensione di ogni vettore per la differenza tra la
coordinata massima e minima rispettiva. In formule:
\begin{equation}
\boldsymbol{y}_{i}=\frac{\boldsymbol{x}_{i}}{\boldsymbol{\Delta}}
\end{equation}
dove 
\begin{equation}
\boldsymbol{\Delta}=(\max_{i}(x_{i,1}),\max_{i}(x_{i,2}),...,\max_{i}(x_{i,D}))
\end{equation}
in questo modo l'input normalizzato avrà ogni componente compresa
nell'intervallo $[-1,1]$
\end{enumerate}
Generalmente \textbf{sottrazione della media} e \textbf{normalizzazione}
vengono effettuate in coppia. Ha senso normalizzare se si ha ragione
di credere che l'input abbia features in scale differenti ma equamente
importanti. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{preprocessing_data}
\par\end{centering}
\caption{Preprocessamento completo}
\end{figure}

\textbf{Attenzione: }un punto importante da sottolineare nelle preelaborazioni
è che qualsiasi statistica (es. media, varianza etc) deve essere calcolata
solo sui dati di addestramento e quindi applicata ai tutti i dati
(allenamento, validazione, test). Ad esempio calcolare la media su
tutti i dati e poi suddividere in allenamento, validazione e test
sarebbe un errore. Invece le statistiche vanno calcolate solo sui
dati di allenamento e poi applicate a tutti i dati.

\subsection{Batch Normalization}

Con l\textquoteright aumento del numero di strati delle reti neurali
permesso dalla sempre crescente potenza di calcolo dei computer odierni
sono sorti vari problemi soprattutto durante l\textquoteright addestramento,
tra cui l\textquoteright esplosione e la \textit{scomparsa del gradiente}.
Una delle possibili soluzioni a questi problemi consiste nel layer
di \textbf{batch normalization}. Come già detto precedentemente, solitamente
l\textquoteright input di una rete viene \textit{normalizzato} (cioè
tutti gli input vengono riscalati per avere valori compresi in un
range scelto, solitamente {[}0,1{]}) o \textit{standardizzato} (cioè
ai valori degli input gli viene sottratta la media e vengono divisi
per la deviazione standard del dataset, per avere dei dati distribuiti
con media 0 e deviazione standard 1). Ciò aiuta l\textquoteright addestramento
in quanto riduce il range dinamico dei dati in input a un range fisso,
permettendo alla rete di estrarre feature più robuste e più velocemente.
Tuttavia se la rete ha un elevato numero di layer, a seconda dei valori
dei pesi l\textquoteright output dei layer potrebbe tornare ad avere
range dinamici ampi. Per ovviare a questo problema, si interpone un
layer di batch normalization dopo il layer della rete da normalizzare.
In questo modo non solo l\textquoteright input della rete ma anche
l\textquoteright output dei vari layer viene standardizzato. Ogni
layer di batch normalization ha \textbf{due pesi} (\textit{per ogni
batch}), un \textbf{fattore di scala} e un \textbf{bias}, che modificano
l\textquoteright output standardizzato permettendo di cambiarne media
e deviazione standard. Questi pesi possono venire aggiustati durante
l\textquoteright addestramento. Il termine \textbf{batch} nel nome
deriva dal gruppo di dati su cui viene effettuata la normalizzazione
nel layer, che in questo caso è appunto un batch utilizzato durante
l\textquoteright addestramento con \textbf{Stochastic Gradient Descent}
(\textbf{SGD}). Per implementare la batch normalization:

si trova il valore medio per il batch corrente $B=\{x_{1},...,x_{m}\}$,
ovvero il valore medio prodotto da un particolare sub-strato della
rete, prima di passare dalla funzione di attivazione non-lineare,
quindi
\[
\mu_{B}=\frac{1}{m}\sum_{i=1}^{m}x_{i}
\]
si trova la varianza per il batch corrente
\[
\sigma_{B}^{2}=\frac{1}{m}\sum_{i=1}^{m}(x_{i}-\mu_{B})^{2}
\]
si normalizza il valore con l\textquoteright equazione
\[
\widehat{x_{i}}=\frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}}
\]
Quindi, ad ogni valore si sottrae la media e si divide per una deviazione
standard a cui si aggiunge il valore $\epsilon$. Il valore $\epsilon$
è una costante positiva, ad esempio 0.001, che conferisce maggiore
stabilità numerica (evita divisioni per zero) e incrementa, di poco,
la varianza per ogni batch. Incrementare la varianza ha lo scopo di
tenere in considerazione che la varianza della popolazione è maggiore
di ogni campione preso dalla popolazione stessa. Il valore normalizzato
viene poi moltiplicato per Gamma a sommato al parametro Beta, entrambi
parametri che la rete apprenderà in fase di training:
\begin{equation}
y_{i}=\gamma\widehat{x_{i}}+\beta
\end{equation}
$y_{i}$ è il valore normalizzato prodotto da ogni sub-strato della
rete che passerà attraverso la funzione di attivazione, quali Sigmoide,
Relu, Tanh ecc ecc. Durante il training il gradiente dovrà propagarsi
a ritroso (backpropagation) attraverso questa trasformazione, affinchè
Beta e Gamma ricevano il segnale di errore e vengano ottimizzati.
La fase di inferenza, rispetto alla fase di training, presenta alcune
differenze: la rete, infatti, non viene esposta ad un batch di input
ma ad un solo valore di input, per il quale dovrà produrre un output.
Se utilizzassimo la medesima tecnica applicata durante il training
dovremmo calcolare media e varianza su un singolo valore e non si
produrrebbe quindi nessun risultato sensato. Per superare questo problema,
la rete, durante la fase di testing, normalizza i valori di input
utilizzando media e varianza stimati durante la fase di training.
La Batch normalization si può utilizzare su \textbf{reti feed forward},
come su \textbf{reti convoluzionali} e \textbf{reti ricorrenti}. Per
le reti ricorrenti, \textit{lstm, gru o vanilla}, media e varianza
vengono calcolate per ogni step di tempo anzichè per ogni strato.
Per le reti convoluzionali media e varianza vengono calcolate per
ogni filtro. I vantaggi introdotti dalla batch normalization sono
molteplici: 
\begin{itemize}
\item Training della rete più veloce 
\item Si possono utilizzare tassi di apprendimento più alti 
\item L\textquoteright inizializzazione dei pesi della rete può essere fatta
con meno cautela 
\item Le funzioni di attivazione quali Sigmoide e Relu sono utilizzabili
anche con reti maggiormente profonde 
\item Fornisce una sorta di regolarizzazione aggiuntiva e potrebbe ridurre
la necessità di Dropout 
\item Miglioramento delle Performance in generale
\end{itemize}

\subsection{Early Stopping}

Consiste nel monitorare l'errore durante la fase di validazione. Quando
alleniamo una rete decidiamo a priori per quante epoche allenarla
(un'epoca corrisponde a sottoporre alla rete tutto il training set),
dopo un certo numero di epoche osserviamo che l'errore di validazione
che prima diminuiva inizia ad aumentare, magari oscillando anche,
questo è il caso dell'overfitting. \textbf{Early stopping} è un metodo
parametrizzato da tre fattori, quantità da monitorare, un delta che
indica il valore assoluto o percentuale di cambiamento della quantità
monitorata per determinare se c'è stato un miglioramento o peggioramento
ed infine un parametro patience che determina dopo quante epoche senza
miglioramenti fermare l'allenamento.
\begin{figure}[H]
\begin{centering}
\includegraphics{earlystopping}
\par\end{centering}
\caption{Early stopping}
\end{figure}


\subsection{Regolarizzazione loss function}

Le reti neurali, durante il loro processo di apprendimento, sfruttano,
come abbiamo visto, la funzione costo per decidere come sistemare
i propri parametri, ovvero pesi e bias. Abbiamo visto che c'è anche
un grande problema, che è quello dell'overfitting. Sono state sviluppate
alcune tecniche che, operando sulla funzione costo, aiutano a ridurre
gli effetti del sovraallenamento di una rete neurale. Tali tecniche
prendono il nome di tecniche di \textbf{regolarizzazione}. Generalmente,
tali tecniche prevedono l'aggiunta di un fattore, dipendente dai pesi,
dopo l'espressione della funzione costo. Tale fattore ha lo scopo
di introdurre un termine di penalità sulla norma di $w$ che ha l\textquoteright effetto
di restringere l\textquoteright insieme entro cui vengono scelti i
parametri. Ciò equivale, essenzialmente, ad imporre condizioni di
regolarità sulla classe di funzioni realizzata dalla rete. Sia $E_{p}(w)$
la funzione costo (o loss function, error function) non regolarizzata,
la funzione costo regolarizzata assume la forma:
\begin{equation}
E(w)=E_{p}(w)+\gamma||w||
\end{equation}
dove $||\bullet||$ denota la norma euclidea $L_{2}$. Per completezza
aggiungiamo che è possibile usare anche altri tipi di norma, ma comunemente
le più usate sono la norma $L_{1},L_{2}$. Il termine $\gamma>0$
è anch'esso un iperparametro. La regolarizzazione può essere vista
come un \textit{compromesso} tra trovare pesi piccoli e minimizzare
la funzione costo. Il parametro $\gamma$ viene detto \textbf{tasso
di regolarizzazione} e serve per determinare il bilanciamento di tale
compromesso: quando $\gamma$ è piccolo, allora preferiamo minimizzare
la funzione costo, mentre quando è grande, cerchiamo di trovare pesi
piccoli. La regolarizzazione aiuta le reti neurali a generalizzare
meglio, in quanto una rete con pesi piccoli non varia il proprio comportamento
se cambiano alcuni dei dati di input. Questo le rende particolarmente
difficile memorizzare le peculiarità dei dati, mentre la aiuta ad
apprendere meglio quelli che sono i modelli e gli schemi dei dati
di allenamento. Il principale problema della regolarizzazione è che
non si è ancora capito esattamente il perché essa aiuti a migliorare
le prestazioni di una rete neurale, ma abbiamo a disposizione solo
evidenze pratiche di questo fatto. Nonostante questo, la regolarizzazione
è ampiamente utilizzata e ci aiuta a migliorare le prestazioni delle
nostre reti neurali.

\subsection{Dropout}

La tecnica di \textbf{dropout} invece funziona diversamente, in quanto
modifica non la funzione di costo della rete, ma la rete stessa. Abbiamo
visto il principio di funzionamento di una rete neurale e come essa
riesca ad allenarsi. Ecco, questa tecnica prevede di applicare il
solito procedimento togliendo prima una certa percentuale di neuroni
in ogni hidden layer! Per ogni epoca di allenamento si sceglie casualmente
con una data probabilità (iperparametro) quali neuroni tenere e quali
scartare e si allena la rete così ottenuta. Si ripete quindi il procedimento,
tenendo e scartando neuroni diversi ad ogni epoca: una volta che si
ritiene che la rete sia pronta, si prende la rete originale e si aggiustano
i pesi uscenti dai neuroni nascosti: abbiamo ottenuto una rete pronta
a svolgere il proprio compito. In poche parole, è come se usassimo
tante reti diverse e poi prendessimo come risultato la media di tutti
i risultati di queste reti. Va tenuto ben presente che questo procedimento
è applicato solo in fase di allenamento: durante il funzionamento
vero e proprio, la rete è considerata nella sua interezza.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{Droout_}
\par\end{centering}
\caption{Dropout: ad ogni epoca, poi, faremo in modo di avere una rete diversa
ogni volta, considerando neuroni diversi.}

\end{figure}


\subsection{Inizializzazione dei pesi}

Prima di partire con l'allenamento i pesi vanno inizializzati. Osserviamo
che non sappiamo quale sarà il valore finale di ogni peso al termine
dell'allenamento, ma se ad esempio normalizziamo i dati di input è
ragionevole supporre che circa la metà dei pesi sarà positiva e la
metà negativa, quindi con media tendente a zero. 
\begin{itemize}
\item Una prima idea di \textbf{inizializzazione} è quella di porre tutti
i pesi a \textbf{zero}. Pessima idea, infatti avendo tutti i pesi
uguali, ogni neurone calcolerà lo stesso output e lo stesso gradiente,
quindi i pesi subiranno tutti gli stessi aggiornamenti e i pesi finali
resteranno tutti uguali tra loro. Questo è un problema, supponiamo
che la nostra rete sia un classificatore di immagini, avere tutti
i pesi uguali significa che ogni pixel di ogni immagine abbia la stessa
importanza degli altri, quindi la rete non impara ad estrarre e riconoscere
nessuna feature.
\item \textbf{Inizializzazione con numeri} grandi: anche questa è una pessima
idea, a seconda della funzione di attivazione del neurone abbiamo
diversi comportamenti ma tutti problematici. Come detto prima, i pesi
grandi peggiorano la capacità di generalizzazione della rete, inoltre
se usassimo funzioni di attivazione come softmax o tangente iperbolica
potremmo essere fin da subito nelle zone di saturazione, zone in cui
il gradiente tende a zero bloccando di fatto l'apprendimento.
\item \textbf{Inizializzare i pesi intorno allo zero}: abbandonata l'idea
di inizializzare con numeri grandi, vogliamo che i pesi siano molto
vicini allo zero, non identicamente nulli e distribuiti in modo che
circa la metà sia positiva e metà sia negativa. Una pratica comune
è inizializzare i pesi campionandoli da una distribuzione gaussiana
a media nulla e varianza piccola (0.01). Questo tipo di inizializzazione
può andare bene se la rete neurale dispone di pochi strati nascosti,
ma in reti neurali più profonde le attivazioni diventano sempre più
piccole mano a mano che si processano i vari strati, fino ridursi
a quantità praticamente nulle. Questo diventa un problema in quanto
durante la fase di back propagation il gradiente accumulato continua
ad essere moltiplicato per quantità piccolissime che portano alla
sua dissolvenza.
\item \textbf{Inizializzazione di Xavier}: l'idea è quindi quella di avere
una distribuzione delle attivazioni tale che la rete neurale sia in
grado di apprendere in maniera efficiente. In quest'ottica, Xavier
(2010) ha proposto una inizializzazione dei pesi secondo una normale
a media nulla con deviazione standard tale che la varianza delle attivazioni
$a^{(k)}$ risulti essere unitaria. Sotto l'assunzione di attivazioni
lineari e simmetriche (plausibile dal momento che anche la tangente
iperbolica ha proprio questo comportamento intorno allo zero) questo
si traduce nel rendere unitaria la varianza degli input $z^{(k)}$.
L'inizializzazione di Xavier va eseguita neurone per neurone a partire
dallo strato iniziale, per questo è necessario preprocessare l'input
della rete come descritto sopra (input a media nulla e varianza unitaria)
quando vogliamo utilizzare questa inizializzazione. Supponiamo di
avere un input $X$ di dimensione $n$ e lo strato, composto da $n$
neuroni sia solamente connesso tramite pesi casuali $W$ di dimensione
$n\times n$, l'output $Y$ dell'i-esimo neurone, di dimensione $n$
anch'esso sarà:
\begin{equation}
Y_{i}=X_{1}W_{i,1}+X_{2}W_{i,2}+...+X_{n}W_{i,n}
\end{equation}
dove $i\in[1,n]$ e indica l'i-esimo neurone e $W_{i}$ il vettore
dei pesi associati. La varianza del prodotto di due variabili aleatorie
è:
\[
Var(W_{i}X_{i})=E[X_{i}]^{2}Var(W_{i})+E[W_{i}]^{2}Var(X_{i})+Var(W_{i})Var(X_{i})
\]
Per ipotesi sia l'input sia i pesi sono a media nulla, semplificando:
\[
Var(W_{i}X_{i})=Var(W_{i})Var(X_{i})
\]
Inoltre assumiamo che $X_{i},W_{i}$ siano indipendenti ed identicamente
distribuite (iid), allora:
\[
Var(Y_{i})=Var(X_{1}W_{i,1}+X_{2}W_{i,2}+...+X_{n}W_{i,n})
\]
\begin{equation}
=nVar(W_{i})Var(X_{i})
\end{equation}
Assumento $Var(X_{1})=1$ perché l'input può essere o i dati di allenamento
già normalizzati o essere l'output dello strato precedente in cui
i pesi sono già stati inizializzati correttamente. Imponiamo quindi:
\[
nVar(W_{i})=1
\]
\[
Var(W_{i})=\frac{1}{n}=\frac{1}{n_{in,i}}
\]
Quindi in definitiva Xavier propone di inizializzare i pesi campionandoli
nel modo seguente:
\begin{equation}
W_{i}\sim N(0,\frac{1}{n_{in,i}})
\end{equation}
Fino ad adesso abbiamo assunto l'ipotesi che la funzione di attivazione
sia lineare e simmetrica intorno allo 0. Usando la funzione ReLU cade
l'ipotesi di simmetria e la distribuzione va adattata. A tale scopo
recentemente è stata proposta l'inizializzazione: 
\begin{equation}
W_{i}\sim N(0,\frac{2}{n_{in,i}})
\end{equation}
Il fattore di correzione 2 ha senso: la ReLU dimezza di fatto gli
input, e pertanto bisogna raddoppiare la varianza dei pesi per mantenere
la stessa varianza delle attivazioni.
\item \textbf{Glorot \& Bengio} seguendo un simile ragionamento a quello
di Xavier ma applicato alla backpropagation, quindi procedendo a ritroso
dall'ultimo strato al primo, invertendo anche il ruolo di input e
output, hanno trovato che deve valere:
\[
n_{in,i}Var(W_{i})=1
\]
\[
n_{out,i}Var(W_{i})=1
\]
Imporre le due condizioni simultaneamente risulta essere troppo restrittivo,
imporrebbe infatti $n_{in,i}=n_{out,i}$ che nel caso di una rete
feedforward totalmente connessa significa che tutti gli strati hanno
la medesima ampiezza e anche in altre tipologie di reti è una forte
imposizione sull'architettura. Si è raggiunto un compromesso tra i
due vincoli usando una distribuzione
\begin{equation}
W_{i}\sim N(0,\frac{2}{n_{in,i}+n_{out,i}})
\end{equation}
\end{itemize}

\subsection{Data augmentation}

Si può anche pensare di \textbf{ampliare} artificialmente\textbf{
i dati di allenamento}, in quanto ottenere nuovi dati per allenare
la rete è sempre una buona idea. Il problema è che non sempre è possibile,
oppure è troppo costoso ottenerne di nuovi. Quindi se ne generano
di nuovi a partire da quelli che abbiamo già a disposizione. Ad esempio,
se la nostra rete dovesse riconoscere delle cifre scritte a mano,
potremmo applicare delle piccole rotazioni o delle lievi dilatazioni
o restrizioni ai dati che abbiamo già in possesso, creando delle immagini
nuove da fornire alla nostra rete neurale. In generale, si cerca di
espandere il set di allenamento cercando di riprodurre quelle che
sono le variazioni che di solito hanno nella pratica.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.75]{Data_aug}
\par\end{centering}
\caption{Data augmentation: Nonostante la differenza sia minima, per l'analisi
svolta dalla rete sono due immagini significativamente diverse.}
\end{figure}


\section{Scelte di non linearità}

Possiamo ora analizzare più nel dettaglio funzioni di attivazioni
accennate nel capitolo 1 avendo ora maggiori conoscienze sull'ottimizzazione.

\subsection{Funzione Sigmoide}

Tale soluzione è stata progressivamente accantonata negli ultimi anni
per via di alcune problematiche che comporta a livello pratico. 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{grafico_sig}\caption{Grafico Sigmoide}
\par\end{centering}
\end{figure}
\[
f(x)=\frac{1}{1+e^{-x}}
\]
La prima e più importante è quella relativa alla dissolvenza del gradiente
in seguito alla \textit{saturazione} dei neuroni, ossia quei neuroni
che presentano valori di output agli estremi del codominio della funzione
di attivazione, in questo caso $(0,1)$. É facile infatti notare che
\[
\lim_{x\rightarrow+\infty}f(x)=1
\]
\[
\lim_{x\rightarrow-\infty}f(x)=0
\]
Tale saturazione diventa problematica durante le fase di back propagation,
in quanto il gradiente locale assume valori prossimi allo zero, che
per la \textit{regola della catena} vanno a moltiplicare tutti i gradienti
calcolati in precedenza e quelli successivi, conducendo così all'annullamento
del gradiente globale.
\[
f'(x)=f(x)(1-f(x))
\]
\[
\lim_{x\rightarrow+\infty}f'(x)=0
\]
\[
\lim_{x\rightarrow-\infty}f'(x)=0
\]
In pratica, si ha un flusso utile del gradiente solo per valori di
input che rimangono all'interno di una zona di sicurezza, cioè nei
dintorni dello zero. Il secondo problema deriva invece dal fatto che
gli output della funzione sigmoide non sono centrati intorno allo
zero, e di conseguenza gli strati processati successivamente riceveranno
anch'essi valori con una distribuzione non centrata sullo zero. Questo
influisce in maniera significativa sulla discesa del gradiente, in
quanto gli input in ingresso ai neuroni saranno sempre positivi, e
pertanto il gradiente dei pesi associati diventerà, durante la fase
di back propagation, sempre positivo o sempre negativo. Tale risultato
si traduce in una dinamica a zig-zag negli aggiornamenti dei pesi
che rallenta in maniera signicativa il processo di convergenza.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{\string"zigzag gradiente\string".png}
\par\end{centering}
\caption{Esempio di dinamica a zig-zag nell'aggiornamento di due pesi $W_{1}$
e $W_{2}$. La freccia blu indica indica l'ipotetico vettore ottimale
per la discesa del gradiente, mentre le frecce rosse i passi di aggiornamento
compiuti: gradienti tutti dello stesso segno comportano due sole possibili
direzioni di aggiornamento.}
\end{figure}
É importante comunque notare che una volta che i gradienti delle singole
osservazioni vengono sommati all'interno dello stesso batch di dati
l'aggiornamento finale dei pesi può avere segni diversi, permettendo
quindi di muoversi lungo un insieme più ampio di direzioni. 

Il terzo e ultimo difetto della funzione sigmoide è che l'operazione
exp(·) al denominatore è molto costosa dal punto dal punto di vista
computazionale, soprattutto rispetto alle alternative che verranno
presentate di seguito.

\subsection{Tangente iperbolica}

Il problema degli output non centrati sullo zero della sigmoide può
essere risolto ricorrendo all'utilizzo della tangente iperbolica,
la quale presenta codominio (\textminus 1, 1) centrato sull'origine
degli assi.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{tangente-iperbolica}
\par\end{centering}
\caption{Grafico tangente iperbolica}
\end{figure}
\[
f(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
\]
Tuttavia, rimane il problema della \textit{saturazione} dei neuroni,
anzi viene addirittura accentuato, dal momento che la zona di sicurezza
risulta ancora più ristretta. Rimane anche il problema della complessità
computazionale della funzione esponenziale.

\subsection{ReLU}

La \textbf{Rectifier Linear Unit} (ReLU) è diventata popolare negli
ultimi anni per via dell'incremento prestazionale che offre nel processo
di convergenza: velocizza infatti di circa 6 volte la discesa del
gradiente rispetto alle alternative viste finora.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{Grafico_relu}
\par\end{centering}
\caption{Grafico ReLU}

\end{figure}
\[
f(x)=x^{+}=max(0,x)
\]
Questo risultato è da attribuire in larga parte al fatto che la ReLU
risolve il \textit{problema della dissolvenza} del gradiente, non
andando a saturare i neuroni. Durante la fase di back propagation
infatti, se il gradiente calcolato fino a quel punto è positivo questo
viene semplicemente lasciato passare, perchè la derivata locale per
il quale viene moltiplicato è pari ad uno.
\[
f'(x)=\begin{cases}
1 & x>0\\
0 & x<0
\end{cases}
\]
\textbf{NB} durante la back propagation l'input $x$ è il gradiente
proveniente dagli strati successivi.

Eventuali problemi sorgono invece quando il gradiente accumulato ha
segno negativo, in quanto questo viene azzerato (la derivata locale
è nulla lungo tutto il semiasse negativo) con la conseguenza che i
pesi non vengono aggiornati. Fortunatamente questo problema può essere
alleviato attraverso l'utilizzo di un algoritmo SGD: considerando
più dati alla volta c'è infatti la speranza che non tutti gli input
del batch provochino l'azzeramento del gradiente, tenendo così in
vita il processo di apprendimento del neurone. Al contrario, se per
ogni osservazione la ReLU riceve valori negativi, allora il neurone
\textquotedbl muore\textquotedbl , e non c'è speranza che i pesi
vengano aggiornati. Valori elevati del learning rate amplificano questo
problema, dal momento che cambiamenti più consistenti dei pesi si
traducono in una maggiore probabilità che questi affondino nella \textquotedbl zona
morta\textquotedbl . 

\subsection{Leaky ReLU}

La \textbf{leaky ReLU} è un tentativo di risolvere il problema della
disattivazione dei neuroni comportato dalla ReLU classica, e consiste
nell'introdurre una piccola \textbf{pendenza negativa} (di circa 0.01)
$\alpha$ nella regione dove la ReLU è nulla, dove $\alpha$ è costante.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{Leaky_relu}
\par\end{centering}
\caption{Grafico Leaky ReLU}
\end{figure}
\[
f(x)=\max(\alpha x,x)
\]
In alcune varianti, \textgreek{a} può essere un parametro da stimare,
al pari degli altri pesi della rete (si parla di \textbf{Parametric
ReLU} ), oppure una variabile casuale: è il caso della \textbf{Randomized
ReLU}, dove ad ogni iterazione la pendenza della parte negativa della
funzione viene scelta casualmente all'interno di un range pressato.
In alcuni recenti esperimenti, Bing Xu et al. (2015) hanno mostrato
come le varianti della ReLU classica siano in grado di aumentare le
performance finali del modello in termini di accuratezza, prima su
tutte la RReLU, che grazie alla sua natura casuale sembra particolarmente
portata alla riduzione del sovradattamento.

\chapter{Classificazioni di immagini}

La visione artificiale (nota anche come \textbf{computer vision})
è l'insieme dei processi che mirano a creare un modello approssimato
del mondo reale (3D) partendo da immagini bidimensionali (2D). Vedere
è inteso non solo come l'acquisizione di una fotografia bidimensionale
di un'area ma soprattutto come l'interpretazione del contenuto di
quell'area. L'informazione è intesa in questo caso come qualcosa che
implica una decisione automatica. Un problema classico nella visione
artificiale è quello di determinare se l'immagine contiene o no determinati
oggetti (Object recognition) o attività. Il problema può essere risolto
efficacemente e senza difficoltà per oggetti specifici in situazioni
specifiche per esempio il riconoscimento di specifici oggetti geometrici
come poliedri, riconoscimento di volti o caratteri scritti a mano.
Le cose si complicano nel caso di oggetti arbitrari in situazioni
arbitrarie. 

Nella letteratura troviamo differenti varietà del problema:
\begin{itemize}
\item \textbf{Recognition} (riconoscimento): uno o più oggetti prespecificati
o memorizzati possono essere ricondotti a classi generiche usualmente
insieme alla loro posizione 2D o 3D nella scena.
\item \textbf{Identification} (identificazione): viene individuata un'istanza
specifica di una classe. Es. Identificazione di un volto, impronta
digitale o veicolo specifico.
\item \textbf{Detection} (rilevamento): l'immagine è scandita fino all'individuazione
di una condizione specifica. Es. Individuazione di possibili cellule
anormali o tessuti nelle immagini mediche.
\end{itemize}
Altro compito tipico è la ricostruzione dello scenario: dati 2 o più
immagini 2D si tenta di ricostruire un modello 3D dello scenario.
Nel caso più semplice si parla di un insieme di singoli punti in uno
spazio 3D o intere superfici. Generalmente è importante trovare la
matrice fondamentale che rappresenta i punti comuni provenienti da
immagini differenti.

Il problema della \textbf{classificazione di immagini} è il compito
di assegnare ad un'immagine di input una e una sola etichetta proveniente
da un insieme fissato di output.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{esempio_classificazione}
\par\end{centering}
\caption{Classificazione di un'immagine}
\end{figure}
La classificazione presenta alcuni problemi:
\begin{itemize}
\item Variazione punto di vista (\textit{Viewpoint variation}): una singola
istanza di un oggetto può essere orientata in modi diversi rispetto
alla camera, producendo immagini diverse;
\item Variazione scala (\textit{Scale variation}): oggetti diversi appartenenti
alla medesima classe possono differire nelle loro dimensioni reali;
\item Deformazione (\textit{Deformation}): alcuni oggetti non sono corpi
rigidi e possono apparire deformati in modi diversi;
\item Occlusione (\textit{Occlusion}): parti dell'oggetto da riconoscere
è nascosto e non visibile;
\item Condizioni di illuminazione (\textit{Illumination conditions}): l'illuminazione
ha un ruolo decisivo nell'informazione codificata all'interno dei
pixel che compongono l'immagine;
\item Disordine di sfondo (\textit{Background clutter}): gli oggetti di
interesse possono mescolarsi e confondersi nell'ambiente circostante,
rendendo difficile l'identificazione;
\item Variazione intra-classe (\textit{Intra-class variation}): oggetti
appartenti alla stessa classe possono differire significativamente
l'uno dall'altro.
\end{itemize}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{Problem_image_class}
\par\end{centering}
\caption{Problemi nella classificazione di immagini}
\end{figure}


\section{Dataset CIFAR-10}

In queste note si farà riferimento al dataset\textbf{ CIFAR-10} che
consiste in 60000 immagini di dimensione $32\times32$ pixels, ogni
pixel ha associato 3 numeri, uno per ogni colore (RGB). Ogni immagine
è etichettata con una di 10 classi, Queste 60000 immagini sono partizionate
in \textit{training set} di 50000 immagini e \textit{test set} di
10000 immagini. Ogni immagine può quindi essere vista come un vettore
appartenente allo spazio $\mathbb{R}^{32\times32\times3}$.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{cifar10}
\par\end{centering}
\caption{CIFAR-10 daset}

\end{figure}


\section{Nearest Neighbor Classifier}

Questo classificatore non ha nulla a che fare con le reti neurali
e non viene quasi mai usato nella pratica, ma ci permetterà di avere
un'idea dell'approccio di base a un problema di classificazione delle
immagini. L'idea di questo classificatore è molto semplice, la fase
di training consiste nel memorizzare tutte le immagini del training
set, la classificazione avviene confrontando l'immagine di test con
ogni immagine del training set, quindi si etichetta l'immagine in
accordo con l'etichetta dell'immagine che più assomiglia. Cosa intendiamo
quando diciamo \textquotedbl\textit{che più assomiglia}\textquotedbl ?
Occorre quindi formalizzare questo concetto secondo una qualche metrica.
Ogni immagine può essere vista come una matrice in cui ogni elemento
è un valore numerico che rappresenta l'intensità di un colore appartenente
allo spazio RGB di un singolo pixel. Definiamo quindi tre distanze:
\begin{enumerate}
\item \textbf{L1 distance: 
\begin{equation}
d_{1}\left(I_{1},I_{2}\right)=\sum_{p}\mid I_{1}^{p}-I_{2}^{p}\mid
\end{equation}
}la distanza è calcolata sommando il modulo delle differenze elemento
per elemento. è anche nota come distanza di Manhattan.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{L1Distance}
\par\end{centering}
\caption{L1 distance}

\end{figure}
\item \textbf{L2 distance:}
\begin{equation}
d_{2}\left(I_{1},I_{2}\right)=\sqrt{\sum_{p}\left(I_{1}^{p}-I_{2}^{p}\right)^{2}}
\end{equation}
 è la distanza euclidea classica.
\item \textbf{L$_{k}$ distance:}
\begin{equation}
d_{k}\left(I_{1},I_{2}\right)=\left(\sum_{p}\left(\mid I_{1}-I_{2}\mid\right)^{k}\right)^{\frac{1}{k}}
\end{equation}
con $k\in[1,+\infty).$ E' una generalizzazione delle precedenti.
\end{enumerate}
Le distanze comunemente usate sono le prime due, L1 e L2. In altre
parole \textbf{Nearest Neihbor Classifier} è ricondotto ad un problema
di minimizzazione riformulando il problema di classificazione come
segue:
\begin{itemize}
\item chiamiamo $\boldsymbol{x}_{i}$ l'i-esima immagine di test (da etichettare);
\item chiamiamo $\boldsymbol{x}_{j}$ la j-esima immagine del training set
(già etichettata);
\item chiamiamo $y_{j}$ l'etichetta della j-esima immagine del training
set;
\item poniamo $y_{i}=y_{j^{*}}$ con $j^{*}=\mathrm{argmin}\left(d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\right)$.
\end{itemize}
Da test effettuati risulta che Nearest Neihbor Classifier, usando
la distanza L1, ha classificato correttamente il 38,6\% delle immagini
del dataset CIFAR-10. Un risultato ragguardevole se comparato alla
probabilità di una corretta classificazione assegnando casualmente
un'etichetta (10\% nel nostro caso) ma ben lontano dalle prestazioni
umane e dalle migliori reti neurali convuluzionali. Utilizzando la
distanza L2 invece si è ottenuto un'accuratezza del 35,4\%. 

\section{K-Nearest Neighbor Classifier}

Questo classificatore è un estensione del precedente e si basa su
un'idea molto semplice: invece di assegnare l'etichetta dell'immagine
più vicina (rispetto ad una definita distanza), troviamo le \textbf{k
immagini più vicine} e assegnamo l'etichetta che compare maggiormente
nelle \textit{k} etichette. In particolare per $k=1$ otteniamo \textit{Nearest
Neihbor Classifier} precedente. Intuituivamente un alto valore di
\textit{k }ha un effetto \textquotedbl levigante\textquotedbl{} sui
confini decisionali e rende il classificatore più resistente ai valori
anomali. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{k-NN}
\par\end{centering}
\caption{confronto NN classifier e 5-NN classifier}
\end{figure}
 La figura 2.5 mostra un confronto tra K-NN e NN, usando punti bidimensionali
come dati da classificare in 3 classi (rosso, blue, verde). Le regioni
colorate evidenziano i confini decisionali. Le regioni bianche mostrano
punti la cui classificazione è ambigua (per esempio in K-NN nel caso
in cui ci sia parità tra due o più classi). Notiamo come nel caso
di punti anomali, NN crea piccole isole di probabili previsioni errate,
mentre 5-NN smussa queste irregolarità.

\paragraph{Vantaggi e svantaggi K-NN}

Tra i vantaggi si sottolinea:
\begin{itemize}
\item Facile da capire e implementare
\item Training in tempo costante $O(1)$ difatti basta salvare il riferimento
al training set.
\end{itemize}
Tra gli svantaggi si sottolinea:
\begin{itemize}
\item Complessità temporale: richiede il confronto di ogni immagine di test
con tutte quelle appartenti al trainin set. La complessità dipende
linearmente dalla dimensione del training set e test set;
\item Elevata complessità spaziale: richiede che tutto il training set sia
memorizzato.
\item La distanza tra immagini non è sempre signicativa come mostrato dalla
seguente immagine.
\end{itemize}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{distance_image}
\par\end{centering}
\caption{Distanza tra immagini, le 3 immagini a sinistra hanno tutte la stessa
distanza dall'originale}
\end{figure}
si pone ora il problema di come scegliere il valore k, come scegliere
il tipo di distanza da usare. Soluzione \textbf{cross-validation. }

\section{Classificatore Lineare (Linear Classifier)}

Un \textbf{classificatore} può anche essere visto come una funzione
che associa ad un'immagine $\boldsymbol{x}$ un vettore le cui componenti
sono il punteggio associato ad ogni classe. Intuitivamente un buon
classificatore associa alla classe corretta un punteggio più alto
rispetto alle classi incorrette. Più formalmente:

\begin{equation}
\mathcal{\boldsymbol{F}:\mathbb{\mathbb{\mathbb{R^{\mathrm{\mathit{d}}}}}}\rightarrow}\mathbb{R}^{L}
\end{equation}
 dove $d$ è la dimensione di una immagine ($32\times32\times3$ nel
caso CIFAR-10), $L$ è la cardinalità dell'insieme delle etichette.
Quindi $\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}}\right)$
è un vettore $L$- dimensionale e la $i$-esima componente $s_{i}=\left[\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}}\right)\right]_{i}$
contiente il punteggio di quanto probabile sia l'appartenenza di $\boldsymbol{x}$
alla classe $i$. Per il momento non abbiamo detto nulla su come è
fatta la funzione vettoriale $\mathcal{\boldsymbol{F}:\mathbb{\mathbb{\mathbb{R^{\mathrm{\mathit{d}}}}}}\rightarrow}\mathbb{R}^{L}$.
Nel classificatore lineare $\boldsymbol{F}$ è una funzione lineare:
\begin{equation}
\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}\mid}W,b\right)=W\boldsymbol{x}+b
\end{equation}
dove $W\in\mathbb{\mathbb{R}}^{L\times d}$ è chiamata \textit{matrice
dei pesi}, $b\in\mathbb{R}^{L}$è chiamato \textit{bias} e sono entrambi
parametri. Prima di essere classificata un'immagine necessita di una
pre elaborazione, denominata \textit{unroll} che consiste nel espandere
la matrice di pixel $I\in\mathbb{R}^{h\times w}$ in un vettore $\boldsymbol{x}\in\mathbb{R}^{(h\cdot w\cdot3)}$
in cui ogni componente del vettore rappresenta l'intensità di uno
specifico colore RGB di un singolo pixel. 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{Unrollpicture}
\par\end{centering}
\caption{Unroll immagine}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{linear_class}
\par\end{centering}
\caption{Linear Classifier}
\end{figure}
Il \textbf{classificatore lineare} assegna ad un'immagine di input
la classe corrispondente al più alto punteggio: 
\begin{equation}
\hat{y_{j}}=arg\max_{i=1,...,L}[s_{j}]_{i}.
\end{equation}
 Nel caso CIFAR-10 ogni immagine viene trasformata in un vettore di
$[3072\times1]$, la matrice $W$ ha una dimensione di $[10\times3072]$,
$b$ ha dimensione $[10\times1]$. 
\begin{itemize}
\item Notiamo che una singola moltiplicazione $W\boldsymbol{x}_{i}$ corrisponde
(nel caso specifico) a 10 classificazioni parallele, in cui ogni riga
della matrice $W$ corrisponde a un classificatore, quindi all'importanza
che ogni pixel possiede per la i-esima classe. 
\item Durante la fase di training abbiamo i dati di input $\left(\boldsymbol{x}_{i},y_{i}\right)$
già classificati e fissati, ma noi abbiamo il controllo sui parametri
$W,b$ e il nostro obiettivo è trovare quei lavori che massimizzano
la classificazione corretta.
\item Rispetto a Nearest Neihbor Classifier una volta terminata la fase
di training il set di immagini di training può essere eliminato, è
necessario solo memorizzare i parametri $W,b.$
\item Inoltre questo classificatore lineare coinvolge solo un prodotto matriciale
che è molto più veloce del confronto di un'immagine con tutto il training
set. 
\end{itemize}

\paragraph{Loss Function}

Dopo aver visto formalmente cosa è un Linear Classifier, cioè una
funzione lineare $\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}\mid}W,b\right)=W\boldsymbol{x}+b$,
ci poniamo ora il problema di trovare i giusti parametri $W,b$ che
rendano consistente e migliore la classificazione. Per fare ciò introduciamo
una funzione in grado di fornirci una misura di quanto la nostra classificazione
sia <<infelice>> (unhappiness), cioè lontana da un risultato corretto.
Si tratterà quindi di una funzione dell'input $\boldsymbol{x}$ e
parametrizzata dai parametri $W,b$ che andrà minimizzata. In letteratura
è nota come \textbf{loss function} (o altre volte \textbf{cost function}).

\begin{equation}
\mathcal{L}\left(\boldsymbol{x},y_{i}|W,b\right)
\end{equation}


\paragraph{Bias trick}

Modifichiamo ora la score function alleggerendo la notazione eliminando
il termine $\boldsymbol{b}$. Richiamiamo la (9) $\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}\mid}W,b\right)=W\boldsymbol{x}+b$,
come evidente risulta scomodo mantenere due set di parametri (il bias
$\boldsymbol{b}$ e i pesi $W$) separati. L'idea è di includere il
vettore di bias nella matrice $W$. Sia:
\begin{equation}
W=\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n}
\end{bmatrix},\boldsymbol{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{bmatrix},\boldsymbol{b}=\begin{bmatrix}b_{1}\\
b_{2}\\
\vdots\\
b_{n}
\end{bmatrix}
\end{equation}
allora la (9) diventa:
\begin{equation}
\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}\mid}W,b\right)=W\boldsymbol{x}+b=\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n}
\end{bmatrix}\cdot\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{bmatrix}+\begin{bmatrix}b_{1}\\
b_{2}\\
\vdots\\
b_{n}
\end{bmatrix}=\begin{bmatrix}s_{1}\\
s_{2}\\
\vdots\\
s_{n}
\end{bmatrix}
\end{equation}
 dove $s_{i}$ rappresenta il punteggio della classe i-esima. Dalla
definizione di prodotto riga per colonna notiamo che la (11) è equivalente
a:

\begin{equation}
\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n} & b_{1}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n} & b_{2}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n} & b_{n}
\end{bmatrix}\cdot\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}\\
1
\end{bmatrix}=\begin{bmatrix}s_{1}\\
s_{2}\\
\vdots\\
s_{n}
\end{bmatrix}
\end{equation}
dove abbiamo esteso la matrice $W$ aggiungendo una colonna contenente
$\boldsymbol{b}$ e esteso il vettore $\boldsymbol{x}$ aggiungendo
1 in posizione $(n+1)$. Rinominando i componenti della matrice $W$
giungiamo alla formula definitiva:
\begin{equation}
\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}\mid}W\right)=\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n} & w_{1,n+1}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n} & w_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n} & w_{m,n+1}
\end{bmatrix}\cdot\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}\\
1
\end{bmatrix}=\begin{bmatrix}s_{1}\\
s_{2}\\
\vdots\\
s_{n}
\end{bmatrix}
\end{equation}
nel nostro esempio CIFAR-10, $\boldsymbol{x}$ è adesso $[3073\times1]$
invece di $[3072\times1]$ e $W$ è $[10\times3073]$ invece di $[10\times3072]$.

\subsection{Interpretazione di un Classificatore Lineare.}

Linear classifier, data in input una immagine (nel caso più generale
dato un generico vettore di dati) calcola il punteggio di una classe
come una somma pesata di ogni pixel attraverso tutti e tre i canali
colori (RGB). Ogni riga della matrice $W$ contiene i pesi per mappare
i tre canali colore di ogni pixel nel punteggio di una classe (una
riga per classe), quindi in definitiva la funzione lineare ha la capacità
di approvare o disapprovare (dipendente dal segno di ciascun $w_{i,j})$
un certo colore in una certa posizione. Per esempio, generalmente,
un'immagine della classe \textquotedbl ship\textquotedbl{} ci aspettiamo
abbia un'alta presenza di colore blu ai lati dell'immagine, mentre
abbia molto meno altri colori.

\subsubsection{Interpretazione Geometrica}

Poichè trattiamo le nostre immagini di input come vettori colonna
possiamo considerare ogni immagine come un punto nello spazio, nell'esempio
CIFAR-10 lo spazio è $\mathbb{R}^{3072}$ (escludendo il bias trick).
Inoltre dall'algebra lineare sappiamo che l'equazione generica di
un \textbf{iperpiano} (sottospazio affine di $\mathbb{R}^{n}$ di
dimensione $n-1$ ) è $\varSigma:a_{1}x_{1}+a_{2}x_{2}+\ldots+a_{n}x_{n}+b=0$
che riscritto in forma più compatta: $\varSigma:<\boldsymbol{a},\boldsymbol{x}>+b=0$
dove $<\cdot,\cdot>$ è il prodotto scalare. Osserviamo che il prodotto
$W\boldsymbol{x}$ corrisponde a eseguire $m$ prodotti scalari, quindi
$\boldsymbol{F}\left(\boldsymbol{\text{\ensuremath{x}}\mid}W,b\right)=W\boldsymbol{x}+b=0$
definisce $m$ iperpiani nello spazio. Ricordando ora che la distanza
di un generico punto $\boldsymbol{P}$ da un iperpiano $\varSigma$
è: 
\begin{equation}
d(\varSigma,\boldsymbol{P})=\frac{|<\boldsymbol{a},\boldsymbol{P}>+b|}{\parallel\boldsymbol{a}\parallel}
\end{equation}
osserviamo che la distanza è direttamente proporzionale al prodotto
scalare dei coefficienti dell'iperpiano con il punto (in modulo, il
segno determina se siamo \textit{sopra} o \textit{sotto} l'iperpiano).
Possiamo vedere ogni iperpiano come il confine di una regione di accettazione,
il punteggio $s_{i}$ esprime tramite il segno se siamo nella regione
di accettazione o no, e tramite il modulo quanto siamo lontani dal
confine, quindi un alto punteggio positivo indica un'alta confidenza
che quella immagine appartenga alla classe i-esima, viceversa un alto
punteggio negativo indica una bassissima confidenza.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{interpretazione_geometrica}
\par\end{centering}
\caption{Interpretazione geomtrica nello spazio bidimensionale}
\end{figure}


\subsubsection{Template}

Un'altra possibile interpretazione per i pesi $W$ è che ogni riga
della matrice corrisponda a un \textbf{template} (o prototipo) per
ogni classe. Il punteggio di ogni classe è ottenuto tramite prodotto
scalare e indica il grado di somiglianza tra l'immagine test e il
template.

\chapter{Reti Convoluzionali}

Le \textbf{reti neurali convoluzionali} (\textbf{Convolutional Neural
Networks, CNN}) sono di fatto delle reti neurali artificiali. Esattamente
come queste ultime infatti, anche le reti neurali convoluzionali sono
costituite da neuroni collegati fra loro tramite dei rami pesati (weight);
i parametri allenabili delle reti sono sempre quindi i \textbf{weight}
ed i \textbf{bias}. Tutto quanto detto in precedenza sull\textquoteright allenamento
di una rete neurale, cioè forward/backward propagation e aggiornamento
dei pesi, vale anche in questo contesto; inoltre un\textquoteright intera
rete neurale convoluzionale utilizza sempre una singola funzione di
costo differenziabile. Tuttavia le reti neurali convoluzionali fanno
la specifica assunzione che il loro input abbia una precisa struttura
di dati, come ad esempio un\textquoteright immagine nel nostro caso,
e ciò permette ad esse di assumere delle specifiche proprietà nella
loro architettura al fine di elaborare al meglio tali dati. Ad esempio
di poter effettuare delle forward propagation più efficienti in modo
da ridurre l\textquoteright ammontare di parametri della rete. Il
nome Convolutional Neural Networks deriva dal fatto che tali reti
utilizzano un'operazione matematica lineare chiamata \textbf{convoluzione}.
Gli strati composti da operazioni di convoluzione prendono il nome
di \textbf{Convolutional Layers}, ma non sono gli unici strati che
compongono una CNN: la tipica architettura prevede infatti l'alternarsi
di \textbf{Convolutional Layers, Pooling Layers e Fully Connected
Layers.}

\section{Inadeguatezza della struttura fully-connected}

Come visto nel capitolo precedente, le reti neurali tradizionali ricevono
in input un singolo vettore, e lo trasformano attraverso una serie
di strati nascosti, dove ogni neurone è connesso ad ogni singolo neurone
sia dello strato precedente che di quello successivo (ovvero \textquotedbl\textit{fully-connected}\textquotedbl )
e funziona quindi in maniera completamente indipendente, dal momento
che non vi è alcuna condivisione delle connessioni con i nodi circostanti.
Nel caso l'input sia costituito da immagini di dimensioni ridotte,
ad esempio $32\times32\times3$ (32 altezza, 32 larghezza, 3 canali
colore), un singolo neurone connesso in questa maniera comporterebbe
un numero totale di 32 \texttimes{} 32 \texttimes{} 3 = 3072 pesi,
una quantità abbastanza grande ma ancora trattabile. Le cose si complicano
però quando le dimensioni si fanno importanti: salire ad appena 256
pixel per lato comporterebbe un carico di $256\times256\times3=196'608$
pesi per singolo neurone, ovvero quasi 2 milioni di parametri per
una semplice rete con un singolo strato nascosto da dieci neuroni.
L'architettura fully-connected risulta perciò troppo esosa in questo
contesto, comportando una quantità enorme di parametri che condurrebbe
velocemente a casi di sovradattamento. Inoltre una rete FC è del tutto
generica e non ha nessun tipo di accorgimento per l'elaborazione di
immagini che presentano una struttura ben determinata. Le Convolutional
Neural Networks prendono invece vantaggio dall'assunzione che gli
input hanno proprio una struttura di questo tipo, e questo permette
loro la costruzione di un'architettura su misura attraverso la formalizzazione
di tre fondamentali proprietà: \textbf{l'interazione sparsa} (\textbf{sparse
interaction}), l\textbf{'invarianza rispetto a traslazioni} (\textbf{invariant
to translation}), e la \textbf{condivisione dei parametri} (\textbf{weight
sharing}). Il risultato è una rete più effcace e allo stesso tempo
parsimoniosa in termini di parametri.

\section{Operazione di convoluzione}

In problemi discreti l'operazione di convoluzione non è altro che
la somma degli elementi del prodotto di Hadamard fra un set di parametri
(che prende il nome di filtro o kernel) e una porzione dell'input
di pari dimensioni. L'operazione di convoluzione viene quindi ripetuta
spostando il filtro lungo tutta la supercie dell'input, sia in altezza
che in larghezza. Questo produce quella che viene chiamata mappa di
attivazioni (o \textbf{features map}), la quale costituisce di fatto
il primo strato nascosto della rete.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{convoluzione}
\par\end{centering}
\caption{Operazione di convoluzione nel caso bidimensionale: un filtro di dimensione
2 \texttimes{} 2 viene moltiplicato elemento per elemento per una
porzione dell'input di uguali dimensioni. L'output dell'operazione
è costituito dalla somma di tali prodotti. L'operazione di convoluzione
viene infine ripetuta spostando il filtro lungo le due dimensioni
dell'input.}
\end{figure}
 Nel caso in cui gli input siano rappresentati da volumi tridimensionali
la procedura rimane la stessa, ma è importante notare che il filtro,
pur mantendo una ridotta estensione spaziale (larghezza e altezza),
viene esteso in profondità in misura uguale all'input.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.35]{Conv_3d}
\par\end{centering}
\caption{Operazione di convoluzione di due differenti filtri (W0 e W1) di dimensione
$3\times3\times3$ su un volume di input $7\times7\times3$.}

\end{figure}
É importante notare che la singola operazione di convoluzione produce
sempre uno scalare, indipendentemente da quali siano le dimensioni
del volume di input, sia esso bidimensionale o tridimensionale. Di
conseguenza, una volta che il filtro viene fatto convolvere lungo
tutta la supercie dell'input, si ottiene sempre una mappa di attivazioni
bidimensionale. 

\section{Strati di una CNN}

\subsection{Convolutional Layer}

Come si può intuire questo è il principale tipo di layer: l\textquoteright utilizzo
di uno o più di questi layer in una rete neurale convoluzionale è
indispensabile. I\textbf{ parametri di un convolutional laye}r in
pratica riguardano un insieme di filtri allenabili. Ciascun \textbf{filtro}
è spazialmente ridotto, lungo le dimensioni di larghezza ed altezza,
ma si estende per l\textquoteright intera profondità del volume di
input a cui viene applicato. Durante la forward propagation si trasla,
o più precisamente si convolve, ciascun filtro lungo la larghezza
e l\textquoteright altezza del volume di input, producendo una \textbf{activation
map} (o \textbf{feature map}) bidimensionale per quel filtro. Intuitivamente,
la rete avrà come obbiettivo quello di apprendere (tramite \textbf{backpropagation}
e \textbf{discesa del gradiente}) dei filtri che si attivano in presenza
di un qualche specifico tipo di feature in una determinata regione
spaziale dell\textquoteright input. L\textquoteright accodamento di
tutte queste activation map, per tutti i filtri, lungo la dimensione
della profondità forma il volume di output di un layer convoluzionale.
Ciascun elemento di questo volume può essere interpretato come l\textquoteright output
di un neurone (con la sua funzione di attivazione) che osserva solo
un piccola regione dell\textquoteright input e che condivide i suoi
parametri con gli altri neuroni nella stessa activation map, dato
che questi valori provengono tutti dall\textquoteright applicazione
dello stesso filtro. In altre parole, ragionando dalla prospettiva
opposta, la mappa di attivazioni è formata da neuroni connessi localmente
allo strato di input attraverso i parametri del filtro che li ha generati.
Attualmente le cose potrebbero non apparire ancora del tutto chiare.
Vediamo dunque più dettagliatamente i concetti appena espressi, suddividendoli
per meglio comprendere l\textquoteright intero processo.

\paragraph{Proprietà di connettività locale}

Era già stato menzionato in precedenza il fatto che, avendo a disposizione
degli input di considerevoli dimensioni come appunto delle immagini,
connettere ciascun neurone di un layer con tutti i neuroni del layer
precedente (o del volume di input) nella pratica non è conveniente.
Infatti qui ciascun neurone è connesso solo ad una piccola regione
locale del volume di input. L\textquoteright estensione spaziale (sempre
larghezza e altezza) di questa regione è un parametro del layer convoluzionale
e viene detto \textbf{receptive field} del neurone. È bene ricordare
quanto già detto in precedenza: l\textquoteright estensione lungo
l\textquoteright asse della profondità della regione considerata è
sempre uguale alla profondità del volume di input. Ciò significa che
rispetto alla profondità non si esclude nulla dell\textquoteright input
per ciascuna regione locale, limitata invece in larghezza ed altezza.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{Layer_conv}
\par\end{centering}
\caption{Layer convoluzionale applicato ad un\textquoteright immagine di CIFAR-10,
si noti come un singolo neurone non sia connesso a tutto l'input ma
solo ad una parte di dimensione uguale al filtro applicato}
\end{figure}
Un semplice esempio può chiarire questi concetti: prendiamo la solita
immagine di CIFAR-10, quindi un volume $[32\times32\times3]$; se
il receptive field è di dimensioni $5\times5$, allora ciascun neurone
del layer convoluzionale avrà dei weight associati ad una regione
locale $5\times5\times3$ del volume di input, per un totale di $5*5\ast3=75$
pesi, numero decisamente inferiore rispetto a quanto visto nella sezione
precedente ($3072$ pesi) per un neurone in un\textquoteright architettura
fully connected.

\paragraph{Output dello strato}

Fin qui si è parlato della connettività di ciascun neurone di un layer
convoluzionale rispetto al volume di input. In questa sottosezione
si parlerà invece della quantità di neuroni presenti nel volume di
output di un layer convoluzionale e di come essi sono organizzati.
In questo caso vi sono 3 iperparametri:
\begin{itemize}
\item \textbf{Profondità (depth)}: corrisponde al numero di filtri $N_{F}$
che compongono lo strato, ognuno in cerca di caratteristiche differenti
nel volume di input. Il set di neuroni (appartanenti a filtri diversi)
connessi alla stessa regione di input prende invece il nome di colonna
profondità (depth column).
\item \textbf{Stride}: specifica il numero di pixel di cui si vuole traslare
il filtro ad ogni spostamento. Poiché come detto eseguendo la convoluzione
tra filtro e parte dell'input otteniamo un singolo scalare, questo
parametro permette di specificare in che maniera allocare le depth
column in tutto lo spazio, cioè larghezza e altezza, dell\textquoteright input.
Quando lo stride è pari ad uno significa che stiamo muovendo il filtro
un pixel alla volta, e di conseguenza viene scannerizzata ogni possibile
posizione dell'input. Valori più alti muovono il filtro con salti
maggiori, e pertanto viene generato un output di dimensioni minori.
\item \textbf{Zero padding}: a volte può risultare conveniente aggiungere
un bordo di zeri al volume di input, in modo così da controllare le
dimensioni dell'output ed evitare incongruenze durante le operazioni.
Lo spessore di questo bordo è determinato dall'iperparametro di zero-padding,
ed è spesso utilizzato per far combaciare la dimensione dell'input
con quella dell'output.
\end{itemize}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{Stride1}
\par\end{centering}
\caption{Stride uguale a 1}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{Stride2}
\par\end{centering}
\caption{Stride uguale a 2}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{Padding}
\par\end{centering}
\caption{Padding uguale a 1}
\end{figure}
Larghezza e altezza del volume di output sono determinati in funzione
del volume di input, dimensione del campo recettivo (dimensione del
filtro), dello stride, quantità di zero-padding. La profondità invece
dipende dal numero di filtri $N_{F}$ applicati. Siano $F$ la dimensione
di un lato di un filtro (i filtri son quadrati), $P$ la quantità
di padding applicata, $S$ il valore di stride allora, dato in input
un volume di dimensione $[W_{1}\times H_{1}\times D_{1}]$ lo strato
convoluzionale produrra un output di dimensione $[W_{2}\times H_{2}\times D_{2}]$
dove:

\begin{equation}
\begin{cases}
W_{2}=\frac{(W_{1}-F+2P)}{S}+1\\
H_{2}=\frac{(H_{1}-F+2P)}{S}+1\\
D_{2}=N_{F}
\end{cases}
\end{equation}
Chiaramente, i valori di stride e zero-padding devono essere scelti
in modo tale che $W_{2}$ e $H_{2}$ siano valori interi. Ad esempio,
l'architettura AlexNet di Krizhevsky et al. (2012) accetta in input
immagini di dimensione {[}227 \texttimes{} 227 \texttimes{} 3{]} e
utilizza nel suo primo convolutional layer 96 filtri di dimensione
11, stride pari a 4 e nessun zero-padding. Dal momento che $(227-11)/4+1=55$
il volume finale dell'output del primo strato avrà dimensione {[}55
\texttimes{} 55 \texttimes{} 96{]}. Ognuno dei 55 \texttimes{} 55
\texttimes{} 96 neuroni di questo volume è connesso ad una regione
di dimensione {[}11 \texttimes{} 11 \texttimes{} 3{]} del volume di
input, e tutti i 96 neuroni appartenenti alla stessa colonna profondità
sono connessi alla stessa regione {[}11 \texttimes{} 11 \texttimes{}
3{]}, ma ovviamente attraverso set diversi di pesi. \textbf{Osservazione:
}è importante che durante la convoluzione i filtri coprano tutto l'input,
non è una buona idea quindi adottare un valore di stride superiore
alla larghezza dei filtri. 

\paragraph{Condivisione dei parametri e invarianza alla traslazione}

Nell'esempio sopra riportato, ognuno dei $55\times55\times96=290400$
neuroni del primo convolutional layer possiede $11\times11\times3=363$
pesi, più uno relativo alla distorsione. In un'architettura come quella
delle reti neurali classiche che non prevede la condivisione dei pesi
questo comporterebbe un numero totale di parametri pari a $290400\times364=105705600$,
solamente per il primo strato. Tale quantità, chiaramente intrattabile
nella realtà, viene drasticamente ridotta dalla proprietà di condivisione
dei parametri (\textbf{weight sharing}) delle CNN, la quale si fonda
su una ragionevole assunzione: se la rilevazione di una caratteristica
in una determinata posizione spaziale risulta utile, lo sarà anche
in differenti posizioni spaziali. Si assume cioè una struttura dell'immagine\textbf{
invarante rispetto a traslazioni}. Questo è il motivo per cui facciamo
scorrere lo stesso filtro su tutto il volume di input. In pratica,
chiamando una singola \textquotedblleft \textit{sezione}\textquotedblright{}
bidimensionale lungo l\textquoteright asse di profondità del volume
di output con depth slice (o mappa di attivazione, ad esempio un volume
55 x 55 x 96 ha 96 depth slice, ciascuna ovviamente di dimensioni
55 x 55), si farà in modo che tutti i neuroni di ciascuna \textbf{depth
slice} (attenzione a non confondere con \textit{depth column}) utilizzino
gli stessi weight e bias. Tornando all'esempio, la proprietà di weight
sharing comporta una diminuzione dei parametri per il primo strato
fino a $(11\times11\times3)\times96+96=34944$, rispetto ai precedenti
105 milioni, ovvero circa tremila volte meno.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.45]{96filtri}
\par\end{centering}
\caption{Set di 96 filtri 11 \texttimes{} 11 \texttimes{} 3 appresi dall'architettura
di Krizhevsky et al. (2012) in un problema di classicazione di immagini.
L'assunzione di weight sharing è ragionevole dal momento che individuare
una linea o uno spigolo è importante in qualsiasi posizione dell'immagine,
e di conseguenza non c'è la necessità di imparare ad localizzare la
stessa caratteristica in tutte le possibili zone.}
\end{figure}
E' importante notare che in determinati contesti l'assunzione di weight
sharing perde di senso. Un caso particolare è quando le immagini in
input presentano una struttura specifica e centrata, e ci si aspetta
di apprendere caratteristiche differenti in una determinata area dell'immagine
piuttosto che in un'altra. Ad esempio, in un problema di riconoscimento
facciale dove i volti sono stati ritagliati e centrati, risulterà
più efficace apprendere nella parte superiore dell'immagine caratteristiche
relative ad occhi o capelli, mentre nella parte inferiore quelle speciche
di bocca e naso.

\noindent Riassumendo, un layer convoluzionale:
\begin{itemize}
\item accetta in ingresso un volume $[W_{1}\times H_{1}\times D_{1}]$;
\item richiede il settaggio dei seguenti parametri:
\begin{itemize}
\item numero di kernel $N_{F}$ (il parametro depth);
\item il lato dell\textquoteright area del receptive field F; 
\item lo stride S; 
\item l\textquoteright ammontare dello zero-padding P;
\end{itemize}
\item produce un volume di output di dimensioni $[W_{2}\times H_{2}\times D_{2}]$
dove:
\begin{itemize}
\item $W_{2}=\frac{(W_{1}-F+2P)}{S}+1$
\item $H_{2}=\frac{(H_{1}-F+2P)}{S}+1$
\item $D_{2}=N_{F}$
\end{itemize}
\item con la tecnica della condivisione dei parametri, si hanno $N_{F}\times N_{F}\times D_{1}$
pesi per ogni kernel, per un totale di $N_{F}\times N_{F}\times D_{1}$
pesi e $N_{F}$ bias;
\item nel volume di output l\textquoteright i-esimo depth slice (di taglia
$W_{2}\times H_{2}$) è il risultato dell\textquoteright operazione
di convoluzione dell\textquoteright i-esimo kernel sul volume di input
con uno stride S e con la successiva aggiunta dell\textquoteright i-esimo
bias.
\end{itemize}

\paragraph{Campo recettivo}

\noindent Un concetto base nelle CNN è il \textbf{campo recettivo}
(\textbf{receptive field}). A causa della connettività sparsa, in
una CNN l'output dipende solo dalla regione dell'input, a differenza
delle reti FC dove il valore di ogni output dipende da tutto l'input.

\noindent Questa regione dell'input è il campo recettivo per quell'output.
Più si va in profondità, più largo è il campo recettivo. Di solito
il campo recettivo si riferisce all'unità di output finale della rete
in relazione all'input della rete, ma la stessa definizione vale per
volumi intermedi.
\begin{figure}[H]
\noindent \begin{centering}
{\small{}\includegraphics[scale=0.8]{receptiveFieldFC}}{\small\par}
\par\end{centering}
\noindent \begin{centering}
{\small{}\includegraphics{receptiveFieldCNN}}{\small\par}
\par\end{centering}
\caption{Campi recettivi di FCN (in alto) e CNN (in basso)}
\end{figure}


\subsection{Pooling layer}

Nell'architettura di una CNN è pratica comune inserire fra due o più
convolutional layers uno strato di \textbf{Pooling}, la cui funzione
è quella di ridurre progressivamente la dimensione spaziale degli
input (larghezza e altezza), in modo da diminuire numero di parametri
e carico computazionale, e di conseguenza controllare anche il sovradattamento.
Per il ridimensionamento si utilizza una semplice funzione, come ad
esempio un\textquoteright operazione di \textbf{max (max-pooling)}
oppure di \textbf{media (average-pooling)} e pertanto non comporta
la presenza di pesi allenabili. L'assenza di pesi allenabili comporta
anche che questo layer svolga le sue mansioni solo durante la forward
propagation, nelle fasi di backward propagation retropropaga gli errori
e basta senza calcolare alcuna derivata. I pooling layer hanno alcuni
iperparametri settabili:
\begin{itemize}
\item il \textbf{lato} F dell\textquoteright estensione spaziale della selezione
quadrata che verrà di volta in volta considerata sull\textquoteright input
in ogni suo depth slice; 
\item il parametro di \textbf{stride} S;
\end{itemize}
Si può notare una certa somiglianza con i parametri di un layer convoluzionale:
questo perchè anche qui si ha una sorta di receptive field che viene
spostato di volta in volta, con un passo specificato dal parametro
di stride, su ciascun depth slice del volume di input. In realtà oltre
a questo, la situazione in questo caso è completamente diversa, dato
che qui non vi è alcuna operazione di convoluzione. Come per il layer
convoluzionale, il volume di output $[W_{2}\times H_{2}\times D_{2}]$
è funzione del volume di input $[W_{1}\times H_{1}\times D_{1}]$
e dei due iperparametri secondo formule analoghe a quelle della sezione
precedente:

\begin{equation}
\begin{cases}
W_{2}=\frac{(W_{1}-F)}{S}+1\\
H_{2}=\frac{(H_{1}-F)}{S}+1
\end{cases}
\end{equation}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{Max_pooling}
\par\end{centering}
\caption{}

\end{figure}


\subsection{Fully Connected layer}

Questo tipo di layer è esattamente uguale ad uno qualsiasi dei layer
di una classica rete neurale artificiale con architettura fully connected:
semplicemente in un \textbf{layer Fully Connected (FC)} ciascun neurone
è connesso a tutti i neuroni del layer precedente, nello specifico
alle loro attivazioni. L'architettura fully-connected implica il rilassamento
dell'assunzione di weight sharing. Valgono quindi la maggior parte
delle considerazioni fatte nel capitolo 2, ad eccezione ovviamente
di quelle che non hanno senso applicate ad un singolo strato o un
sottoinsieme degli strati della rete (es. l'algoritmo di discesa del
gradiente è unico per tutta la rete, non ha senso applicarne uno diverso
per uno strato). Solitamente si utilizza più di un layer FC in serie
e l\textquoteright ultimo di questi avrà un numero di neuroni $K$
pari al numero di classi presenti nel dataset (nel caso della classificazione)
o in generale il numero di neuroni è determinato dall'output della
rete desiderato. La funzione principale dei FC layer nell\textquoteright ambito
delle reti neurali convoluzionali è quello di effettuare una sorta
di raggruppamento delle informazioni ottenute fino a quel momento,
esprimendole con un singolo numero (l\textquoteright attivazione di
uno dei suoi neuroni), il quale servirà nei successivi calcoli per
la classificazione finale.

\section{Architettura generale di una CNN}

Finora abbiamo visto i singoli strati che possono essere impiegati
nell'architettura di una Convolutional Neural Network. Come per le
reti neurali tradizionali, anche qui non esiste una linea guida per
la scelta degli iperparametri, ne tantomeno per la sequenza da adottare
nella scelta degli strati. Ci sono però alcune considerazioni ragionevoli
che si possono fare per cercare quantomeno di muoversi lungo la giusta
direzione. La dimensione dei filtri dovrebbe, almeno in linea teorica,
essere motivata dalla correlazione spaziale dell'input che un particolare
strato riceve, mentre il numero di fiitri utilizzati (quindi di feature
maps generate) si riflette sulla capacità della rete di cogliere rappresentazioni
gerarchiche: questo signica che gli strati finali necessitano di un
numero di filtri maggiore di quello destinato agli strati iniziali,
altrimenti si limitano le possibilità di combinare features di basso
livello per rappresentare features di alto livello, che sono di fatto
quelle più vicine alla classicazione finale. La rappresentazione gerarchica
sottintende che generalmente non è un sufficiente un singolo strato
convoluzionale, in quanto features estratte nello stesso strato sono
gerarchicamente di pari importanza. Inoltre anche per le CNN valgono
le tecniche per affrontare l'overfitting quali dropout, batch normalization
e altre trattate in precedenza con gli opportuni accorgimenti del
caso. La maggior parte delle architetture CNN seguono il seguente
schema di composizione dei layer: 

\noindent\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
INPUT -> {[}{[}CONV -> RELU{]}{*}N -> POOL?{]}{*}M -> {[}FC -> RELU{]}{*}K
-> FC%
\end{minipage}}

dove:
\begin{itemize}
\item {*} indica la ripetizione
\item POOL? indica l'opzionalità del pooling layer
\item N > 0 e solitamente N <= 3
\item M >= 0
\item k >= 0 e solitamente k < 3
\end{itemize}
Nelle prossime pagine mostreremo alcune architetture note di reti
CNN e varianti per compiti specifici (es. segmentation)

\section{Esempi architetture CNN}

Architetture note di CNN sono:
\begin{enumerate}
\item \textbf{LeNet-5}: sviluppato nel 1998 per identificare cifre scritte
a mano per il riconoscimento dello zip code nel servizio postale (60000
parametri). L'idea era quella di usare una sequenza di 3 layers: convoluzione,
pooling, non-linearità. I pixel in un'immagine sono altamente correlati,
la convoluzione può essere usata come modo di estrarre le features
spaziali e per avere connessioni sparse, mentre il subsample viene
fatto usando la media spaziale delle mappe (average pooling). Le non-linearità
usate in questa rete sono Tanh o sigmoidi. Infine, l'ultimo layer
ha un MLP come classificatore.
\item \textbf{AlexNet}: sviluppata nel 2012, è simile a LeNet-5 (60 milioni
di parametri, conv.: 3.7 milioni, FC: 58.6 milioni). La dimensione
dell'input è di 224x224x3, mentre i layer sono:
\begin{itemize}
\item 5 layer convoluzionali
\item 3 MLP
\end{itemize}
Per evitare l'overfitting la rete usa ReLU, Dropout (0.5), weight
decay e maxpooling. Il primo layer convoluzionale ha 96 filtri 11x11
con stride=4. L'output è costituito da 2 volumi da 55x55x48. 
\item \textbf{VGG16}: introdotta nel 2014 come variante della AlexNet (138
milioni di parametri, conv.:11\%, FC:89\%). La dimensione dell'input
è di 224x224x3. L'idea è quella di usare convoluzioni multiple 3x3
in sequenza per ottenere campi recettivi più larghi con meno parametri
e più non-liearità anzichè con filtri più larghi in un layer singolo.
\item \textbf{GoogleNet}: rete con alta efficienza computazionale (5 milioni
di parametri, 22 layers di moduli Inception). Questa rete è basata
su moduli Inception, ovvero una sorta di <<rete nella rete>> o <<moduli
locali>>. Scegliere la giusta dimensione del kernel per le operazioni
di convoluzione è difficile, dato che l'immagine potrebbe mostrare
features rilevanti a scalature diverse. Le reti troppo dense tendono
inoltre a overfittare e a diventare computazionalmente molto costose.
La soluzione è quella di sfruttare dimensioni multiple del filtro
allo stesso livello e poi fare un merge tramite concatenazione delle
mappe di attivazione dell'output (con zero padding).
\item \textbf{Inception Module}: per ridurre il carico computazionale della
rete, il numero dei canali di input è ridotto aggiungendo un layer
convoluzionale 1x1 prima delle convoluzioni 3x3 e 5x5. Il volume dell'output
ha dimensione simile, ma il numero di operazioni richieste è ridotto
significativamente grazie alla convoluzione 1x1 (che incrementa quindi
il numero di non-linearità). Tale layer convoluzionale è anche chiamato
''bottleneck layer''.
\end{enumerate}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{lenet}
\par\end{centering}
\caption{LeNet-5}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{alexnet}
\par\end{centering}
\caption{AlexNet}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{vgg16}
\par\end{centering}
\caption{VGG16}
\end{figure}

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{GoogleNet}
\par\end{centering}
\caption{GoogleNet}
\end{figure}

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{Inception}
\par\end{centering}
\caption{Inception module }

\end{figure}


\section{Data Augmentation}

La \textbf{Data Augmentation} è tipicamente effettuata tramite:
\begin{itemize}
\item \textbf{trasformazioni geometriche}
\begin{itemize}
\item shift/rotazioni/distorsioni
\item shear
\item scaling
\item flip
\end{itemize}
\item \textbf{trasformazioni fotometriche}
\begin{itemize}
\item aggiunta di rumore
\item modifica dell'intensità media
\item superimposizione di altre immagini
\item modificazione del contrasto
\end{itemize}
\end{itemize}
\textbf{Test Time Augmentation}

\noindent Anche se la CNN è allenata usando Data Augmentation, non
raggiungerà un'invarianza perfetta rispetto alle trasformazioni considerate.

\noindent La \textbf{Test Time Augmentation (TTA)} può essere performata
a test time per migliorare la precisione delle predizioni. TTA è particolarmente
utile per le immagini di test in cui il modello è parecchio insicuro.

\noindent Gli step della TTA sono:
\begin{enumerate}
\item performare un'augmentation randomica su ogni immagine di test \emph{I}
\item fare una media delle predizioni per ogni \emph{I}
\item prendere il vettore delle medie per la definizione della predizione
finale
\end{enumerate}
\textbf{Confusion matrix}

\noindent L'idea della \textbf{confusion matrix} sta nel fatto che
l'elemento \emph{C(i, j) }della matrice corrisponda alla percentuale
degli elementi appartenenti alla classe \emph{i }classificati come
elementi della classe \emph{j}. Quindi la confusion matrix ideale
ha tutti 1 sulla diagonale principale.
\begin{figure}[H]
\begin{centering}
\includegraphics{\string"confusion matrix\string".png}
\par\end{centering}
\caption{Esempio di confusion matrix}

\end{figure}

\noindent \textbf{Classificazione a due classi}

\noindent La performance della classificazione nel caso di classificatori
binari può anche essere misurata in termini della \textbf{ROC (Receiver
Operating Characteristic) curve}. 

\noindent Il classificatore ideale raggiungerebbe: 
\begin{itemize}
\item FPR = 0\%
\item TPR = 100\%
\end{itemize}
Quindi, più larga è l'\textbf{Area Under Curve (AUC)}, meglio è.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{roc}
\par\end{centering}
\caption{ROC (ogni punto rosso è specifico per un parametro)}
\end{figure}


\section{Transfer Learning}

L'output del fully connected layer ha la stessa dimensione del numero
di classi L, e ogni componente fornisce un punteggio di appartenenza
a una certa classe per l'immagine in input.

\noindent L'input del fully connected layer può essere visto come
un descrittore dell'immagine in input, cioè un vettore di features,
le quali sono definite per massimizzare la performance di classificazione
e sono allenate con backpropagation.

\noindent Come sappiamo, più ci muoviamo in profondità, più la risoluzione
spaziale è ridotta e il numero di mappe (relativo al campo percettivo)
aumenta, quindi cerchiamo dei pattern di alto livello senza preoccparci
troppo della loro posizione esatta nell'immagine. 

\noindent La parte convoluzionale può quindi essere vista come un
estrattore di feature molto generale, mentre i FC layers sono molto
personalizzati, dato che sono pensati per risolvere obiettivi di classificazione
specifici.

\noindent L'idea alla base del \textbf{transfer learning} è quindi
quella di:
\begin{itemize}
\item prendere un modello pre-allenato (es.: VGG)
\item rimuovere e modificare i FC layers 
\item congelare i pesi nella CNN
\item allenare l'intera rete sui nuovi dati di training
\end{itemize}
Ci sono due opzioni di congelamento della CNN:
\begin{itemize}
\item \textbf{Transfer Learning}: solo i layers della FCN vengono allenati
(una buona opzione quando si hanno pochi dati di training e ci si
aspetta che la CNN pre-allenata sia compatibile col problema)
\item \textbf{Fine tuning}: l'intera CNN è riallenata, ma i layer convoluzionali
sono inizializzati al modello pre-allenato (una buona opzione quando
si hanno abbastanza dati di training o quando non ci si aspetta che
la CNN pre-allenata sia compatibile col problema)
\end{itemize}
\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.3]{transf1}
\par\end{centering}
\caption{Architettura tipica di una CNN (verde: estrattore feature, giallo:
classificatore feature}

\end{figure}


\section{Segmentazione di immagini}

La \textbf{segmentazione semantica} di un immagine ha come obiettivo
quella di assegnare ad ogni pixel un'etichetta proveniente da un insieme
fissato. 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.43]{Segmentazione}
\par\end{centering}
\caption{Segmentazione di una camera da letto}
\end{figure}
In figura 4.9 viene mostrato un esempio di segmentazione, ogni etichetta
assegnata ad un pixel viene rappresentata come un colore differente.
Più formalmente:

\noindent Data un'immagine \textit{I, }associamo ad ogni pixel identificato
dalle sue coordinate (r,c) dove r e c identificano riga e colonna
rispettivamente, un'etichetta proveniente da un insieme $\Lambda$.

\subsection{Fully-Convolutional Neural Networks per la segmentazione semantica}

Le CNN sono pensate per processare imput di dimensione fissata. I
layer convoluzionali e di subsampling operano in maniera scorrevole
sull'immagine, mentre i FC layers vincolano l'input a una dimensione
fissata. Dando in pasto un'immagine di dimensione più larga alla rete,
quindi, non si possono calcolare i punteggi delle varie classi, ma
si possono comunque estrarre le features.

\noindent Ad ogni modo, dato che la FCN è lineare, può essere rappresentata
come una convoluzione, i cui pesi sono associati al neurone di output
$o_{i}$. Si ha che:

\begin{equation}
o_{i}\mathrel{=\mathop{\left(\boldsymbol{w_{i}}\mathbin{\odot}s\right)(0,0)}+}b_{i}
\end{equation}

\noindent dove $\boldsymbol{w_{i}}=$\{\emph{w$_{i,j}\}_{i=j:N}$}
sono i pesi associati a \emph{$o_{i}$}.

\noindent Quindi, dato che la FCN è lineare, può essere rappresentata
come una convoluzione di L filtri di dimensione 1x1xN. Ognuno di questi
filtri convoluzionali contiene i pesi della FCN per il corrispondente
neurone in output. Per ogni classe in output, otteniamo quindi un'immagine
(\textbf{heatmap}) avente una risoluzione più bassa rispetto a quella
dell'immagine in input e probabilità delle classi per il campo recettivo
di ogni pixel.

\noindent In un'immagine più grande, ogni pixel nella heatmap corrisponde
a un campo recettivo nell'immagine in input.

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.95]{heatmap}
\par\end{centering}
\begin{centering}
\includegraphics[scale=0.52]{heatmap_explan}
\par\end{centering}
\caption{Esempio di heatmap (sopra), Direct Heatmap Predictions (sotto)}
\end{figure}

\noindent Le \textbf{Fully-Convolutional Neural Networks} sono dunque
un mezzo per effettuare predizioni su immagini di dimensione arbitraria.
Ci sono varie soluzioni per segmentare le immagini tramite FCNN:
\begin{enumerate}
\item \textbf{Direct Heatmap Predictions}: possiamo assegnare l'etichetta
predetta nella heatmap all'intero campo recettivo, nonostante questa
sarebbe una stima molto grossolana
\item \textbf{Shift and Stich}: assumiamo che ci sia un ratio \emph{f} tra
la dimensione dell'input e la dimensione della heatmap in output.
Compiamo i seguenti step:
\begin{itemize}
\item calcoliamo le heatmap per tutti i possibili shift \emph{$f^{2}$}dell'input
($0\leq r,c<f$)
\item mappiamo le predizioni dalle \emph{$f^{2}$ }heatmap all'immagine
(ogni pixel della heatmap fornisce la predizione del pixel centrale
del campo recettivo)
\item frapponiamo le heatmap per formare un immagine larga quanto l'input
\end{itemize}
\item \textbf{solo convoluzioni}: campo recettivo piccolo e metodo molto
inefficiente
\end{enumerate}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.8]{shift_and_stich}
\par\end{centering}
\caption{Shift and Stich}
\end{figure}

\noindent La segmentazione semantica affronta una tensione tra semantica
e locazione, ovvero si pone l'obiettivo di fare predizioni locali
rispetto alla struttura globale. Un modo per fare ciò è rappresentato
nella figura seguente.

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{semant_segment}
\par\end{centering}
\caption{Rete con convoluzione e upsampling}

\end{figure}

\noindent Come possiamo notare, la prima parte della rete è una normale
CNN (convoluzioni + pooling/downsampling), mentre la seconda metà
è pensata per fare un upsampling delle predizioni per coprire ogni
pixel dell'immagine (convoluzioni + upsampling).

\noindent Per fare \textbf{l'upsampling} possiamo usare 2 metodi:
\begin{itemize}
\item \textbf{Nearest Neighbor}: si riempiono tutti i pixel del layer di
output col valore del corrispettivo pixel del layer di input
\item \textbf{Bed of Nails}: si riempiono tutti i pixel del layer di output
con degli 0, eccetto un pixel che avrà il valore del corrispettivo
pixel del layer in input
\end{itemize}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{upsampling}
\par\end{centering}
\caption{UpSampling}

\end{figure}

\noindent Per mantenere anche le informazioni delle posizioni dei
pixel, usiamo un meccanismo chiamato \textbf{Max Unpooling} (l'inverso
del Max Pooling, vedi figura sotto).

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{maxUnpooling}
\par\end{centering}
\caption{Max Unpooling}
\end{figure}

\noindent Inoltre possiamo anche fare una convoluzione trasposta,
che è esattamente l'operazione reciproca della convoluzione. L'unico
problema sta nei pixel che si sovrappongono, ma la soluzione consiste
nel fare una somma pesata tra i pixel in input e la parte di filtro
compresa nella regione di overlap.

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics{transpose_conv}
\par\end{centering}
\begin{centering}
\includegraphics{transpose_conv(1)}
\par\end{centering}
\caption{Convoluzione trasposta (stride = 2)}
\end{figure}

\noindent I filtri di upsampling possono essere appresi durante il
training con un'inizializzazione uguale all'interpolazione bilineare.
Le predizioni derivate dall'upsampling sono comunque troppo grossolane.
La soluzione a questo problema è quella di usare delle \textbf{Skip
Connections}. Queste integrano una tradizionale rete di contrazione
dove la convoluzione è rimpiazzata dalla convoluzione trasposta.

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{skip_connections}
\par\end{centering}
\caption{Skip connections}
\end{figure}


\paragraph{Metodi per allenare una FCNN}
\begin{enumerate}
\item \textbf{Patch-Based}: eseguiamo i seguenti step:
\begin{itemize}
\item prepariamo un set di training per una rete di classificazione
\item ritagliamo delle patch $x_{i}$ dalle immagini e assegnamo a ogni
patch l'etichetta corrispondente al punto centrale della patch
\item alleniamo una CNN per la classificazione da zero o facciamo fine-tuning
di un modello pre-allenato sulle classi di segmentazione
\item spostiamo i FC layers nelle convoluzioni 1x1
\item disegnamo la parte di upsampling della rete e ne alleniamo i filtri
\item la rete di classificazione è allenata per minimizzare l'errore di
classificazione \emph{l} su un mini-batch
\begin{equation}
\hat{\theta}=min_{\theta}\sum_{x_{j}}l(\boldsymbol{x_{j}},\theta)
\end{equation}
\item i batch sono poi randomicamente assemblati durante il training
\end{itemize}
\item \textbf{Full-Image}: è possibile allenare direttamente una FCNN che
include i layer di upsampling, quindi il learning diventa la seguente
minimizzazione 
\begin{equation}
min_{x_{j}\in I}\sum_{x_{j}}l(\boldsymbol{x_{j}},\theta)
\end{equation}

dove \emph{$\boldsymbol{x_{j}}$}sono i pizel in una regione dell'immagine
di input e la funzione di errore è valutata sulle etichette corrispondenti
nell'annotazione. Tale metodo è chiamato anche ''\textit{metodo end-to-end}''
ed è più efficiente del Patch-Based.
\end{enumerate}

\subsection{U-Net}

La \textbf{U-Net} è formata da una parte contrattiva e una parte espansiva
e non ha FC layers (è \textbf{simmetrica}). Essa usa un gran numero
di feature maps nella parte di downsampling e usa molta data augmentation.

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{UNet}
\par\end{centering}
\caption{U-Net}
\end{figure}

\noindent Nella parte di \textbf{contrazione}, la rete ripete blocchi
aventi:
\begin{itemize}
\item 2 layer convoluzionali 3x3 + ReLU (opzione 'valid', no padding)
\item maxpooling 2x2
\end{itemize}
A ogni downsampling, inoltre, il numero di feature maps è raddoppiato.
La particolarità di questa rete è che implementa le skip connections,
tramite le quali aggrega tramite concatenazione l'ultimo layer convoluzionale,
per ogni blocco nella rete di contrazione, col primo layer convoluzionale
del corrispettivo blocco della rete di espansione. 

\noindent Nella parte di \textbf{espansione}, la rete ripete blocchi
aventi:
\begin{itemize}
\item convoluzione trasposta 2x2, dimezzando il numero di feature maps (ma
raddoppiando la risoluzione spaziale)
\item concatenazione delle cropped features corrispondenti
\item 2 layer convoluzionali 3x3 + ReLU
\end{itemize}
Il risultato è un'immagine in output più piccola di quella in input.

\noindent La rete U-Net usa il metodo di training Full-Image con \textbf{una
funzione di errore} pesata (\textbf{NB}. È la funzione appositamente
formulata per la segmentazioni di immagini biomediche per cui U-Net
è stata pensata \url{https://arxiv.org/pdf/1505.04597.pdf})

\begin{equation}
\hat{\theta}=min_{\theta}\sum_{x_{j}}w(\boldsymbol{x_{j}})log(\boldsymbol{x_{j}},\theta)
\end{equation}

\noindent dove il peso è dato dalla relazione

\begin{equation}
w(\boldsymbol{x})=w_{c}(\boldsymbol{x})+w_{0}e^{-\frac{(d_{1}(\boldsymbol{x})+d_{2}(\boldsymbol{x}))^{2}}{2\sigma^{2}}}
\end{equation}

\noindent In quest'ultima relazione, $w_{c}$è usato per bilanciare
le proporzioni delle classi, $d_{1}$è la distanza tra il bordo e
la cellula più vicina, mentre $d_{2}$è la distanza tra il bordo e
la seconda cellula più vicina (i pesi sono più grandi quando la distanza
dalle due celle più vicine al bordo è piccola). Inoltre, il primo
termine dell'equazione tiene conto dello sbilanciamento delle classi
nel training set, mentre il secondo migliora la performance della
classificazione ai bordi di oggetti diversi tra loro.

\subsection{Global Averaging Pooling}

L'idea principale consiste nell'usare, anzichè le convoluzioni tradizionali,
uno stack di convoluzioni 1x1 + ReLU, che corrisponde alle reti di
MLP usate in maniera scorrevole su tutta l'immagine. Viene quindi
introdotto il \textbf{Global Averaging Pooling layer}, che calcola
la media di ogni feature map al posto del FC layer alla fine della
rete e predire usando una semplice softmax. L'importante è che il
numero di feature maps sia uguale al numero di classi in output. Tra
i vantaggi di questo layer vi è quello di riuscire a classificare
immagini di dimensioni diverse.

\noindent In pratica è come se avessimo una\textbf{ rete nella rete
(NiN)} composta da:
\begin{itemize}
\item MLP convolutional layers + ReLU + dropout
\item maxpooling
\item GAP layer
\item softmax
\end{itemize}
Delle semplici NiN raggiungono la miglior performance su piccoli dataset
grazie al fatto che il GAP riduce efficacemente l'overfitting rispetto
al FC. Possiamo quindi affermare che il GAP agisce come un regolarizzatore
strutturale.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.55]{GAP}
\par\end{centering}
\caption{Network in Network}
\end{figure}


\paragraph{Weakly-Supervised Localization}

\noindent La \textbf{Weakly-Supervised Localization} effettua una
localizzazione su un'immagine senza box di contorno annotati. Il training
set è fornito come per la classificazione con coppie <immagine, etichetta>
(\emph{I}, \emph{l}) dove non è fornita alcun'informazione sulla localizzazione.

\paragraph{CAM}

\noindent I vantaggi del GAP layer vanno oltre al semplice agire come
regolarizzatore che previene l'overfitting. Infatti, la rete può conservare
una notevole abilità di localizzazione fino al layer finale. Una CNN
allenata sulla categorizzazione di un oggetto è quindi capace di localizzare
con successo le regioni discriminative per la classificazione contenenti
gli oggetti con cui gli umani interagiscono piuttosto che gli umani
stessi. Ciò viene fatto tramite una tecnica chiamata \textbf{Class
Activation Mapping (CAM)}, che permette di identificare quali regioni
di un'immagine verranno usate per la discriminazione. Essa richiede
solo un FC layer dopo il GAP e una piccola regolarizzazione. Tale
layer calcola lo score $S_{c}$ per ogni classe \emph{c} come 

\begin{equation}
S_{c}=\sum_{k}w_{k}^{c}F_{k}
\end{equation}

\noindent dove 
\begin{equation}
F_{k}=\sum_{(x,y)}f_{k}(x,y)
\end{equation}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{CAM(1)}
\par\end{centering}
\caption{CAM}

\end{figure}

\noindent Dopodichè, calcola la probabilità \emph{$P_{c}$ }della
classe \emph{c }come

\begin{equation}
P_{c}=\frac{e^{S_{c}}}{\sum_{i}e^{S_{i}}}
\end{equation}

\noindent Ad ogni modo, unendo la (128) e la (129), e sapendo che
il CAM è definito come
\begin{equation}
M_{c}(x,y)=\sum_{k}w_{k}^{c}f_{k}(x,y)
\end{equation}

\noindent otteniamo che

\begin{equation}
S_{c}=\sum_{(x,y)}M_{c}(x,y)
\end{equation}

\noindent dove $M_{c}(x,y)$ indica direttamente l'importanza delle
attivazioni a (x,y) per la classe \emph{c}. Inoltre, grazie alla softmax,
la profondità dell'ultimo layer convoluzionale può essere diversa
dal numero di classi. Potrebbe anche essere necessario un upsampling
per poter matchare l'immagine in input (vedi figura sotto). I pesi
rappresentano quindi l'importanza di ogni feature map per portare
alla predizione finale.

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{CAM}
\par\end{centering}
\caption{Class Activation Mapping}
\end{figure}


\paragraph{Grad-CAM}

\noindent CAM ha il grande svantaggio di dover modificare la rete
per poter predire un'altra classe. Una tecnica che risolve questo
problema è la \textbf{Grad-CAM}, la quale usa il \textbf{gradiente}
per calcolare la heatmap a bassa risoluzione $\boldsymbol{g^{c}}$,
dove 

\begin{equation}
\boldsymbol{g^{c}=}RELU\left(\sum_{k}w_{k}^{c}\boldsymbol{a_{k}}\right)
\end{equation}

\begin{equation}
w_{k}^{c}=\frac{1}{nm}\sum_{i}\sum_{j}\frac{\partial y_{c}}{\partial\boldsymbol{a_{k}}(\boldsymbol{i},\boldsymbol{j})}
\end{equation}

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.6]{Grad-CAM}
\par\end{centering}
\caption{Grad-CAM}
\end{figure}


\paragraph{Augmented Grad-CAM}

\noindent Consideriamo l'operatore di augmentation $\mathcal{A_{\mathit{l}}}:\mathbb{R^{\mathrm{N\mathbin{x}M}}\rightarrow\mathbb{R^{\mathrm{N\mathbin{x}M}}}}$,
che include rotazioni e traslazioni randomiche dell'immagine di input
\textbf{x}. L\textbf{'Augmented Grad-CAM} incrementa la risoluzione
delle heatmap tramite \textbf{image augmentation}. Tutte le risposte
generate dalla CNN alle versioni multiple augmentate della stessa
immagine di input hanno molte informazioni per la ricostruzione della
heatmap \textbf{h} ad alta risoluzione.

\noindent 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.6]{aug-grad-cam}
\par\end{centering}
\caption{Augmented Grad-CAM}
\end{figure}

\noindent Viene fatta la cosiddetta \textbf{Super-Resolution (SR)}
della heatmap sfruttando le informazioni condivise nelle heatmap a
bassa risoluzione calcolate dallo stess input sotto diverse, ma note,
trasformazioni. Le CNN sono in generale invarianti alle rototraslazioni,
in termini di predizioni, ma ogni heatmap a bassa risoluzione\textbf{
$\boldsymbol{g_{l}}$} contiene di fatto diverse informazioni. Modelliamo
le heatmap calcolate col Grad-CAM come risultato di un operatore di
downsampling sconosciuto $\mathcal{\boldsymbol{D}}:\mathbb{R}^{\mathrm{N\mathbin{x}M}}\rightarrow\mathbb{R^{\mathrm{N\mathbin{x}M}}}$.
L'alta risoluzione della heatmap \textbf{h} è recuperata risolvendo
un problema inverso

\begin{equation}
argmin_{h}\frac{1}{2}\sum_{l=1}^{L}\bigparallel\mathcal{DA\mathit{_{l}}\mathrm{\boldsymbol{h}-g_{\mathit{l}}\bigparallel}_{\mathrm{2}}^{\mathrm{2}}}+\lambda TV_{l_{1}}(\boldsymbol{h})+\frac{\mu}{2}\bigparallel\boldsymbol{h}\bigparallel{}_{\mathrm{2}}^{\mathrm{2}}
\end{equation}

\noindent dove 
\begin{equation}
TV_{l_{1}}(\boldsymbol{h})=\sum_{i,j}\bigparallel\partial_{x}\boldsymbol{h}(i,j)\bigparallel+\bigparallel\partial_{y}\boldsymbol{h}(i,j)\bigparallel
\end{equation}
è la \textbf{regolarizzazione Anistropic Total Variation} usata per
preservare i bordi della heatmap target ad alta risoluzione. Questo
viene risolto tramite \textbf{Subgradient Descent} (\url{https://en.wikipedia.org/wiki/Subderivative#The_subgradient})
dato che la funzione è convessa e non-smooth (non differenziabile
e discontinua).

\section{Localizzazione}

\subsection{R-CNN}

Per quanto riguarda il task del riconoscimento degli oggetti (\textbf{object
detection}), una possibile soluzione potrebbe essere quella di usare
una finestra scorrevole (\textbf{sliding window}) che ha un\textquoteright etichetta
assegnata al pixel centrale e consente di analizzare un\textquoteright immagine
di dimensioni variabili a piccoli pezzi di dimensioni fisse. Tali
\textquotedblleft pezzi\textquotedblright , cioè regioni di immagine,
vengono poi date in pasto a un modello preallenato per effettuare
predizioni sulle stesse. Questo approccio ha, però, vari contro, tra
cui il fatto che non riutilizza features condivise fra regioni sovrapposte.
Una possibile idea per risolvere questo problema è usare un algoritmo
a proposta di regione (\textbf{region proposal algorithm}), che classifica
tramite una CNN l\textquoteright immagine dentro ogni regione proposta.
Tale rete prende quindi il nome di R-CNN (dove R sta per \textbf{regioni}).

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{rcnn}
\par\end{centering}
\caption{Funzionamento R-CNN}
\end{figure}
Lo step 4 viene performato da un regressore \textbf{Support Vector
Machine (SVM)}, allenato per minimizzare l\textquoteright errore di
classificazione sulla regione di interesse (Region of Interest, \textbf{ROI})
estratta e da un regressore \textbf{Bounding Box (BB)}, che rifinisce
le regioni correggendo la stima del bounding box dall\textquoteright algoritmo
di estrazione delle ROI. La CNN pre allenata, invece, è fine-tunata
sulle classi da rilevare inserendo un FC layer dopo l\textquoteright estrazione
delle feature. Per scartare le regioni che non corrispondono a nessun
oggetto, inoltre, va inclusa la classe di background.

\subsection{Fast R-CNN}

Le R-CNN hanno delle limitazioni, fra cui la lentezza computazionale,
perciò sono state introdotte le Fast R-CNN, nelle quali:
\begin{enumerate}
\item Le immagini intere sono date in pasto alla CNN che estrae le feature
maps.
\item Le proposte di regione sono identificate a partire dall\textquoteright immagine
e proiettate nelle feature maps. Inoltre, le regioni sono direttamente
ritagliate dalle feature maps anziché dall\textquoteright immagine
(si riusa la computazione convoluzionale).
\item Una dimensione fissata è comunque richiesta per poter dare i dati
in pasto a un FC layer. Dopodichè i layer di ROI pooling estraggono
un vettore di feature di dimensione fissa H x W da ogni regione proposta.
In pratica ogni ROI nelle feature maps è diviso in una griglia H x
W su cui viene fatto un maxpooling per ottenere il feature vector
(vettore di feature).
\item I layer FC stimano sia le classi che la posizione dei bounding box
(ovvero, come nel regressore BB). Viene poi usata una combinazione
convessa (cioè una combinazione lineare con termini aventi coefficienti
non-negativi la cui somma è 1) dei due come loss multitask da ottimizzare
(come nelle R-CNN, ma senza l\textquoteright uso di regressori SVM).
\end{enumerate}
\begin{figure}[h]
\begin{centering}
\includegraphics[scale=0.35]{fast-r-cnn}
\par\end{centering}
\caption{Funzionamento Fast R-CNN}
\end{figure}
In questa architettura è quindi possibile retro-propagare l\textquoteright errore
attraverso l\textquoteright intera rete, in modo che essa venga allenata
in maniera end-to-end. Dal momento che le convoluzioni non sono ripetute
su aree sovrapposte, la stragrande maggioranza del tempo di test è
speso sull\textquoteright estrazione delle ROI.

\subsection{Faster R-CNN}

Invece dell\textquoteright algoritmo di estrazione delle ROI, le \textbf{Faster
R-CNN} allenano una \textbf{Region Proposal Network (RPN)}, la quale
è una Fully-CNN. La RPN opera sulle stesse feature maps usate per
la classificazione, cioè negli ultimi layer convoluzionali. La RPN
può quindi essere vista come un modulo aggiuntivo che migliora l\textquoteright efficienza
e concentra la Fast R-CNN sulle regioni più promettenti per il riconoscimento
degli oggetti.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{faster-r-cnn}
\par\end{centering}
\caption{RPN}
\end{figure}
Il suo scopo è quello di associare a ogni locazione spaziale k box
\textquotedblleft ancora\textquotedblright , ovvero ROI aventi scale
e ratio differenti. Tale rete produrrà in output delle ancore candidate
di dimensione H x W x k insieme a dei punteggi di stima per ogni ancora.
Il layer intermedio di una RPN è un layer convoluzionale standard
che prende come input l\textquoteright ultimo layer della rete di
estrazione delle feature e usa un filtro di dimensione $3\times3\times256$.
Tale layer è usato per ridurre la dimensionalità della feature map,
cioè mappa una certa regione a un vettore di dimensione inferiore
$H\times W\times256$. Vi sono poi 2 layer, cioè:
\begin{itemize}
\item La \textbf{rete di classificazione}: allenata per predire la \textbf{probabilità
dell\textquoteright oggetto}, cioè la probabilità che una certa ancora
contenga un oggetto (dovrà quindi produrre 2k punteggi in output,
ovvero k coppie con probabilità <contiene, non contiene>). Essa è
fatta da uno stack di layer convoluzionali di dimensione 1x1. Ognuna
delle k coppie di probabilità corrisponde a un\textquoteright ancora
specifica ed esprime la probabilità che quell\textquoteright ancora,
in quella locazione spaziale, contenga un oggetto.
\item La \textbf{rete di regressione}: allenata per aggiustare ognuna delle
k ancore predette (deve quindi fare 4k stime, ovvero una stima per
ognuna delle 4 coordinate del bounding box e per ogni bounding box).
\end{itemize}
Abbiamo detto che la RPN restituisce in outut $H\times W\times k$
proposte di regioni, quindi rimpiazza l\textquoteright algoritmo di
proposta di regioni (\textbf{selective search}). Nelle Faster R-CNN,
dopo la RPN vi è una soppressione dei non-massimi basata su punteggi
di oggettività. Le proposte rimanenti sono quindi date in pasto al
layer di ROI pooling e infine classificate dall\textquoteright architettura
Fast R-CNN standard.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.55]{faster-r-cnn_2}
\par\end{centering}
\caption{Faster R-CNN}

\end{figure}
La procedura di training è la seguente:
\begin{enumerate}
\item Allenare la RPN mantenendo congelata la rete strutturale (Deep ConvNet)
e allenando solo i layer RPN.
\item Si allena la Fast R-CNN usando le proposte di regioni restituite dalla
RPN allenata nello step precedente, cioè si fa fine-tuning dell\textquoteright intera
Fast R-CNN includendo la Deep ConvNet.
\item Si fa fine-tuning in cascata della RPN.
\item Si congela la Deep ConvNet e si fa fine-tuning solo degli ultimi layer
della Faster R-CNN.
\end{enumerate}

\section{Altri esempi di architetture}

\subsection{YOLO (You Only Look Once)}

YOLO è un metodo \textit{region-free}, in cui riformuliamo il riconoscimento
degli oggetti come un problema di regressione singolo che parte dai
pixel dell\textquoteright immagine alle coordinate dei bounding box
e alle probabilità delle classi. Esso risolve questi problemi di regressione
in una volta sola, tramite una CNN larga.

Gli step su cui si basa YOLO sono: 
\begin{enumerate}
\item Dividere l\textquoteright immagine in una griglia grossolana (di dimensione
N x M). 
\item Ogni griglia contiene B ancore (\textbf{bounding box base}) associate. 
\item Per ogni cella e per ogni ancora prediciamo:
\begin{enumerate}
\item l\textquoteright offset del bounding box base: (\textit{dx, dy, dh,
dw, punteggio dell\textquoteright oggettività}). 
\item il punteggio di classificazione del bounding box base sulle C categorie
considerate (includendo lo sfondo).
\end{enumerate}
\end{enumerate}
\begin{figure}[H]
\begin{centering}
\includegraphics{yolo}
\par\end{centering}
\caption{}
\end{figure}
L\textquoteright output della rete avrà quindi dimensione: $N\times M\times B\times(5+C)$.
L\textquoteright intera predizione è fatta in un singolo passo di
forward sull\textquoteright immagine e da una singola rete convoluzionale.
Ciò lo rende molto veloce, ma meno accurato rispetto alle R-CNN.

\subsection{Mask R-CNN}

La segmentazione d\textquoteright istanza combina le sfide di rilevamento
degli oggetti (istanze multiple presenti nell\textquoteright immagine)
e di segmentazione semantica (etichette associate a ogni pixel). Un\textquoteright architettura
che risolve tale problema è la Mask R-CNN.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.8]{mask_rccn}
\par\end{centering}
\caption{Mask R-CNN}

\end{figure}

Come nelle Fast R-CNN, viene classificata l\textquoteright intera
ROI e viene fatta una regressione sul bounding box (possibilmente
stimando la posa degli oggetti). Dentro ogni ROI viene poi fatta,
per ogni classe, una segmentazione semantica tramite la stima di una
maschera (vedi figura sotto).

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.8]{out_mask_rcnn}
\par\end{centering}
\caption{Output mask R-CNN}

\end{figure}


\chapter{Reti Ricorrenti}

\section{Modellazione sequenziale}

Finora abbiamo considerato solo dataset <<statici>>. Ora invece
ci concentreremo su dataset <<\textit{dinamici}>> in due modi:
\begin{itemize}
\item \textbf{modelli senza memoria} 
\begin{itemize}
\item \textbf{modelli autoregressivi}: predicono il prossimo input a partire
dal precedente sfruttando dei <<ritardi>>
\item \textbf{Feed Forward Neural Networks}: generalizzano i modelli autoregressivi
usando layer nascosti non-lineari
\end{itemize}
\end{itemize}
\begin{figure}[H]
\includegraphics[scale=0.4]{autoregressive}

\caption{Modello autoregressivo (sopra), FFNN (sotto)}

\end{figure}

\begin{itemize}
\item \textbf{modelli con memoria} (sistemi dinamici lineari, Hidden Markov
models, Recurrent Neural Networks)
\end{itemize}
I \textbf{sistemi dinamici} sono modelli generativi con uno stato
nascosto che non può essere osservato direttamente. Tale stato ha
delle dinamiche che possono essere affette da rumore e produce l'output.

\subsection{Sistemi dinamici lineari}

\noindent Nei \textbf{sistemi dinamici lineari} ciò si traduce nell'avere
uno stato continuo con incertezza Gaussiana che può essere stimato
usando il \textbf{Kalman filtering}. Le trasformazioni sono assunte
essere lineari.

\subsection{Hidden Markov models}

Negli \textbf{HMM} lo stato è assunto come discreto e può essere stimato
tramite \textbf{l'algoritmo di Viterbi}, mentre le transizioni sono
\textit{stocastiche} (si usa la cosiddetta ''matrice di transizione'').
L'output è una funzione stocastica degli stati nascosti.

\subsection{Recurrent Neural Networks}

Le \textbf{reti neurali ricorrenti} implementano la memoria tramite
delle connessioni chiamate \textbf{connessioni ricorrenti}. Inoltre,
gli stati distribuiti consentono di salvare efficentemente le informazioni,
mentre le dinamiche non-lineari consentono di aggiornare stati nascosti
complessi. 

\noindent \emph{''Con abbastanza neuroni e tempo, le RNN possono calcolare
qualsiasi cosa che possa essere calcolata da un computer.'' }(Computation
Beyond the Turing Limit, Hava T. Siegelmann, 1995)

\noindent 
\begin{figure}[H]
\includegraphics{RNN(1)}

\caption{Rete neurale ricorrente}
\end{figure}

\noindent Nella figura sopra sono raffigurati due tipi di neuroni:
quelli grigi, che dipendono dall'\textbf{input corrente}, e quelli
blu, che dipendono dalla \textbf{storia della rete}. Inoltre abbiamo
dei parametri nuovi chiamati:
\begin{itemize}
\item $V$ rappresenta i pesi tra neuroni blu e neuroni grigi
\item $V_{B}$ rappresenta i pesi tra i neuroni blu
\item $W$ rappresenta i pesi tra neuroni grigi
\item $W_{B}$ rappresenta i pesi tra neuroni grigi e neuroni blu
\end{itemize}
Le funzioni di output dei neuroni sono

\noindent 
\begin{equation}
h_{j}^{t}(\cdot)=h_{j}^{t}\left(\sum_{j=0}^{J}w_{ji}^{(1)}\cdot x_{i,n}+\sum_{b=0}^{B}v_{jb}^{(1)}\cdot c_{b}^{t-1}\right)
\end{equation}

\noindent 
\begin{equation}
c_{b}^{t}(\cdot)=c_{b}^{t}\left(\sum_{j=0}^{J}w_{bi}^{(1)}\cdot x_{i,n}+\sum_{b'=0}^{B}v_{bb'}^{(1)}\cdot c_{b'}^{t-1}\right)
\end{equation}

\noindent mentre la funzione di output della rete è

\begin{equation}
g^{t}(x_{n}|w)=g\left(\sum_{j=0}^{J}w_{1j}^{(2)}\cdot h_{j}^{t}(\cdot)+\sum_{b=0}^{B}v_{1b}^{(2)}\cdot c_{b}^{t}(\cdot)\right)
\end{equation}

\noindent La rete nascosta è chiamata \textbf{context network} e contiene
\emph{B} neuroni, mentre la rete ''visibile'' ne ha \emph{J}. La context
network è la parte più difficile da allenare in quanto non è una FFNN. 

\noindent La tecnica usata per allenarla è chiamata \textbf{Backpropagation
Through Time}. Gli step che la compongono sono:
\begin{enumerate}
\item dispiegare (unfold) la RNN per \emph{U} step temporali, ottenendo
una FFNN: sia \emph{N }una RNN che deve apprendere un task temporale
a partire dal tempo \emph{t-u }fino al tempo \emph{t}, e sia \emph{N{*}
}la FFNN che risulta dall'unfold della rete \emph{N:}
\begin{itemize}
\item per ogni istante di tempo nell'intervallo $(t-u,t]$ la rete \emph{N{*}
}ha un layer contenente \emph{k} neuroni, dove \emph{k} è il numero
di neuroni di \emph{N}
\item per ogni layer di \emph{N{*}} c'è una copia di ogni neurone in \emph{N}
\item per ogni istante di tempo $\tau\in(t-u,t]$ il peso sinaptico dal
neurone \emph{i }nel layer \emph{$\tau$ }al neurone \emph{j }nel
layer \emph{$\tau+1$ }di \emph{N{*} }è una copia della connessione
sinaptica dal neurone \emph{i }al neurone \emph{j} in \emph{N}
\end{itemize}
\begin{figure}[H]
\includegraphics[scale=0.85]{backprop_through_time}

\caption{Unfolding della rete}

\end{figure}

\item inizializzare le repliche di $W_{B}$e $V_{B}$in modo da renderle
uguali
\item fare una backpropagation su tutta la nuova rete: tutti i pesi sono
allenati col gradient descent, cioè considerando un istante di tempo
generico $\tau$. Applicando il solito metodo di aggiornamento, avremo
valori diversi per gli stessi pesi in diversi istanti di tempo. Ciò
non è conveniente perchè di fatto sono gli stessi pesi, quindi poi
se ne fa la media dopo averli aggiornati sulla base del gradient descent.
Una tecnica alternativa possibile è quella di aggiornare i pesi con
la media del gradiente, ottenendo quindi gli stessi risultati ma il
processo sarà più efficiente. La formula di aggiornamento per i pesi
da fornire alla context network è:
\begin{equation}
W_{B}=W_{B}-\eta\cdot\frac{1}{U}\sum_{0}^{U-1}\frac{\partial E}{\partial W_{B}^{t-u}}
\end{equation}

Mentre per mantenere la memoria del contesto si usa:
\end{enumerate}
\begin{equation}
V_{B}=V_{B}-\eta\cdot\frac{1}{U}\sum_{0}^{U-1}\frac{\partial E^{t}}{\partial V_{B}^{t-u}}
\end{equation}


\section{Vanishing Gradient}

Le RNN non riescono ad andare indietro nel passato per più di 10 step
a causa del cosiddetto effetto del \textbf{Vanishing Gradient}. Per
spiegarlo proviamo a semplificare la RNN come nella figura sotto.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.85]{RNN_simple}

\caption{}
\end{figure}

\noindent La backpropagation su un'intera sequenza \emph{S} è calcolata
come 

\begin{equation}
\frac{\partial E}{\partial w}=\sum_{t=1}^{S}\frac{\partial E^{t}}{\partial w}=\sum_{t=1}^{S}\frac{\partial E^{t}}{\partial y^{t}}\frac{\partial y^{t}}{\partial h^{t}}\frac{\partial h^{t}}{\partial h^{k}}\frac{\partial h^{k}}{\partial w}
\end{equation}

\noindent dove

\begin{equation}
\frac{\partial h^{t}}{\partial h^{k}}=\prod_{i=k+1}^{t}\frac{\partial h_{i}}{\partial h_{i-1}}=\prod_{i=k+1}^{t}v^{(1)}g'\left(h^{i-1}\right)
\end{equation}

\noindent Considerando la norma di questi termini

\begin{equation}
\Vert\frac{\partial h_{i}}{\partial h_{i-1}}\Vert=\parallel v^{(1)}\Vert\Vert g'\left(h^{i-1}\right)\Vert
\end{equation}

\noindent otteniamo che

\begin{equation}
\Vert\frac{\partial h^{t}}{\partial h^{k}}\Vert\leq\left(\gamma_{v}\gamma_{g}^{'}\right)^{t-k}
\end{equation}

\noindent dove se $\left(\gamma_{v}\gamma_{g}^{'}\right)<1$ allora
la norma converge a 0. In particolare $\gamma_{v}$ è la norma dei
pesi e $\gamma_{g}^{'}$ è la norma della derivata $g^{'}$ e vale
1 se usiamo una ReLU. Usando sigmoide o Tanh otteniamo il cosiddetto
\textbf{Vanishing Gradient}, che dimostra che più si va indietro nel
passato, più il gradiente tenderà a 0 indipendentemente dall'errore.
Basandosi sullo stesso principio, se $v^{(1)}>1$ il gradiente incrementerà
drammaticamente fino a risultare in un'<<\textit{esplosione}>> che
colpirà la capacità di apprendimento della rete.

\noindent Per risolvere questo problema si può usare la ReLU, che
forza tutti i gradienti ad essere 0 o 1. Infatti ricordiamo che:

\begin{equation}
ReLU(x)=f(x)=max(0,x)
\end{equation}

\begin{equation}
f'(x)=1_{x>0}
\end{equation}

\noindent Il trick sta quindi nel costruire la RNN usando piccoli
moduli fatti apposta per ricordare valori per tanto tempo.

\section{Long Short-Term Memories}

Nel 1997, Hochreiter\&Schmidhuber hanno risolto il problema del vanishing
gradient disegnando una \textbf{cella di memoria} che usa unità logistiche
e lineari con interazioni moltiplicative. L'informazione entra nella
cella quando il suo gate di \emph{''write''} è on, rimane nella cella
fintantochè il suo gate di \emph{''keep'' }è on, e viene infine letta
dalla cella mettendo a on il suo gate\emph{ }di \emph{''read''}. 

\noindent Il problema viene quindi risolto poichè il loop ha un peso
fissato, cioè non vanno imparati i pesi della parte di rete ''unfoldata''
dato che valgono 1.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.45]{LSTM}

\caption{LSTM}
\end{figure}

\noindent La \textbf{LSTM} è composta da:
\begin{itemize}
\item un \textbf{input gate} (ci dice quanto teniamo in memoria)
\begin{equation}
i_{t}=\sigma\left(W_{i}\cdot\left[h_{t-1},x_{t}\right]+b_{i}\right)
\end{equation}
\begin{equation}
\tilde{C}_{t}=tanh\left(W_{C}\cdot\left[h_{t-1},x_{t}\right]+b_{C}\right)
\end{equation}

\begin{figure}[H]
\includegraphics[scale=0.9]{input_gate}

\caption{Input gate}

\end{figure}

\item un \textbf{forget gate} 
\begin{equation}
f_{t}=\sigma\left(W_{f}\cdot\left[h_{t-1},x_{t}\right]+b_{f}\right)
\end{equation}

\begin{figure}[H]
\includegraphics[scale=0.9]{forget_gate}

\caption{Forget gate}

\end{figure}

\item un \textbf{memory gate}
\begin{equation}
C_{t}=f_{t}\ast C_{t-1}+i_{t}\ast\tilde{C_{t}}
\end{equation}

\begin{figure}[H]
\includegraphics[scale=0.9]{memory_gate}

\caption{Memory gate}

\end{figure}

\item un \textbf{output gate}
\begin{equation}
o_{t}=\sigma\left(W_{o}\cdot\left[h_{t-1},x_{t}\right]+b_{o}\right)
\end{equation}
\begin{equation}
h_{t}=o_{t}\ast tanh(C_{t})
\end{equation}

\begin{figure}[H]
\includegraphics[scale=0.9]{output_gate}

\caption{Output gate}

\end{figure}

\end{itemize}
Possiamo quindi costruire un grafo computazionale con trasformazioni
continue come nella figura sotto. In pratica le LSTM funzionano come
i layer convoluzionali nelle CNN perchè sono una sorta di \textit{estrattori
di features}.

\noindent 
\begin{figure}[H]
\includegraphics{LSTM(1)}\caption{LSTM in sequenza (le parti nei riquadri neri sono i layer nascosti)}

\end{figure}

\noindent Un altro approccio possibile consiste nell'usare reti \textbf{LSTM
bidirezionali} che sfruttano l'intera sequenza in input. Sono composte
da una RNN che attraversa l'intera sequenza da sinistra a destra e
da una RNN che attraversa l'intera sequenza da destra a sinistra.
Queste due RNN vengono poi concatenate per essere usate come rappresentazione
delle features. Per inizializzarle bisogna specificare lo stato iniziale
trattandolo come parametro da apprendere. L'inizializzazione può essere
fatta a 0 oppure a con valori randomici. Dopodichè si inizia a predirre
randomicamente i valori dello stato iniziale, si backpropaga l'errore
di predizione nel tempo fino allo stato iniziale e si calcola il gradiente
dell'errore rispetto ai nuovi valori dello stato iniziale.

\paragraph{Gated Recurrent Unit}

La \textbf{GRU} combina i gate di input e forget in un unico ''\textbf{update
gate}'' e fa un merge tra lo stato della cella e lo stato nascosto
(più altri cambiamenti). Le parti che caratterizzano tale unità sono:

\begin{equation}
z_{t}=\sigma\left(W_{z}\cdot\left[h_{t-1},x_{t}\right]\right)
\end{equation}
\begin{equation}
r_{t}=\sigma\left(W_{r}\cdot\left[h_{t-1},x_{t}\right]\right)
\end{equation}

\begin{equation}
\tilde{h_{t}}=tanh\left(W\cdot\left[r_{t}\ast h_{t-1},x_{t}\right]\right)
\end{equation}

\begin{equation}
h_{t}=(1-z_{t})\ast h_{t-1}+z_{t}\ast\tilde{h_{t}}
\end{equation}

\noindent 
\begin{figure}[H]
\includegraphics{GRU}

\caption{GRU}
\end{figure}

\noindent La GRU può essere usata in alternativa alla LSTM.

\section{Modellazione Sequence to Sequence}

Il \textbf{sequence modeling} risolve vari problemi in base all'architettura
usata. Le architetture possibili sono:
\begin{itemize}
\item \textbf{uno a uno}: un input di dimensione fissa viene trasformato
in un output di dimensione fissata (es.: image classification)
\item \textbf{uno a molti}: output sequenziale (es.: captioning delle immagini
che prende un'immafine in input e produce in output una sequenza di
parole)
\item \textbf{molti a uno}: input sequenziale (es.: analisi dei sentimenti
dove una data frase viene classificata sulla base della positività
o negatività dei sentimenti che esprime)
\item \textbf{molti a molti}:
\begin{itemize}
\item input e output sequenziali (es.: traduttori, cioè una RNN che legge
una frase in inglese e ne produce in output la traduzione francese)
\item input e output sequenziali sincronizzati (es.: classificazione dei
video, dei quali vogliamo etichettare ogni frame)
\end{itemize}
\end{itemize}
\begin{figure}[H]
\includegraphics[scale=0.85]{seq2seq_architectures}

\caption{Architetture seq2seq}

\end{figure}

\noindent Il modello seq2seq segue la classica \textbf{architettura
encoder-decoder}, in cui durante la fase di inferenza (e non durante
il training) il decoder dà in pasto l'output di ogni istante di tempo
all'input successivo.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.7]{enc-dec}

\caption{Architettura Encoder-Decoder}

\end{figure}

\noindent Il processo di \textit{training} è caratterizzato da una
\textbf{trasformazione} delle parole a identificativi e da una fase
di \textbf{embedding}. Il processo di inferenza (testing) è caratterizzato
anch'esso da una prima fase di \textbf{embedding} e da una \textbf{trasformazione}
di identificativi in parole. La fase di embedding consiste nella rappresentazione
delle parole usando un vettore di parole (vocabolario) denso. L'encoder
può essere una RNN così come una CNN, ad esempio nel caso di image
captioning.

\noindent Vi sono inoltre dei \textbf{caratteri speciali} che vengono
dati in input al decoder:
\begin{itemize}
\item <PAD>: durante il training gli esempi sono dati in pasto alla rete
in batch, i cui input devono essere della stessa larghezza. Questo
carattere viene usato per ''gonfiare'' input più corti rendendoli
della stessa dimensione del batch
\item <EOS>: necessario per il batching nel decoder, ovvero indica al decoder
dove finisce la frase e gli consente di indicare la stessa cosa nel
suo output
\item <UNK>: su dati reali può ampiamente migliorare l'efficienza delle
risorse per ignorare le parole che non si presentano abbastanza spesso
nel vocabolario (rimpiazzandole quindi con questo carattere)
\item <SOS>/<GO>: questo è l'input del decoder nel primo istante di tempo
per consentirgli di capire quando iniziare a generare l'output
\end{itemize}
La \textbf{preparazione dei batch} si compone dei seguenti step:
\begin{enumerate}
\item prendere un campione di coppie <source\_sequence, target\_sequence>
di dimensione batch\_size
\item appendere <EOS> a source\_sequence
\item prependere <SOS> a target\_sequence per ottenere la target\_input\_sequence
e appendere <EOS> per ottenere la target\_output\_sequence
\item ''gonfiare'' le frasi fino alla max\_input\_length (o max\_target\_length)
all'interno dello stesso batch usando il token <PAD>
\item codificare i token basandosi sul vocabolario (embedding)
\item sostituire i token OOV (out of vocabulary) con <UNK> e calcolare la
lunghezza di ogni sequenza di input e target nel batch
\end{enumerate}
Il \textbf{modello seq2seq} ci dice quindi che, data una coppia <S,T>,
legge S e restituisce in output T' che corrisponda a T. In pratica
il problema riguarda la probabilità di ottenere una sequenza in output
data una sequenza in input:

\begin{equation}
p(y_{1},...,y_{T^{'}}|x_{1},...,x_{T})=\prod_{t=1}^{T^{'}}p(y_{t}|v,y_{1},...,y_{t-1})
\end{equation}

\noindent dove \emph{v }è lo stato nascosto. Quindi, tale problema
si riconduce alla \textbf{massimizzazione della crossentropy media}:

\begin{equation}
\frac{1}{|S|}\sum_{(T,S)\in\mathcal{S}}\mathrm{log}p(T|S)
\end{equation}


\section{Macchine neurali di Turing}

Le \textbf{Neural Turing Machines} combinano una RNN con una banca
di memoria esterna, cioè un array di vettori. La rete principale scrive
su e legge da questa memoria a ogni step.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.8]{neural_turing_machine}

\caption{Neural Turing Machine}

\end{figure}

\noindent La sfida è quella di imparare cosa scrivere/leggere e dove
scrivere/leggere. La soluzione è quella di, a ogni step, leggere e
scrivere ovunque ma con diversa misura. Tale meccanismo è chiamato
\textbf{attention mechanism} e si basa sul porre l'\textbf{attenzione}
su una specifica parte della memoria per produrre l'output. In pratica
la RNN dà una \textbf{distribuzione di attenzione} (dettata dalla
softmax) che descrive come distribuiamo in memoria ciò che ci interessa
(vedi figura sotto).

\noindent 
\begin{figure}[H]
\includegraphics{attention}

\caption{Attention mechanism in lettura}

\end{figure}

\noindent Il risultato della lettura è una somma pesata differenziabile:

\begin{equation}
r\leftarrow\sum_{i}a_{i}M_{i}
\end{equation}

\noindent In fase di scrittura, invece, anzichè scrivere in una posizione
specifica, scriviamo ovunque ma con diversa misura. La RNN dà una
distribuzione di attenzione, descrivendo quanto dovremmo cambiare
ogni posizione della memoria nella direzione del valore di scrittura.
Sotto vi è una figura che spiega questo passaggio.

\noindent 
\begin{figure}[H]
\includegraphics{attention_write}

\caption{Attention mechanism in scrittura}

\end{figure}

\noindent L'aggiornamento della memoria si basa sulla formula seguente:

\begin{equation}
M_{i}\leftarrow a_{i}w+(1-a_{i})M_{i}
\end{equation}


\paragraph{Meccanismo di attenzione}

\noindent Vi sono due componenti principali in una Neural Turing Machine:
l'\textbf{attention mechanism} e il \textbf{RNN controller}. L'architettura
è inoltre divisa in due parti: 
\begin{itemize}
\item \textbf{content-based attention}: cerca in memoria e si concentra
su posti che corrispondono a ciò che si sta cercando
\item \textbf{location-based attention}: consente il movimento relativo
nella memoria, abilitando la NTM a ciclare
\end{itemize}
\begin{figure}[H]
\includegraphics{ntm_attention}

\caption{Attention in una NMT}

\end{figure}

\noindent Innanzitutto, il controller dà un \textbf{query vector}
(output della RNN, usato come meccanismo di indirizzamento) all'attention
mechanism e a ogni entry in memoria viene assegnato un punteggio basandosi
sulla similarità con il vettore. I punteggi vengono quindi convertiti
in una \textbf{distribuzione} di probabilità usando la softmax. Poi
si \textbf{interpola} il risultato della softmax con l'attention generata
dallo step precedente, tenendo conto di un \textbf{interpolation amount}
che rappresenta la velocità di \textit{interpolazione}, che è un parametro
predetto dalla RNN (vedi figura 5.17: se è 0 si prende solo A, se
è 1 si prende solo B). Fatto ciò, viene effettuata una convoluzione
tra l'attenzione e un filtro di shift, che consente al controller
di spostare il focus dell'attention mechanism su una particolare zona.
Infine, si affina la \textbf{distribuzione dell'attention}, che viene
poi data in pasto all'operazione di read o write.

\noindent Consideriamo il seguente dataset sequenziale:$\left\{ \left(\left(x_{1},...,x_{n}\right),\left(y_{1},...,y_{n}\right)\right)\right\} _{i=1}^{N}$.

\noindent Il ruolo del decoder è quello di modellare la probabilità
generativa $P(y_{1},$...,y$_{m}|x)$. Nei modelli seq2seq \textit{''vanilla''}
(convenzionali), il decoder viene condizionato inizializzando lo stato
iniziale con l'ultimo stato dell'encoder. Ciò funziona bene per frasi
medio-corte, mentre per frasi lunghe ciò diventa un \textit{bottleneck}.

\noindent La \textbf{funzione di attention} mappa il query vector
e l'insieme di coppie chiave-valore a un output, che è calcolato come
la somma pesata dei valori, dove il peso assegnato a ogni valore è
calcolato da una \textbf{funzione di compatibilità}. Tale funzione:
\begin{enumerate}
\item compara lo stato nascosto corrente $h_{t},$ con stati di origine
h$_{s}$ per derivare l'attention usando una funzione di scoring tra
le due seguenti: 
\begin{equation}
score(\boldsymbol{h_{t}},\boldsymbol{\bar{h_{s}}})=\begin{cases}
\boldsymbol{h_{t}^{T}}\boldsymbol{W\bar{h_{s}}}\\
\boldsymbol{v_{a}^{T}}tanh\left(\boldsymbol{W_{1}h_{t}}+\boldsymbol{W_{2}\bar{h_{s}}}\right)
\end{cases}
\end{equation}
\item applica la softmax sugli score e calcola i pesi dell'attention, uno
per ogni token dell'encoder, nel modo seguente:
\begin{equation}
\alpha_{ts}=\frac{e^{score(\boldsymbol{h_{t}},\boldsymbol{\bar{h_{s}}})}}{\sum_{s'=1}^{S}e^{score(\boldsymbol{h_{t}},\boldsymbol{\bar{h_{s'}}})}}
\end{equation}
\item calcola il \textbf{context vector} come media pesata degli stati di
origine:
\begin{equation}
\boldsymbol{c_{t}=}\sum_{s}\alpha_{ts}\boldsymbol{\bar{h_{s}}}
\end{equation}
\item combina il context vector con lo stato nascosto target corrente per
produrre il vettore di attention finale:
\begin{equation}
\boldsymbol{a_{i}=}f\left(c_{t},\boldsymbol{h_{i}}\right)=tanh\left(\boldsymbol{W_{c}}[\boldsymbol{c_{t}};\boldsymbol{h_{t}}]\right)
\end{equation}
\end{enumerate}
\begin{figure}[H]
\includegraphics[scale=0.6]{attention_function}

\caption{Esempio di funzione di compatibilità}

\end{figure}

\noindent Per visualizzare i pesi dell'attention tra le frasi di \textcolor{red}{source}
e \textcolor{blue}{target }si può usare la cosiddetta \textbf{alignment
matrix}. Per ogni passo di decodifica (cioè per ogni token target
generato) descrive quali sono i token di origine che sono più presenti
nella somma pesata e che hanno quindi condizionato la decodifica.
L'attention è quindi uno strumento che, durante la decodifica, consente
alla rete di prestare più attenzione a varie parti della frase originale.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.7]{attention_visual}

\caption{Esempio di alignment matrix}

\end{figure}

\noindent Tra le possibili applicazioni del meccanismo di attention,
oltre alle traduzioni, troviamo il riconoscimento vocale, l'image
captioning e la generazione automatica di risposte (es.: chatbot).

\paragraph{Chatbot}

I \textbf{chatbot} possono essere definiti lungo due dimensioni:
\begin{itemize}
\item l'\textbf{algoritmo core}:
\begin{itemize}
\item \textbf{generativo}: codifica la domanda in un context vector e genera
la risposta parola per parola usando la distribuzione della probabilità
condizionata sul vocabolario della risposta (modello encoder-decoder) 
\item \textbf{retrieval}: si basa sulla conoscenza di base delle coppie
domanda-risposta, cioè quando viene fatta una nuova domanda, la fase
di inferenza la codifica in un context vector e recupera, usando misure
di similarità, i top-k nearest neighbours basandosi sulla conoscenza
di base posseduta
\end{itemize}
\item la \textbf{gestione del contesto}:
\begin{itemize}
\item \textbf{single-turn}: costruisce il vettore di input considerando
la domanda in arrivo (può perdere informazioni importanti riguardanti
la storia della conversazione e generare, quindi, risposte irrilevanti,
ma è più facile da implementare e allenare)
\[
\left\{ \left(\boldsymbol{q}_{i},\boldsymbol{a}_{i}\right)\right\} 
\]
\item \textbf{multi-turn}: costruisce il vettore di input considerando un
contesto di conversazione ''multi-turn'', cioè basandosi sulla domanda
in arrivo ma anche sulle domande precedenti
\[
\left\{ \left(\left[\boldsymbol{q}_{i-2};\boldsymbol{a}_{i-2};\boldsymbol{q}_{i-1};\boldsymbol{a}_{i-1};\boldsymbol{q}_{i}\right],\boldsymbol{a}_{i}\right)\right\} 
\]
\end{itemize}
\end{itemize}
I chatbot generativi, in particolare, usano una RNN e la trainano
per mappare ciò che viene detto dalla prima persona a ciò che viene
risposto dal bot basandosi sempre sull'attention mechanism. Tale meccanismo
di risposta single-turn è stato esteso nel 2017 in un \textbf{attention
mechanism gerarchico}.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.6]{hier_gen_chatbot}

\caption{Chatbot gerarchico multi-turn generativo}

\end{figure}

\noindent In tale modello si usano le seguenti formule:

\begin{equation}
\boldsymbol{U}=\left(u_{1},...,u_{m}\right)
\end{equation}

\begin{equation}
\boldsymbol{u}_{i}=\left(w_{i,1},...,w_{i,T_{i}}\right)
\end{equation}
\begin{equation}
\boldsymbol{h}_{i,k}=concat\left(\overrightarrow{\boldsymbol{h}_{i,k}},...,\overleftarrow{\boldsymbol{h}_{i,k}}\right)
\end{equation}
\begin{equation}
\boldsymbol{r}_{i,t}=\sum_{j=1}^{T_{i}}\boldsymbol{\alpha}_{i,t,j}\boldsymbol{h}_{i,j}
\end{equation}
\begin{equation}
e_{i,t,j}=\eta\left(\boldsymbol{s}_{t-1},\boldsymbol{1}_{i+1,t},\boldsymbol{h}_{i,j}\right)
\end{equation}
\begin{equation}
\boldsymbol{\alpha}_{i,y,j}=\frac{exp\left(e_{i,t,j}\right)}{\sum_{k=1}^{T_{i}}exp\left(e_{i,t,j}\right)}
\end{equation}
\begin{equation}
\left(\boldsymbol{1}_{i,t},...,\boldsymbol{1}_{m,t}\right)
\end{equation}
\begin{equation}
\boldsymbol{c}_{t}=\sum_{i=1}^{m}\beta_{i,t}\boldsymbol{1}_{i,t}
\end{equation}

\noindent dove $\boldsymbol{\alpha}_{i,y,j}$ è la \textbf{softmax}.
Le reti di attenzione gerarchiche sono state usate anche per la classificazione
degli argomenti (es.: dataset di Yahoo Answers), così come per l'analisi
dei sentimenti. 

\section{Trasformatore}

Un modello \textbf{trasformatore} è caratterizzato da uno stack di
encoder che codificano la sequenza in input e da uno stack di decoder
che producono l'output. Ogni encoder e decoder hanno una struttura
interna che consente di non usare un modello ricorrente. Ciò consente
di allenare il modello con una quantità molto vasta di parole. L'elemento
più importante che li caratterizza è la self-attention, che presta
attenzione a una parola basandosi sulle parole che sono connesse ad
essa.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.6]{enc-dec_transformer}

\caption{Encoder e decoder di un transformer}

\end{figure}

\noindent Andando più nel dettaglio, gli elementi che compongono un
trasformatore sono:
\begin{itemize}
\item \textbf{dot-product attention scalata}
\item \textbf{multi-head attention}
\item \textbf{FFNN posizionale}
\item \textbf{embedding e softmax}
\item \textbf{codifica posizionale}
\end{itemize}
\begin{figure}[H]
\includegraphics{transformer}

\caption{Architettura del Transformer}

\end{figure}

\noindent L'attention qui è definita come
\begin{equation}
Attention(Q,K,V)=softmax\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V
\end{equation}

\noindent In particolare, Q, K e V sono delle matrici che indicano
rispettivamente l'encoding delle parole in input (\textbf{query}),
la trasformazione delle query in chiavi (\textbf{key}) e il valore
di queste parole trasformate in chiavi (\textbf{value}). Nella dot-product
\textbf{attention scalata}, a ogni step viene paragonato ogni valore
di query $q_{i}$ con ogni altro valore chiave $k_{j}$. 

\noindent Per quanto riguarda la multi-head attention, si cerca di
calcolare la dot-product attention più volte in parallelo e poi concatenare
i risultati. 

\noindent Dopodichè si ha una \textbf{FFNN} che, per ogni parola in
input, calcola la \textbf{multi-head attention} per quella parola
e poi calcola una funzione non-lineare (sempre per la stessa parola):

\begin{equation}
FFNN\left(x\right)=max\left(0,xW_{1}+b_{1}\right)W_{2}+b_{2}
\end{equation}

\noindent Vengono quindi ripetute le stesse operazioni sul decoder
in modo tale da ottenere in output, per ogni parola in input, la probabilità
di generare la 'giusta' parola (ad esempio, se in input ho ''A B C''
e in output voglio avere ''D E F'', il decoder mi calcolerà qual è
la probabilità di generare D al primo step, E al secondo e F al terzo).

\noindent Infine, per tenere traccia della posizione delle parole
in input si aggiunge un \textbf{encoding posizionale}, che assegna
una sorta di timestamp alle parole. Ci sono due tipi di encoding posizionale,
cioè il \textbf{learned positional embedding} e la \textbf{sinusoide}.
Quest'ultima può essere:

\begin{equation}
PE_{(pos,2i)}=sin\left(\frac{pos}{1000^{2i/d_{model}}}\right)
\end{equation}

\noindent oppure:

\begin{equation}
PE_{(pos,2i+1)}=cos\left(\frac{pos}{1000^{2i/d_{model}}}\right)
\end{equation}


\section{Word embedding}

La performance di un'applicazione nel mondo reale (es.: chatbot, classificatori
di documenti, sistemi di recupero delle informazioni) dipende dalla
codifica dell'input, che può essere basata su:
\begin{itemize}
\item rappresentazioni locali
\begin{itemize}
\item \textbf{N-gram}
\item \textbf{bag-of-words}
\item \textbf{codifica 1-di-N} 
\end{itemize}
\item rappresentazioni continue
\begin{itemize}
\item \textbf{Latent Semantic Analysis}
\item \textbf{Latent Dirichlet Allocation}
\item \textbf{rappresentazioni distribuite}
\end{itemize}
\end{itemize}
In particolare nella \textbf{rappresentazione N-gram} si cerca di
determinare la probabilità $P(s=w_{1},...,w_{k})$ di una frase per
capire qual è il modello sottostante di un certo documento. Questo
viene ottenuto tramite il calcolo della probabilità congiunta
\begin{equation}
P(s_{k})=\prod_{i}^{k}P(w_{i}|w_{1},...,w_{i-1})
\end{equation}

\noindent Dal momento che nei modelli di lingua tradizionali n-gram
''\textit{la probabilità di una parola dipende solo dal contesto delle
n-1 parole precedenti}'', la probabilità diventa

\begin{equation}
\hat{P}(s_{k})=\prod_{i}^{k}P(w_{i}|w_{i-n+1},...,w_{i-1})
\end{equation}

\noindent Un processo tipico di apprendimento \textit{ML-smoothing}
effettua una levigazione della probabilità per evitare di avere probabilità
pari a 0. Per fare ciò calcola la probabilità

\begin{equation}
\hat{P}(w_{i}|w_{i-n+1},...,w_{i-1})=\frac{\#w_{i-n+1},...,w_{i-1},w_{i}}{\#w_{i-n+1},...,w_{i-1}}
\end{equation}


\paragraph{Neural Net Language Model}

\noindent Si definisce \textbf{embedding} una tecnica che mappa una
parola (o frase) dal suo spazio di input alto-dimensionale (il corpo
di tutte le parole) a uno spazio vettoriale numerico di dimensione
inferiore.

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.7]{embedding_esempi}

\caption{Esempi di embedding}

\end{figure}

\noindent Ogni parola unica \textbf{\emph{w}} in un vocabolario V
(con dimensione tipicamente nell'ordine di$10^{6}$) viene quindi
mappata in uno spazio m-dimensionale continuo (con dimensione tipicamente
compresa nel range 100 < m < 500). Un modello tipico per il \textit{word
embedding} è il\textbf{ Neural Net Language Model}. Per ogni sequenza
di training, si ha che l'input è costituito da una coppia <contesto,
target> fatta così: <$w_{t-n+1}...w_{t-1},w_{t}$>. L'obiettivo, inoltre,
è quello di minimizzare la funzione di errore
\begin{equation}
E=-log\hat{P}\left(w_{t}|w_{t-n+1}...w_{t-1}\right)
\end{equation}

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.7]{neural_net_language_model}

\caption{Neural Net Language Model}

\end{figure}

\noindent Il \textbf{layer di proiezione} contiene i vettori in $C_{|V|,m}$.
La softmax è usata per produrre in output una \textbf{distribuzione
multinomiale} con probabilità
\begin{equation}
\hat{P}(w_{i}=w_{t}|w_{t-n+1}...w_{t-1})=\frac{e^{y_{w_{i}}}}{\sum_{i'}^{|V|}e^{y_{w_{i'}}}}
\end{equation}

\noindent dove:
\begin{itemize}
\item $y=b+U\cdot tanh(d+Hx)$
\item \emph{x} è la concatenazione \emph{C(w) }del contesto dei vettori
dei pesi
\item \emph{d} e \emph{b} sono dei bias, rispettivamente degli elementi
\emph{h} e \emph{|V|}
\item \emph{U }è la matrice di dimensione $|V|\times h$ dei pesi tra il
layer nascosto e il layer di output
\item \emph{H} è la matrice di dimensione $h\times(n-1)\cdot m$ dei pesi
tra il layer di proiezione e il layer nascosto
\end{itemize}

\paragraph{Word2vec di Google}

L'idea è quella di ottenere una performance che consente il training
di un modello meno profondo su una quantità di dati più grande. In
questo modello non ci sono layer nascosti, il layer di proiezione
è condiviso (non solo la matrice dei pesi) e il contesto contiene
le parole provenienti sia dalla storia che dal futuro.

\noindent Le due architetture possibili sono l'\textbf{architettura
skip-gram} e l'\textbf{architettura bag-of-words continua} (vedi figura
sotto).

\noindent 
\begin{figure}[H]
\includegraphics{word2vec}

\caption{Architettura skip-gram (a sinistra) e architettura bag-of-words (a
destra)}
\end{figure}

\noindent In particolare, nell'architettura bag-of-words continua
(\textbf{CBOW}), per ogni sequenza di training, si ha che l'input
è costituito da una coppia <contesto, target> fatta così: <$w_{t-n+1}...w_{t-1}w_{t+1}...w_{t+\frac{n}{2}},w_{t}$>.
L'obiettivo, inoltre, è quello di minimizzare la funzione di errore
\begin{equation}
E=-log\hat{P}\left(w_{t}|w_{t-\frac{n}{2}}...w_{t-1}w_{t+1}...w_{t+\frac{n}{2}}\right)
\end{equation}

\noindent 
\begin{figure}[H]
\includegraphics[scale=0.7]{cbow}

\caption{CBOW}

\end{figure}

\noindent In pratica, per ogni coppia <contesto, target> solo le parole
di contesto vengono aggiornate. Se la probabilità $\hat{P}(w_{i}=w_{t}|context)$
è sovrastimata, una certa porzione di $C'(w_{i})$ viene sottratta
dai vettori delle parole contestuali in $C_{|V|,m}$; se invece è
sottostimata, la porzione viene aggiunta ai vettori delle parole contestuali
in $C_{|V|,m}$. 

\noindent Applicazioni di \textbf{word2vec} riguardano la classificazione
di documenti e l'analisi sentimentale.

\paragraph{GloVe}

\textbf{GloVe} fa esplicitamente ciò che word2vec fa implicitamente,
ovvero codifica il significato come \textbf{vettore di offset} in
uno spazio di embedding. Tale significato viene codificato da dei
rapporti di probabilità di co-occorrenza. L'allenamento avviene con
una \textit{least squares pesata}

\begin{equation}
J=\sum_{i,j=1}^{V}f\left(X_{ij}\right)\left(w_{i}^{T}\tilde{w_{j}}+b_{i}+\tilde{b_{j}}-logX_{ij}\right)^{2}
\end{equation}

\noindent In pratica questo metodo si basa sull'individuazione dei
\textbf{nearest neighbours} di una certa parola.

\chapter{Autoencoder}

\section{Struttura}

Un' \textbf{autoencoder} è un tipo di rete neurale che nella sua formulazione
più semplice cerca di apprendere la \textbf{funzione identità}. In
altre parole impara a produrre una copia dell'input. 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=1.1]{Autoencoder_schema}
\par\end{centering}
\caption{Schema generale di un autoencoder}
\end{figure}
È composto principalmente da due parti.
\begin{enumerate}
\item \textbf{Encoder}, la cui funzione è mappare l'input in una rappresentazione
latente di dimensione minore ed eventualmente con altri vincoli. 
\item \textbf{Decoder} che mappa la rappresentazione latente nell'output.
\end{enumerate}
Formalmente possiamo descrivere un autoencoder attraverso tre funzioni
matematiche:

\[
{\displaystyle \phi:{\mathcal{X}}\rightarrow{\mathcal{F}}}
\]

\[
{\displaystyle \psi:{\mathcal{F}}\rightarrow{\mathcal{X}}}
\]

\begin{equation}
{\displaystyle \phi,\psi={\underset{\phi,\psi}{\operatorname{arg\,min}}}\,\|X-(\psi\circ\phi)X\|^{2}}
\end{equation}

$X\in\mathbb{R}^{d}$ e $F\in\mathbb{R}^{n}$ con $n\ll d$. Le features
$F$ vengono solitamente chiamate \textbf{rappresentazione latente}.
Lo schema di un modello di autoencoder è abbastanza generale ed è
quindi implementabile tramite varie architetture come \textit{feedforward,
convoluzionali, ricorrenti }e con varie profondità della rete, andando
incontro al ben noto \textit{trade-off} tra capacità di approssimazione
e sovradattamento.\textit{ }

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.85]{autoencoder}
\par\end{centering}
\caption{Autoencoder feedforward}
\end{figure}


\section{Utilizzo delle reti autoencoder}

\subsection{Riduzione dimensionale}

Una rete autoencoder è usata principalmente come algoritmo di \textbf{riduzione
dimensionale} (o compressione) con determinate proprietà:
\begin{itemize}
\item \textbf{Data specificità}: l'autoencoder è adatta a comprimere solo
dati simili a quelli su cui è allenata, o addirittura solo questi
ultimi. Apprende features specifiche, quindi un' autoencoder allenata
a comprimere immagini di caratteri scritti a mano non sarà adatta
a comprimere foto generiche.
\item \textbf{Lossy}: l'output non sarà esattamente identico all'input,
si ha quindi una degradazione.
\item \textbf{Self-supervized}.
\end{itemize}
Una caratteristica spesso cercata nella rappresentazione latente è
la \textbf{sparsità. }Questa proprietà può essere raggiunta aggiungendo
un termine di penalità proporzionale alla rappresentazione latente.
Tipicamente viene utilizzata la norma L1. Riprendendo la notazione
formale precedente la loss function diventa:

\begin{equation}
L=\|X-(\psi\circ\phi)X\|^{2}+\lambda\mathop{\sum_{i}|f_{i}|}
\end{equation}

Minimizzando il secondo termine imponiamo che la rappresentazione
latente abbia la somma dei valori assoluti delle singole componenti
minima ma contemporaneamente minimizzando il primo termine imponiamo
che la rappresentazione latente sia in grado di separare significativamente
le features. Idealmente, se non considerirassimo la dimensionalità,
una rappresentazione efficacie a minimizzare questa loss function
sarebbe la \textit{one-hot encode.} Questa considerazione suggerisce
che la rete tenderà comunque ad una rappresentazione in cui la maggior
parte delle componenti $f_{i}$ siano tendenti a zero e solo il minimo
necessario di componenti siano con modulo significativamente diversi
da zero.

\subsection{Eliminazione del rumore}

È possibile usare le reti autoencoder per rimuovere e/o attenuare
il rumore nei dati. Per far ciò, durante il training, ai dati di input
viene aggiunto del \textbf{rumore} e la rete tenterà di ricostruire
l'input originale. Oltre a poter utilizzare l'autoencoder per rimuovere
il rumore dai dati, aggiungere del rumore in input permette alla rete
di apprendere una \textbf{rappresentazione latente} più robusta, in
quanto è forzata ad estrarre features idealmente indipendenti dalle
fluttuazioni dovute al rumore. Questo può migliorare le prestazioni
anche negli altri casi d'uso.

\begin{figure}[H]
\includegraphics[scale=0.12]{denoising_autoencoder}
\centering{}\caption{Schema denoising autoencoder}
\end{figure}


\subsection{Inizializzatore di pesi}

Gli autoencoder possono essere utilizzate per \textbf{inizializzare}
reti neurali con altri compiti, come per esempio un classificatore.
In particolare quando si hanno pochi dati già classificati per il
training. Per far questo si procede in questo modo:
\begin{enumerate}
\item Si progetta la rete per classificare normalmente. Ricordando la struttura
delle reti CNN (senza perdere generalità) per la classificazione ricordiamo
che essa è strutturata in due parti. La prima parte è incaricata di
estrarre le features mentre la seconda usa le features per classificare
effettivamente. Sostituiamo la seconda parte della rete con il simmetrico
della prima parte. In questo modo abbiamo una struttura che produce
un output della stessa dimensione dell'input. Di fatto abbiamo ottenuto
una rete autoencoder. Alleniamo l'autoencoder ottenuto con i dati
privi di etichette.
\item Scartiamo il decoder e manteniamo l'encoder con i pesi ottenuti dall'allenamento.
\item Connettiamo la parte precendentemente sostituita dal decoder (classificatore).
\item Usiamo la tecnica del fine tuning per completare l'allenamento con
i pochi dati etichettati.
\end{enumerate}
Le reti autoencoder forniscono una buona inizializzazione, riducendo
anche il rischio di overfitting, perchè dovendo approssimare la funzione
identità apprendono una rappresentazione latente su dati simili ma
non uguali a quelli effettivamente usati nel training.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{classificatore_autoencoder}
\par\end{centering}
\caption{Classificatore inizializzato con autoencoder}
\end{figure}


\subsection{Autoencoder generativi}

I \textbf{Generative Models} o modelli generativi permettono di creare
dati come foto, film, musica, testi e usarli negli ambiti più diversi
come \textbf{data augmentation}, simulazioni e pianificazioni. È possibile
tecnicamente usare autoencoder per generare nuovi dati, ma non senza
qualche criticità. Un opzione per generare nuovi dati potrebbe essere
la seguente:
\begin{enumerate}
\item Alleniamo l'autoencoder su un insieme di dati $X$ in modo da costruire
una rappresentazione latente $F$.
\item Scartiamo l'encoder
\item Generiamo dei vettori casuali nello spazio $F$ e li usiamo come input
del decoder che produrra in output un nuovo dato.
\end{enumerate}
\begin{figure}[H]
\includegraphics[scale=0.45]{Autoencoder_sampling}\caption{Generazione con autoencoder}

\end{figure}

Mentre i punti 1 e 2 non creano eccessivi problemi, il punto 3 è critico.
Non sappiamo nulla a priori della distibuzione che assume lo spazio
latente $F$ quindi non siamo in grado di generare con alta probabilità
dei vettori di input per il decoder che producano un output sensato.
\begin{center}
\begin{figure}[h]
\begin{centering}
\includegraphics[scale=0.4]{autoencoder_generativo}
\par\end{centering}
\centering{}\caption{Spazio latente irregolare}
\end{figure}
\par\end{center}

Osservando l'esempio qui sopra vediamo una rete autoencoder allenata
a rappresentare figure geometriche chiuse. Nel momento in cui diamo
in input un vettore casuale (punto viola) notiamo che il decoder genera
una figura senza significato per il contesto. Questo avviene perchè
non campioniamo con la funzione di probabilità costruita dalla rete.
Questa distribuzione ($\phi_{f}$) è estremamente difficile da stimare
a priori e devono essere applicate tecniche sofisticate come i \textbf{variational
autoencoder}.
\begin{thebibliography}{1}
\bibitem{key-1}Lu, Z., Pu, H., Wang, F., Hu, Z., \& Wang, L. (2017).
The Expressive Power of Neural Networks: A View from the Width. Neural
Information Processing Systems, 6231-6239.

\bibitem{key-2}Slides delle lezioni

\bibitem{key-3}Appunti di PoliMI Data Scientists del corso di Soft
Computing
\end{thebibliography}

\end{document}
