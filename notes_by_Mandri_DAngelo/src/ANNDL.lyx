#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{algorithm,algpseudocode}

\usepackage[]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language italian
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style french
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{titlepage}    
\end_layout

\begin_layout Plain Layout

	
\backslash
begin{center}
\end_layout

\begin_layout Plain Layout

       
\end_layout

\begin_layout Plain Layout

		
\backslash
vspace*{1cm}
\end_layout

\begin_layout Plain Layout

		
\backslash
Huge
\end_layout

\begin_layout Plain Layout

        
\backslash
textbf{Articial Neural Networks and Deep Learning}
\end_layout

\begin_layout Plain Layout

                          
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

		
\backslash
vspace{1.5cm}
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

		
\backslash
Large
\end_layout

\begin_layout Plain Layout

        
\end_layout

\begin_layout Plain Layout

		
\backslash
textbf{Antonino Elia Mandri 
\backslash

\backslash
 Stefano D'Angelo}
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

                           		
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

		
\backslash
vspace{1.5cm}      
\end_layout

\begin_layout Plain Layout

       		
\end_layout

\begin_layout Plain Layout

		
\backslash
includegraphics[width=0.8
\backslash
textwidth]{polimi.png}
\end_layout

\begin_layout Plain Layout

		
\backslash
vspace{1.5cm}
\end_layout

\begin_layout Plain Layout

                          
\end_layout

\begin_layout Plain Layout

		Anno Accademico 2020/2021                
\end_layout

\begin_layout Plain Layout

	
\backslash
end{center} 
\end_layout

\begin_layout Plain Layout


\backslash
end{titlepage}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Introduzione
\end_layout

\begin_layout Section
Apprendimento automatico
\end_layout

\begin_layout Standard
Il 
\series bold
machine learning
\series default
 è una branca dell'intelligenza artificiale che raccoglie un insieme di
 metodi quali: statistica computazionale, riconoscimento di pattern, reti
 neurali artificiali, filtraggio adattivo, teoria dei sistemi dinamici,
 elaborazione delle immagini, data mining, algoritmi adattivi, ecc; che
 utilizza metodi statistici per migliorare progressivamente la performance
 di un algoritmo nell'identificare pattern nei dati.
 Nell'ambito dell'informatica, l'apprendimento automatico è una variante
 alla programmazione tradizionale nella quale si predispone in una macchina
 l'abilità di apprendere qualcosa dai dati in maniera autonoma, senza ricevere
 istruzioni esplicite a riguardo.
 
\end_layout

\begin_layout Standard
\noindent
Immaginiamo di possedere un insieme di una certa esperienza E, per esempio
 dei dati, che chiameremo 
\begin_inset Formula $D=x_{1},x_{2},...,x_{N}$
\end_inset

, definiamo quindi i seguenti paradigmi di apprendimento:
\end_layout

\begin_layout Itemize
apprendimento supervisionato (
\series bold
supervised learning
\series default
): in cui al modello vengono forniti degli output desiderati 
\begin_inset Formula $t_{1},t_{2},...,t_{N}$
\end_inset

 e l'obiettivo è quello di estrarre una regola generale che associ l'input
 
\begin_inset Formula $D$
\end_inset

 all'output corretto;
\end_layout

\begin_layout Itemize
apprendimento non supervisionato (
\series bold
unsupervised learning
\series default
): è una tecnica di apprendimento automatico che consiste nel fornire al
 sistema informatico una serie di input (esperienza del sistema), 
\begin_inset Formula $D$
\end_inset

 nel nostro caso, che egli riclassificherà ed organizzerà sulla base di
 caratteristiche comuni per cercare di effettuare ragionamenti e previsioni
 sugli input successivi; 
\end_layout

\begin_layout Itemize
L'apprendimento per rinforzo (
\series bold
reinforcement learning
\series default
): è una tecnica di apprendimento automatico che punta ad attuare sistemi
 in grado di apprendere ed adattarsi alle mutazioni dell'ambiente in cui
 sono immersi, attraverso la distribuzione di una "ricompensa" detta rinforzo
 che consiste nella valutazione delle loro prestazioni.
 Il modello produce una serie di azioni 
\begin_inset Formula $a_{1},a_{2},...,a_{N}$
\end_inset

 che interagiscono con l'ambiente e ricevendo una serie di ricompense 
\begin_inset Formula $r_{1},r_{2},...,r_{N}$
\end_inset

 impara a produrre azioni che massimizzino le ricompense nel lungo periodo.
\end_layout

\begin_layout Standard
Una definizione di cosa sia il machine learning fu data da Mitchell nel
 97: 
\end_layout

\begin_layout Standard
\begin_inset Quotes qld
\end_inset


\shape italic
A computer program is said to learn from experience E with respect to some
 class of task T and a performance measure P, if its performance at tasks
 in T, as measured by P, improves because of experience E
\shape default

\begin_inset Quotes qrd
\end_inset


\end_layout

\begin_layout Section
Deep learning
\end_layout

\begin_layout Standard
Senza addentrarci nel complesso e variegato mondo del machine learning,
 sappiamo in linea generale che si tratta di algoritmi che fanno largo uso
 della statistica.
 Funzionano bene su un'ampia varietà di problemi.
 Tuttavia, tali algortimi, non sono riusciti a risolvere i problemi centrali
 dell'IA, come il riconoscimento del linguaggio o il riconoscimento di oggetti
 e altri ancora.
 Ciò avviene sonstanzialmente a causa dell'alta dimensionalità dei dati
 da trattare.
 I meccanismi usati dal machine learning per generalizzare risultano insufficien
ti per apprendere complesse funzioni multidimensionali.
 Il 
\series bold
Deep Learning
\series default
, la cui traduzione letterale significa apprendimento profondo, è una sottocateg
oria del Machine Learning e indica quella branca dell’Intelligenza Artificiale
 che fa riferimento agli algoritmi ispirati alla struttura e alla funzione
 del cervello chiamate 
\series bold
reti neurali artificiali
\series default
.
 Le architetture di Deep Learning (reti neurali artificiali) sono per esempio
 state applicate nella computer vision, nel riconoscimento automatico della
 lingua parlata, nell’elaborazione del linguaggio naturale, nel riconoscimento
 audio e nella bioinformatica.
 Potremmo definire il Deep Learning come un sistema che sfrutta una classe
 di algoritmi di apprendimento automatico che: 
\end_layout

\begin_layout Itemize
usano vari livelli di unità non lineari a cascata per svolgere compiti di
 
\shape italic
estrazione di caratteristiche e di trasformazione
\shape default
.
 Ciascun livello successivo utilizza l’uscita del livello precedente come
 input.
 Gli algoritmi possono essere sia di tipo supervisionato sia non supervisionato
 e le applicazioni includono l’analisi di pattern (apprendimento non supervision
ato) e classificazione (apprendimento supervisionato);
\end_layout

\begin_layout Itemize
apprendono multipli livelli di rappresentazione che corrispondono a differenti
 livelli di astrazione; questi livelli formano una gerarchia di concetti.
\end_layout

\begin_layout Standard
La differenza principale consiste in come vengono estratte le caretteristiche
 utili alla risoluzione del problema.
 Tali caratteristiche vengono estratte manualmente nel machine learning
 , mentre vengono estratte autonomamente negli algoritmi di deep learning.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ml_vs_dl.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Differenze tra machine learning e deep learning.
 I riquadri evidenziati indicano le parti apprese dai dati
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Percettrone
\end_layout

\begin_layout Standard
Nell'apprendimento automatico, il 
\series bold
percettrone
\series default
 è un tipo di classificatore binario che mappa i suoi 
\series bold
ingressi
\series default
 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 (un vettore di tipo reale) in un valore di output 
\begin_inset Formula $\boldsymbol{f\left(x\right)}$
\end_inset

 (uno scalare di tipo reale) calcolato con 
\begin_inset Formula 
\begin{equation}
\boldsymbol{f(x)}=\chi(\left\langle \boldsymbol{w},\boldsymbol{x}\right\rangle +b)
\end{equation}

\end_inset

dove 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 è un vettore di 
\series bold
pesi
\series default
 con valori reali, l'operatore 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 è il prodotto scalare (che calcola una somma pesata degli input), 
\shape italic
b
\series bold
\shape default
 
\series default
è il 
\series bold
bias
\series default
, un termine costante che non dipende da alcun valore in input e 
\begin_inset Formula $\chi(y)$
\end_inset

 è la funzione di output.
 Le scelte più comuni per la funzione 
\begin_inset Formula $\chi(y)$
\end_inset

 sono:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\chi(y)=sign(y)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\chi(y)=y\Theta(y)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\chi(y)=y$
\end_inset


\end_layout

\begin_layout Standard
dove 
\begin_inset Formula $\Theta(y)$
\end_inset

 è la 
\series bold
funzione di Heaviside
\series default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename percettrone.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
percettrone
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h_{j}\left(\boldsymbol{x}|\boldsymbol{w},b\right)=h_{j}\left(\sum_{i=1}^{I}w_{i}\cdot x_{i}-b\right)=h_{j}\left(\sum_{i=0}^{I}w_{i}\cdot x_{i}\right)=h_{j}\left(\boldsymbol{w}^{T}\boldsymbol{x}\right)
\end{equation}

\end_inset

Non tutti i problemi di classificazione sono affrontabili con strumenti
 lineari come il percettrone.
 Sorgono spontanee alcune domande, come inizializziamo e modifichiamo il
 vettore di pesi 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 del percettrone? Quale funzione di attivazione scegliamo? 
\end_layout

\begin_layout Subsection
Apprendimento Hebbiano
\end_layout

\begin_layout Quotation
\begin_inset Quotes fld
\end_inset


\shape italic
The strength of a synapse increases according to the simultaneous activation
 of the relative input and the desired target
\shape default

\begin_inset Quotes frd
\end_inset

 (Donald Hebb, The Organization of Behavior, 1949).
 
\end_layout

\begin_layout Standard
La 
\series bold
regola di Hebb
\series default
 è la seguente: l'efficacia di una particolare sinapsi cambia se e solo
 se c'è un'intensa attività simultanea dei due neuroni, con un'alta trasmissione
 di input nella sinapsi in questione.
 L'
\series bold
apprendimento Hebbiano
\series default
 può essere riassunto come segue: 
\begin_inset Formula 
\begin{equation}
\begin{cases}
w_{i}^{k+1}=w_{i}^{k}+\Delta w_{i}^{k}\\
\Delta w_{i}^{k}=\eta\cdot x_{i}^{k}\cdot t^{k}
\end{cases}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
dove:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\eta$
\end_inset

: rateo di apprendimento;
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{i}^{k}$
\end_inset

: l'i-esimo input al tempo 
\begin_inset Formula $k$
\end_inset

;
\end_layout

\begin_layout Itemize
\begin_inset Formula $t^{k}$
\end_inset

: l'output desiderato al tempo 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Standard
L'inizializzazione dei pesi parte con dei valori casuali.
 La soluzione può non esistere e se esiste non essere unica, ma tutte ugualmente
 corrette.
 Questo algoritmo può non convergere alla soluzione per due motivi:
\end_layout

\begin_layout Enumerate
La soluzione non esiste;
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\eta$
\end_inset

 è troppo grande, continuiamo a modificare i pesi con passo elevato, viceversa
 un valore di 
\begin_inset Formula $\eta$
\end_inset

 troppo piccolo aumenta sensibilmente il tempo di convergenza.
\begin_inset Marginal
status open

\begin_layout Plain Layout
Cercare esempio
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Feed Forward Neural Networks
\end_layout

\begin_layout Standard
Una 
\series bold
rete neurale feed-forward
\series default
 ("rete neurale con flusso in avanti") o rete feed-forward è una rete neurale
 artificiale dove le connessioni tra le unità non formano cicli, differenziandos
i dalle reti neurali ricorrenti.
 Questo tipo di rete neurale fu la prima e più semplice tra quelle messe
 a punto.
 In questa rete neurale le informazioni si muovono solo in una direzione,
 avanti, rispetto a nodi d'ingresso, attraverso nodi nascosti (se esistenti)
 fino ai nodi d'uscita.
 Nella rete non ci sono cicli.
 Le reti feed-forward non hanno memoria di input avvenuti in tempi precedenti,
 per cui l'output è determinato solamente dall'attuale input.
 
\end_layout

\begin_layout Subsubsection
Percettrone a singolo strato
\end_layout

\begin_layout Standard
La più semplice rete feed-forward è il 
\series bold
percettrone a singolo strato
\series default
 (SLP dall'inglese 
\series bold
single layer perceptron
\series default
), utilizzato verso la fine degli anni '60.
 Un SLP è costituito da uno strato in ingresso, seguito direttamente dall'uscita.
 Ogni unità di ingresso è collegata ad ogni unità di uscita.
 In pratica questo tipo di rete neurale ha un solo strato che effettua l'elabora
zione dei dati, e non presenta nodi nascosti, da cui il nome.
 Gli SLP sono molto limitati a causa del piccolo numero di connessioni e
 dell'assenza di gerarchia nelle caratteristiche che la rete può estrarre
 dai dati (questo significa che è capace di combinare i dati in ingresso
 una sola volta).
 Famosa fu la dimostrazione, che un SLP non è in grado di rappresentare
 la funzione XOR.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename XOR_percettrone.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Problema XOR percettrone
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Percettrone multistrato
\end_layout

\begin_layout Standard
Il 
\series bold
Percettrone multistrato
\series default
 (in acronimo MLP dall'inglese Multilayer perceptron) è un modello di rete
 neurale artificiale che mappa insiemi di dati in ingresso in un insieme
 di dati in uscita appropriati.
 È fatta di strati multipli di nodi in un grafo diretto, con ogni strato
 completamente connesso al successivo.
 Eccetto che per i nodi in ingresso, ogni nodo è un neurone (elemento elaborante
) con una funzione di attivazione non lineare.
 Il Percettrone multistrato usa una tecnica di apprendimento supervisionato
 chiamata backpropagation per l'allenamento della rete.
 La MLP è una modifica del Percettrone lineare standard e può distinguere
 i dati che non sono separabili linearmente.
\end_layout

\begin_layout Standard
Prima di addentrasi in metodologie di progettazioni delle reti neurali è
 utile introdurre alcuni concetti.
\end_layout

\begin_layout Subsection
Funzioni di attivazione
\end_layout

\begin_layout Standard
Vediamo brevemente diversi tipi di funzioni di attivazione:
\end_layout

\begin_layout Standard

\series bold
ReLU.
 
\series default
The Rectified Linear Unit è una funzione di attivazione definita come la
 parte positiva del suo argomento:
\series bold
 
\begin_inset Formula 
\begin{equation}
f(x)=x^{+}=ReLU(x)=max(0,x)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Grafico_relu.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
grafico ReLU
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
dove 
\begin_inset Formula $x$
\end_inset

 è l'input a un neurone.

\series bold
 
\end_layout

\begin_layout Standard

\series bold
Sigmoid.
 
\series default
La funzione sigmoidea è una funzione matematica che produce una curva sigmoide;
 una curva avente un andamento ad "S".
 Spesso, la funzione sigmoide si riferisce ad uno speciale caso di funzione
 logistica mostrata a destra e definita dalla formula:
\begin_inset Formula 
\begin{equation}
f(x)=\frac{1}{1+e^{-x}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Generalmente, una funzione sigmoidea è una funzione continua e derivabile,
 che ha una derivata prima non negativa e dotata di un minimo locale ed
 un massimo locale.
 Le funzioni sigmoidee sono spesso usate nelle reti neurali per introdurre
 la non linearità nel modello e/o per assicurarsi che determinati segnali
 rimangano all'interno di specifici intervalli.
 Un motivo per la relativa popolarità nelle reti neurali è perché la funzione
 sigmoidea soddisfa questa proprietà:
\begin_inset Formula 
\begin{equation}
\frac{d}{dx}sig(x)=sig(x)(1-sig(x))
\end{equation}

\end_inset

Questa relazione polinomiale semplice fra la derivata e la funzione stessa
 è, dal punto di vista informatico, semplice da implementare.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename grafico_sig.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
grafico sigmoide
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Tangente iperbolica: 
\series default

\begin_inset Formula 
\begin{equation}
tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename tangente-iperbolica.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
grafico tanh
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Queste e altre funzioni verranno esaminate più attentamente dopo aver introdotto
 tecniche di ottimizzazione delle reti neurali.
 
\end_layout

\begin_layout Paragraph
Softmax 
\end_layout

\begin_layout Standard
In matematica, una funzione softmax, o funzione esponenziale normalizzata,
 è una generalizzazione di una 
\series bold
funzione logistica
\series default
 che comprime un vettore 
\begin_inset Formula $\boldsymbol{z}\in\mathbb{R}^{k}$
\end_inset

 di valori reali arbitrari in un vettore 
\begin_inset Formula ${\displaystyle \sigma(\mathbf{z})}$
\end_inset

 di valori reali compresi in un intervallo 
\begin_inset Formula ${\displaystyle (0,1)}$
\end_inset

 la cui somma è 
\begin_inset Formula ${\displaystyle 1}$
\end_inset

.
 La funzione è data da:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\displaystyle \sigma:\mathbb{R}^{K}\to\left\{ \boldsymbol{z}\in\mathbb{R}^{K}|z_{i}>0,\sum_{i=1}^{K}z_{i}=1\right\} }
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\displaystyle \sigma(\mathbf{z})_{j}={\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}}}
\]

\end_inset


\end_layout

\begin_layout Chapter
Reti Neurali
\end_layout

\begin_layout Standard
L'utilità dei modelli di rete neurale sta nel fatto che queste possono essere
 usate per comprendere una funzione utilizzando solo le osservazioni sui
 dati.
 Ciò è particolarmente utile nelle applicazioni in cui la complessità dei
 dati o la difficoltà di elaborazione rende la progettazione di una tale
 funzione impraticabile con i normali procedimenti di analisi manuale.
\end_layout

\begin_layout Standard
I compiti in cui le reti neurali sono applicate possono essere classificate
 nelle seguenti categorie:
\end_layout

\begin_layout Itemize
funzioni di approssimazione, o di 
\series bold
regressione
\series default
, tra cui la previsione di serie temporali e la modellazione;
\end_layout

\begin_layout Itemize

\series bold
classificazione
\series default
, compresa la struttura e la sequenza di generici riconoscimenti, l'individuazio
ne delle novità ed il processo decisionale;
\end_layout

\begin_layout Itemize
l'elaborazione dei dati, compreso il "
\series bold
filtraggio
\series default
" (eliminazione del rumore), il clustering, separazione di segnali e compression
e.
\end_layout

\begin_layout Standard
Le aree di applicazione includono i sistemi di controllo (controllo di veicoli,
 controllo di processi), simulatori di giochi e processi decisionali (backgammon
, scacchi), riconoscimento di pattern (sistemi radar, identificazione di
 volti, riconoscimento di oggetti, ecc), riconoscimenti di sequenze (riconoscime
nto di gesti, riconoscimento vocale, OCR), diagnosi medica, applicazioni
 finanziarie, data mining, filtri spam per e-mail.
\end_layout

\begin_layout Section*
Pregi
\end_layout

\begin_layout Standard
Le reti neurali per come sono costruite lavorano in parallelo e sono quindi
 in grado di trattare molti dati.
 Si tratta in sostanza di un sofisticato sistema di tipo statistico dotato
 di una buona immunità al rumore; se alcune unità del sistema dovessero
 funzionare male, la rete nel suo complesso avrebbe delle riduzioni di prestazio
ni ma difficilmente andrebbe incontro ad un blocco del sistema.
 I software di ultima generazione dedicati alle reti neurali richiedono
 comunque buone conoscenze statistiche; il grado di apparente utilizzabilità
 immediata non deve trarre in inganno, pur permettendo all'utente di effettuare
 subito previsioni o classificazioni, seppure con i limiti del caso.
 Da un punto di vista industriale, risultano efficaci quando si dispone
 di dati storici che possono essere trattati con gli algoritmi neurali.
 Ciò è di interesse per la produzione perché permette di estrarre dati e
 modelli senza effettuare ulteriori prove e sperimentazioni.
\end_layout

\begin_layout Section*
Difetti
\end_layout

\begin_layout Standard
I modelli prodotti dalle reti neurali, anche se molto efficienti, non sono
 spiegabili in linguaggio simbolico umano: i risultati vanno accettati "così
 come sono", da cui anche la definizione inglese delle reti neurali come
 "black box": in altre parole, a differenza di un sistema algoritmico, dove
 si può esaminare passo-passo il percorso che dall'input genera l'output,
 una rete neurale è in grado di generare un risultato valido, o comunque
 con una alta probabilità di essere accettabile, ma non è possibile spiegare
 come e perché tale risultato sia stato generato.
 Come per qualsiasi algoritmo di modellazione, anche le reti neurali sono
 efficienti solo se le variabili predittive sono scelte con cura.
\end_layout

\begin_layout Standard
Non sono in grado di trattare in modo efficiente variabili di tipo categorico
 (per esempio, il nome della città) con molti valori diversi.
 Necessitano di una fase di addestramento del sistema che fissi i pesi dei
 singoli neuroni e questa fase può richiedere molto tempo, se il numero
 dei record e delle variabili analizzate è molto grande.
 Non esistono teoremi o modelli che permettano di definire la rete ottima,
 quindi la riuscita di una rete dipende molto dall'esperienza del creatore.
\end_layout

\begin_layout Section
Teorema di approssimazione universale
\end_layout

\begin_layout Standard
Nella teoria matematica delle reti neurali artificiali, il 
\series bold
teorema di approssimazione universale
\series default
 afferma che una feed-forward network con un singolo strato nascosto e contenent
e un numero finito di neuroni può approssimare una qualsiasi funzione misurabile
 secondo Lebesgue, con qualsiasi grado di accuratezza, su un sotto insieme
 compatto di 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 sotto deboli ipotesi sulla funzione di attivazione dei neuroni.
 Il teorema non dice nulla su algoritmi di apprendimento da utilizzare.
 Sebbene una rete feed-forward con un singolo strato nascosto sia un approssimat
ore universale, l'ampiezza di queste reti deve essere esponenzialmente grande.
 Nel 2017 Lu et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"
literal "false"

\end_inset

 dimostrarono una variante del teorema per reti feed-forward con ampiezza
 limitata.
 In particolare provarono che una rete di ampiezza 
\begin_inset Formula $n+4$
\end_inset

 con funzione di attivazione ReLU può approssimare una qualsiasi funzione
 integrabile secondo Lebesgue definita su uno spazio 
\begin_inset Formula $n-dimensionale$
\end_inset

 rispetto alla norma 
\begin_inset Formula $L_{1}$
\end_inset

 se è permesso alla rete di crescere in profondità (quindi non più a singolo
 strato nascosto).
 Provarono anche la limitata potenza espressiva se l'ampiezza della rete
 è minore o uguale a 
\begin_inset Formula $n$
\end_inset

.
 Nessuna funzione integrabile secondo Lebesgue ad eccezione di quelle definite
 su insiemi a misura nulla può essere approssimata da una rete con ampiezza
 
\begin_inset Formula $n$
\end_inset

 e funzione di attivazione ReLU.
 La formulazione originale del teorema non fa assunzioni che la funzione
 di attivazione sia ReLU ma solo che sia continua, limitata e non costante.
 I due teoremi sono formalmente enunciati nel modo seguente:
\end_layout

\begin_layout Paragraph*

\series bold
Ampiezza illimitata:
\end_layout

\begin_layout Standard
Sia 
\begin_inset Formula $\varphi:\mathbb{R}\rightarrow\mathbb{R}$
\end_inset

 una funzione continua, limitata e non costante (chiamata 
\shape italic
funzione di attivazione
\shape default
).
 Sia 
\begin_inset Formula $I_{m}$
\end_inset

 l'ipercubo unitario 
\begin_inset Formula $[0,1]^{m}$
\end_inset

.
 Sia 
\begin_inset Formula $C(I_{m})$
\end_inset

 lo spazio delle funzioni a valori reali definite su 
\begin_inset Formula $I_{m}$
\end_inset

.
 Dato un 
\begin_inset Formula $\varepsilon>0$
\end_inset

 e una qualsiasi funzione 
\begin_inset Formula $f\in C(I_{m})$
\end_inset

, esiste allora un intero 
\begin_inset Formula $N\in\mathbb{N}$
\end_inset

, costanti reali 
\begin_inset Formula $v_{i},b_{i}\in\mathbb{R}$
\end_inset

 e vettori reali 
\begin_inset Formula $\boldsymbol{w}_{i}\in\mathbb{R}^{m}$
\end_inset

 per 
\begin_inset Formula $i=1,...,N$
\end_inset

 tale che possiamo definire:
\begin_inset Formula 
\begin{equation}
F(\boldsymbol{x})=\sum_{i=1}^{N}v_{i}\varphi(\boldsymbol{w}_{i}^{T}\boldsymbol{x}+b_{i})
\end{equation}

\end_inset

come realizzazione approssimativa della funzione 
\begin_inset Formula $f$
\end_inset

 per cui vale:
\begin_inset Formula 
\begin{equation}
|F(\boldsymbol{x})-f(\boldsymbol{x})|<\varepsilon
\end{equation}

\end_inset

per ogni 
\begin_inset Formula $x\in\mathbb{R}^{m}.$
\end_inset

 In altre parole, funzioni della forma 
\begin_inset Formula $F(\boldsymbol{x})$
\end_inset

 sono dense in 
\begin_inset Formula $C(I_{m})$
\end_inset

.
 Questo vale ancora se si sostituisce a 
\begin_inset Formula $I_{m}$
\end_inset

 un qualsiasi sotto insieme compatto di 
\begin_inset Formula $\mathbb{R}^{m}.$
\end_inset


\end_layout

\begin_layout Paragraph
Ampiezza limitata: 
\end_layout

\begin_layout Standard
Per ogni funzione integrabile secondo Lebesgue 
\begin_inset Formula $f:\mathbb{\mathbb{R}}^{n}\rightarrow\mathbb{R}$
\end_inset

 e ogni 
\begin_inset Formula $\varepsilon>0$
\end_inset

, esiste una rete fully-connected ReLU 
\begin_inset Formula $A$
\end_inset

 con ampiezza 
\begin_inset Formula $d_{m}\leq n+4$
\end_inset

 tale che la funzione 
\begin_inset Formula $F_{A}$
\end_inset

 rappresentata da tale rete soddisfa:
\begin_inset Formula 
\begin{equation}
\int_{\mathbb{R}^{n}}|F_{A}(\boldsymbol{x})-f(\boldsymbol{x})|d\boldsymbol{x}<\varepsilon.
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Regressione
\end_layout

\begin_layout Standard
L'analisi della 
\series bold
regressione
\series default
 è una tecnica usata per analizzare una serie di dati che consistono in
 una variabile dipendente e una o più variabili indipendenti.
 Lo scopo è stimare un'eventuale relazione funzionale esistente tra la variabile
 dipendente e le variabili indipendenti.
 La variabile dipendente nell'equazione di regressione è una funzione delle
 variabili indipendenti più un termine d'errore.
 Quest'ultimo è una variabile casuale e rappresenta una variazione non controlla
bile e imprevedibile nella variabile dipendente.
 I parametri sono stimati in modo da descrivere al meglio i dati.
 Il metodo più comunemente utilizzato per ottenere le migliori stime è il
 metodo dei "
\series bold
minimi quadrati
\series default
" (OLS), ma sono utilizzati anche altri metodi.
\end_layout

\begin_layout Subsection
Variabili casuali
\end_layout

\begin_layout Standard
In matematica, e in particolare nella teoria della probabilità, una 
\series bold
variabile casuale
\series default
 (detta anche 
\series bold
variabile aleatoria
\series default
 o 
\series bold
variabile stocastica
\series default
) è una variabile che può assumere valori diversi in dipendenza da qualche
 fenomeno aleatorio.
 Le variabili casuali sono per noi molto importanti perchè l'obiettivo di
 una rete neurale per regressione è quello di approssimare una funzione
 target 
\begin_inset Formula $t$
\end_inset

 incognita avendo a disposizioni 
\begin_inset Formula $N$
\end_inset

 osservazioni (coppie input-output della funzione 
\begin_inset Formula $t$
\end_inset

).
 Anche per la classificazioni di immagini è possibile ricondursi a variabili
 aleatorie.
\end_layout

\begin_layout Section
Classificazione
\end_layout

\begin_layout Standard
La 
\series bold
classificazione statistica
\series default
 è quell'attività che si serve di un algoritmo statistico al fine di individuare
 una rappresentazione di alcune caratteristiche di un'entità da classificare
 (oggetto o nozione), associandole una etichetta classificatoria.
 La classificazione si divide in due tipologie:
\end_layout

\begin_layout Enumerate

\series bold
Classificazione in due classi
\series default
 
\begin_inset Formula $\{\Omega_{0},\varOmega_{1}\}$
\end_inset

 in cui generalmente si utilizza come funzione di attivazione nel layer
 di output (singolo neurone):
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\tanh$
\end_inset

 codificando le classi nel modo seguente 
\begin_inset Formula $\{\Omega_{0}=-1,\varOmega_{1}=1\}$
\end_inset

.
 Ovviamente il codomino della funzione tanh varia con continuità nell'intervallo
 (-1,1), quindi si prende come valore quello meno distante.
\end_layout

\begin_layout Enumerate
Sigmoid codificando le classi nel modo seguente 
\begin_inset Formula $\{\Omega_{0}=0,\varOmega_{1}=1\}$
\end_inset

.
 Analoga considerazione a riguardo del codominio fatta per tanh.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Classificazione in più di due classi
\series default
 
\begin_inset Formula $\{\Omega_{0},\varOmega_{1},...,\varOmega_{n}\}$
\end_inset

, il layer di output ha tanti neuroni quante sono le classi.
 Un possibile approcio è codificare le classi usando la codifica onehot
 dove la classe i-esima viene rappresentata da un vettore di dimensione
 
\begin_inset Formula $n$
\end_inset

 in cui ogni componente è 0 ad esclusione dell'i-esimo elemento che vale
 1.
 I neuroni dello strato di output useranno softmax come funzione di attivazione.
\end_layout

\begin_layout Section
Maximum Likelihood Estimation (stimatore di massima verosimiglianza)
\end_layout

\begin_layout Standard
Supponiamo di avere 
\begin_inset Formula $N$
\end_inset

 campioni di una variabile aleatoria di cui conosciamo la distribuzione
 di probabilità ma non conosciamo tutti i parametri di tale distribuzione,
 
\series bold
MLE
\series default
 si pone l'obiettivo di trovare i parametri tali per cui è massima la probabilit
à che gli 
\begin_inset Formula $N$
\end_inset

 campioni appartengano alla distribuzione di probabilità con i parametri
 così trovati.
 Dato 
\begin_inset Formula $\boldsymbol{\theta}=(\theta_{1},\theta_{2},...,\theta_{p})^{T}$
\end_inset

 un vettore di parametri, cerchiamo 
\begin_inset Formula $\boldsymbol{\theta}_{MLE}:$
\end_inset


\end_layout

\begin_layout Itemize
Scriviamo la probabilità condizionata 
\begin_inset Formula $L=P(\boldsymbol{x}|\boldsymbol{\theta})$
\end_inset

 con 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 vettore di input;
\end_layout

\begin_layout Itemize
opzionalmente, se agevola i calcoli, calcoliamo 
\begin_inset Formula $l=\log(P(\boldsymbol{x}|\boldsymbol{\theta}))$
\end_inset

;
\end_layout

\begin_layout Itemize
cerchiamo il massimo rispetto a 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 con gli strumenti dell'analisi matematica:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\nabla_{\boldsymbol{\theta}}(L)$
\end_inset

 o 
\begin_inset Formula $\nabla_{\boldsymbol{\theta}}(l)=0$
\end_inset


\end_layout

\begin_layout Itemize
Controlliamo che il valore di 
\begin_inset Formula $\boldsymbol{\theta}_{MLE}$
\end_inset

 sia un massimo (tramite hessiana o altro)
\end_layout

\end_deeper
\begin_layout Standard
Questa è la risoluzione analitica, non sempre possibile o conveniente.
 In ogni caso cerchiamo quel valore di 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 che massimizza la probabilità, quindi in generale si può affrontare il
 problema con altri metodi quali:
\end_layout

\begin_layout Itemize
tecniche di ottimizzazione: per esempio moltiplicatori di Lagrange;
\end_layout

\begin_layout Itemize
tecniche numeriche: per esempio la discesa del gradiente, approfondita largament
e in seguito;
\end_layout

\begin_layout Standard
Vediamo un esempio classico della probabilità:
\end_layout

\begin_layout Standard
supponiamo di avere 
\begin_inset Formula $N$
\end_inset

 indipendenti e identicamente distribuiti (i.i.d.) campioni di numeri reali
 provenienti da una distribuzione gaussiana di varianza 
\begin_inset Formula $\sigma^{2}$
\end_inset

 nota e media 
\begin_inset Formula $\mu$
\end_inset

 incognita:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{x}=x_{1},x_{2},...,x_{n}\sim N(\mu,\sigma^{2})
\end{equation}

\end_inset

 con 
\begin_inset Formula 
\begin{equation}
N(\mu,\sigma^{2})=p(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
\end{equation}

\end_inset

calcoliamo la likelihood 
\begin_inset Formula $L=P(\boldsymbol{x}|\boldsymbol{\theta})$
\end_inset

: 
\begin_inset Formula 
\begin{equation}
L=p(x_{1},x_{2},...,x_{n}|\mu,\sigma^{2})=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}}
\end{equation}

\end_inset

passiamo al logaritmo:
\begin_inset Formula 
\begin{equation}
l=\log(\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}})=\sum_{i=1}^{N}\log(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=N\cdot\log(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)^{2}
\end{equation}

\end_inset

deriviamo rispetto a 
\begin_inset Formula $\mu$
\end_inset

 che è il parametro che vogliamo stimare:
\begin_inset Formula 
\begin{equation}
\frac{\partial l(\boldsymbol{x}|\mu)}{\partial\mu}=\frac{\partial}{\partial\mu}(N\cdot\log(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)^{2})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\frac{1}{\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)
\end{equation}

\end_inset

ora uguagliamo a zero la derivata parziale:
\begin_inset Formula 
\begin{equation}
\frac{1}{\sigma^{2}}\sum_{i=1}^{N}(x_{i}-\mu)=0
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{N}(x_{i}-\mu)=0
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{N}x_{i}=\sum_{i=1}^{N}\mu
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\mu_{MLE}=\frac{1}{N}\sum_{i=1}^{N}x_{i}.
\end{equation}

\end_inset

Vediamo un esempio pratico di come applicare lo stimatore di massima verosomigli
anza con una generica rete neurale per la regressione.
\end_layout

\begin_layout Subsection
MLE per regressione
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename neural_network_for_regression.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Rete neurale per regressione
\end_layout

\end_inset


\end_layout

\end_inset

Il problema della regressione può essere esposto in maniera semplice e intuitiva
, sacrificando la formalità, come dati una serie di punti (coppie input-output
 della funione incognita) campionati sperimentalmente cerchiamo una qualche
 funzione che a parità di input fornisca l'output più 
\begin_inset Quotes qld
\end_inset

vicino
\begin_inset Quotes qrd
\end_inset

 possibile.
 Osserviamo che non cerchiamo una funzione che passi esattamente dai punti
 campionati (quello è il compito dell'interpolazione) ma che minimizzi la
 differenza con i tutti i campioni.
 Nell'esempio in fig.
 2.1 abbiamo una funzione incognita 
\begin_inset Formula $t_{n}$
\end_inset

 che vogliamo stimare tramite la funzione calcolata dalla rete neurale 
\begin_inset Formula $g(x_{n}|w).$
\end_inset

 Poichè dobbiamo tenere conto che la nostra approssimazione presenterà degli
 errori e non avendo alcuna informazione aggiuntiva sulla funzione 
\begin_inset Formula $t_{n}$
\end_inset

 modelliziamo questo errore come un rumore aggiunto alla funzione 
\begin_inset Formula $g(x_{n}|w).$
\end_inset

 Per semplicità dei calcoli ipotiziamo che il rumore abbia varianza nota.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
t_{n}=g(x_{n}|w)+\epsilon_{n}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\epsilon_{n}\sim N(0,\sigma^{2})
\end{equation}

\end_inset

è allora intuitivo osservare che in ogni punto 
\begin_inset Formula $t_{n}$
\end_inset

 ha distribuzione di probabilità gaussiana di media 
\begin_inset Formula $g(x_{n}|w)$
\end_inset

 e varianza 
\begin_inset Formula $\sigma^{2}$
\end_inset


\begin_inset Formula 
\begin{equation}
\Rightarrow t_{n}\sim N(g(x_{n}|w),\sigma^{2}).
\end{equation}

\end_inset

Come nell'esempio teorico dobbiamo stimare la media di una distribuzione
 normale.
\begin_inset Formula 
\begin{equation}
p(t|g(x|w),\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t-g(x|w))^{2}}{2\sigma^{2}}}
\end{equation}

\end_inset

scriviamo la likelihood per l'insieme di osservazioni 
\begin_inset Formula $L(w)=p(\boldsymbol{t}|g(\boldsymbol{x}|w),\sigma^{2})$
\end_inset


\begin_inset Formula 
\begin{equation}
L(w)=p(t_{1},t_{2},...,t_{N}|g(x|w),\sigma^{2})=\prod_{i=1}^{N}p(t_{i}|g(x_{i}|w),\sigma^{2})
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}}
\end{equation}

\end_inset

cerchiamo i pesi 
\begin_inset Formula $w$
\end_inset

 che massimizzano la likelihood 
\begin_inset Formula $L(w)$
\end_inset


\begin_inset Formula 
\begin{equation}
argmax_{w}L(w)=argmax_{w}(\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}})
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
=argmax_{w}(\sum_{i=1}^{N}\log(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}})
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
=argmax_{w}(N\cdot\log(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2})
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
=argmin_{w}(\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2}).
\end{equation}

\end_inset

Siamo giunti quindi a trovare tramite lo stimatore di massima verosomiglianza
 la 
\series bold
funzione di errore
\series default
 da minimizzare che useremo per allenare la nostra rete e quindi trovare
 i pesi 
\begin_inset Formula $w$
\end_inset

 ottimali per la regressione.
 Infatti 
\begin_inset Formula $\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2}$
\end_inset

 è definita SSE (
\shape italic
Sum of Squared Errors
\shape default
) ed è alla base della tecnica di ottimizzazione e regressione nota come
 
\series bold
metodo dei minimi quadrati.
 
\series default
In maniera analoga è possibile procedere anche nel caso la varianza sia
 incognita.
\series bold

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename sse.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
\begin_inset Caption Standard

\begin_layout Plain Layout
SSE
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
MLE classificazione, panoramica altre loss function
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
MLE per classificazione binaria
\end_layout

\begin_layout Standard
Usiamo MLE per determinare la 
\series bold
loss function
\series default
 adatta al problema di classificazione binaria, la rete mostrata in figura
 utilizza la funzione sigmoide nell'ultimo neurone di conseguenza l'output
 varierà con continuità tra 0 e 1.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename NN_classification.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Rete neurale per classificazione binaria
\end_layout

\end_inset


\end_layout

\end_inset

Adottiamo ancora un approcio probabilistico.
 Idealmente, in un problema di classificazione binaria, noi vorremmo che
 ad ogni input 
\begin_inset Formula $x_{n}$
\end_inset

 corrisponda un solo numero 
\begin_inset Formula $t_{n}\in\{0,1\}$
\end_inset

.
 Una distribuzione di probabilità su due soli valori 0 e 1 è la distribuzione
 di Bernoulli.
 
\end_layout

\begin_layout Standard
Una variabile aleatoria discreta 
\begin_inset Formula $X$
\end_inset

 ha distribuzione di Bernoulli 
\begin_inset Formula ${\displaystyle \mathcal{B}(p)}$
\end_inset

 di parametro 
\begin_inset Formula ${\displaystyle p\in[0,1]}$
\end_inset

 se e solo se:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(X=1)=p
\]

\end_inset


\begin_inset Formula 
\[
P(X=0)=1-p
\]

\end_inset

La conoscenza del parametro 
\begin_inset Formula $p$
\end_inset

 deriva da assunzioni sul fenomeno aleatorio in esame.
 Ad esempio nel lancio di una moneta non truccata sappiamo che 
\begin_inset Formula $p$
\end_inset

 è uguale a 0,5 dato che assumiamo che le leggi fisiche che governano il
 moto di una moneta non hanno preferenze tra testa o croce.
 Nel nostro caso quello che cerchiamo è una probabilità a posteriori del
 valore assunto da 
\begin_inset Formula $t_{n}$
\end_inset

 sapendo l'input 
\begin_inset Formula $x_{n}.$
\end_inset

 L'idea è utilizzare come parametro della distribuzione la conoscenza acquisita
 dalla rete analizzando l'input.
 
\begin_inset Formula 
\[
t_{n}\sim B(g(x_{n}|w))
\]

\end_inset

Quello che vogliamo fare è massimizzare il numero di classificazioni corrette
 su tutto l'input.
 I nostri campioni osservati durante il training sono indipendenti ed identicame
nte distribuiti:
\begin_inset Formula 
\[
p(t_{n}|g(x_{n}|w))=g(x_{n}|w)^{t_{n}}(1-g(x_{n}|w))^{1-t_{n}}
\]

\end_inset

Scriviamo la likelihood congiunta:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(w)=p(t_{1},t_{2},...,t_{n}|g(x|w))=\prod_{n=1}^{N}p(t_{n}|g(x_{n}|w))=
\]

\end_inset


\begin_inset Formula 
\[
\prod_{n=1}^{N}g(x_{n}|w)^{t_{n}}(1-g(x_{n}|w))^{1-t_{n}}
\]

\end_inset

Massimizzando rispetto a w:
\begin_inset Formula 
\[
argmax_{w}(\prod_{n=1}^{N}g(x_{n}|w)^{t_{n}}(1-g(x_{n}|w))^{1-t_{n}})
\]

\end_inset

Passiamo al logaritmo
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
argmax_{w}(\sum_{n=1}^{N}t_{n}log(g(x_{n}|w))+(i-t_{n})log(1-g(x_{n}|w)))
\]

\end_inset

Massimizzare quest'ultima formula è equivalente a minimizzare:
\begin_inset Formula 
\[
argmin_{w}(-\sum_{n=1}^{N}t_{n}log(g(x_{n}|w))+(i-t_{n})log(1-g(x_{n}|w)))
\]

\end_inset

In questo modo siamo giunti ad avere una funzione di errore da minimizzare
 per la classificazione binaria.
 Tale funzione di errore prende il nome di Cross-entropy.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(w)=\sum_{n=1}^{N}t_{n}log(g(x_{n}|w))+(i-t_{n})log(1-g(x_{n}|w))
\]

\end_inset


\end_layout

\begin_layout Section
Ottimizzazione
\end_layout

\begin_layout Standard
Fino ad ora abbiamo solo accennato a come impostare i pesi di una rete neurale
 (
\shape italic
hebbian learning
\shape default
) ma non siamo mai entrati nello specifico.
 Ricordiamo brevemente come è composta una rete neurale e come valutiamo
 la sua bontà.
\end_layout

\begin_layout Itemize
Una rete è formata da uno o più percettroni, disposti in vari strati di
 ampiezza variabile.
 Per il momento ci limitiamo alle reti Fully Connected (FC) in cui ogni
 neurone è collegato a tutti i neuroni dello strato successivo, ad eccezione
 ovviamente dello strato di output.
 Ogni neurone esegue la somma pesata del proprio vettore di input e propaga
 in uscita il valore della funzione di attivazione (ReLU, sigmoide etc...)
 applicata alla somma di input.
\end_layout

\begin_layout Itemize
Ogni rete deve essere validata tramite una error function (o loss function)
 che permetta di quantificare la bontà della rete e quindi avere un responso
 sulla validità dei pesi impostati.
\end_layout

\begin_layout Standard
Tale 
\shape italic
error function
\shape default
 che ricordiamo essere funzione della rete e quindi dei suoi pesi 
\begin_inset Formula $w$
\end_inset

, deve essere differenziabile (o quasi) rispetto a 
\begin_inset Formula $w.$
\end_inset

 L'idea generale è quella di sfruttare gli strumenti del calcolo infinitesimale
 per trovare il minimo della funzione di errore.
 Nel caso la funzione di errore fosse differenziabile e convessa sappiamo
 dalla teoria del calcolo che certamente esiste un minimo assoluto e sappiamo
 ricavarlo analiticamente.
 Sfortunatamente nel caso generale possiamo richiedere alla funzione di
 errore solo la continuità e la quasi differenziabilità, inoltre già una
 rete neurale di medio bassa complessità presenta al suo interno migliaia
 o milioni di pesi che rendono praticamente impossibile trovare una soluzione
 analitica.
 Quello che si riesce agilmente a fare è calcolare numericamente il 
\series bold
gradiente
\series default
 (con un bassissimo tasso di errore) che ricordiamo fornisce la direzione
 di massima crescità della funzione.
 
\end_layout

\begin_layout Subsection
Discesa del gradiente
\end_layout

\begin_layout Paragraph
Gradiente
\end_layout

\begin_layout Standard
Nel calcolo differenziale vettoriale, il gradiente di una funzione a valori
 reali (ovvero di un campo scalare) è una funzione vettoriale.
 Il gradiente di una funzione è spesso definito come il vettore che ha come
 componenti le derivate parziali della funzione, anche se questo vale solo
 se si utilizzano coordinate cartesiane ortonormali.
 In generale, il gradiente di una funzione 
\begin_inset Formula ${\displaystyle f}$
\end_inset

, denotato con 
\begin_inset Formula ${\displaystyle \nabla f},$
\end_inset

 (il simbolo 
\begin_inset Formula ${\displaystyle \nabla}$
\end_inset

 si legge nabla), è definito in ciascun punto dalla seguente relazione:
 per un qualunque vettore 
\begin_inset Formula $\boldsymbol{v}$
\end_inset

, il prodotto scalare 
\begin_inset Formula $\nabla f\cdot\boldsymbol{v}$
\end_inset

 dà il valore della derivata direzionale di 
\begin_inset Formula $f$
\end_inset

 rispetto a 
\begin_inset Formula $\boldsymbol{v}$
\end_inset

 (sse 
\begin_inset Formula $f$
\end_inset

 è differenziabile).
 Nel caso unidimensionale il gradiente corrisponde alla derivata della funzione
 e indica la pendenza, quindi il tasso di variazione della funzione, e il
 verso in cui la funzione cresce (nel caso unidimensionale la direzione
 del vettore è determinata e unica, il segno invece determina il verso,
 positiva la funzione cresce a destra, negativa la funzione cresce a sinistra).
 La derivata è definita formalmente come:
\begin_inset Formula 
\begin{equation}
\frac{df(x)}{dx}=\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}
\end{equation}

\end_inset

mentre chiamiamo derivata parziale rispetto a 
\begin_inset Formula $x_{i}$
\end_inset

 di una generica funzione reale 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 derivabile:
\begin_inset Formula 
\begin{equation}
\frac{\partial f(\boldsymbol{x})}{x_{i}}=\lim_{h\rightarrow0}\frac{f(x_{1},...,x_{i}+h,...,x_{n})}{h}
\end{equation}

\end_inset

con 
\begin_inset Formula $i\in\{1,...,n\}$
\end_inset

.
 Il gradiente è quindi un vettore le cui componenti sono tutte le derivate
 parziali rispetto agli assi di una funzione:
\begin_inset Formula 
\begin{equation}
\nabla f(\boldsymbol{x})=(\frac{\partial f(\boldsymbol{x})}{x_{1}},\frac{\partial f(\boldsymbol{x})}{x_{2}},...,\frac{\partial f(\boldsymbol{x})}{x_{n}})
\end{equation}

\end_inset

ogni componente indica quanto e in che verso la funzione cresce, né consegue
 che un vettore così definito individui la direzione e verso in cui la funzione
 ha crescita massima, inoltre il modulo indica di quanto cresce la funzione
 in questa direzione.
 
\end_layout

\begin_layout Standard

\series bold
Considerazioni pratiche.
 
\series default
Notiamo che la formulazione matematica del gradiente è definito come il
 limite del rapporto incrementale con l'incremento 
\begin_inset Formula $h$
\end_inset

 che tende a zero, ovviamente tale operazione non può essere svolta direttamente
 da un calcolatore ma viene svolta tramite le tecniche del calcolo numerico,
 in prima approssimazione è sufficiente calcolare il rapporto con un incremento
 molto piccolo come ad esempio 
\begin_inset Formula $1e-5$
\end_inset

 inoltre spesso funziona meglio (soprattutto in prossimità di punti angolosi,
 in cui formalmente la derivata non è definita) la 
\shape italic
derivata numerica simmetrica:
\begin_inset Formula 
\begin{equation}
\frac{f(x+h)-f(x-h)}{2h}
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Idea generale
\end_layout

\begin_layout Standard
In ottimizzazione e analisi numerica il metodo di discesa del gradiente
 (detto anche metodo del gradiente, metodo steepest descent o metodo di
 discesa più ripida) è una tecnica che consente di determinare i punti di
 massimo e minimo di una funzione di più variabili.
 Si voglia risolvere il seguente problema di ottimizzazione non vincolata
 nello spazio 
\begin_inset Formula $n-$
\end_inset

dimensionale 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset


\begin_inset Formula 
\begin{equation}
minimizzare\,f(\boldsymbol{x}),\qquad x\in\mathbb{R}^{n}.
\end{equation}

\end_inset

La tecnica di 
\series bold
discesa del gradiente
\series default
 si basa sul fatto che, per una data funzione 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $f(\boldsymbol{x})$
\end_inset

, la direzione di massima discesa in un assegnato punto 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 corrisponde a quella determinata dall'opposto del suo gradiente in quel
 punto 
\begin_inset Formula $\boldsymbol{p}_{k}=-\nabla f(\boldsymbol{x}).$
\end_inset

 Questa scelta per la direzione del gradiente garantisce che la soluzione
 tenda ad un punto di minimo di 
\begin_inset Formula $f$
\end_inset

.
 Il metodo del gradiente prevede dunque di partire da una soluzione iniziale
 
\begin_inset Formula ${\displaystyle \mathbf{x}_{0}}$
\end_inset

 scelta arbitrariamente e di procedere iterativamente aggiornandola come
\begin_inset Formula 
\begin{equation}
\boldsymbol{x}_{k+1}=\boldsymbol{x}_{k}+\eta_{k}\boldsymbol{p}_{k}
\end{equation}

\end_inset

dove 
\begin_inset Formula $\eta_{k}\in\mathbb{R}^{+}$
\end_inset

 corrisponde alla lunghezza del passo di discesa, la cui scelta diventa
 cruciale nel determinare la velocità con cui l'algoritmo convergerà alla
 soluzione richiesta.
 Si parla di metodo stazionario nel caso in cui si scelga un passo 
\begin_inset Formula ${\displaystyle \eta_{k}={\bar{\eta}}}$
\end_inset

 costante per ogni 
\begin_inset Formula ${\displaystyle k}$
\end_inset

, viceversa il metodo si definisce dinamico.
 In quest'ultimo caso una scelta conveniente, ma computazionalmente più
 onerosa rispetto a un metodo stazionario, consiste nell'ottimizzare, una
 volta determinata la direzione di discesa 
\begin_inset Formula ${\displaystyle \mathbf{p}_{k}}$
\end_inset

, la funzione di una variabile 
\begin_inset Formula ${\displaystyle {f}_{k}(\eta_{k}):=f(\mathbf{x}_{k}+\eta_{k}\mathbf{p}_{k})}$
\end_inset

 in maniera analitica o in maniera approssimata.
 Si noti che, a seconda della scelta del passo di discesa, l'algoritmo potrà
 convergere a uno qualsiasi dei minimi della funzione 
\begin_inset Formula ${\displaystyle f}$
\end_inset

, sia esso locale o globale.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename discesa_unidimesionale.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Discesa gradiente 1-D
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 350px-Gradient_descent.svg.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Discesa gradiente 2-D
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Algoritmo generale
\end_layout

\begin_layout Standard
Lo schema generale per l'ottimizzazione di una funzione 
\begin_inset Formula $f(\boldsymbol{x})$
\end_inset

 mediante metodo del gradiente è il seguente:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Formula $k=0$
\end_inset


\end_layout

\begin_layout Plain Layout
while 
\begin_inset Formula $\nabla f(\boldsymbol{x})\neq0$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space \hspace*{}
\length 1cm
\end_inset

calcolare la direzione di discesa 
\begin_inset Formula $\boldsymbol{p}_{k}=-\nabla f(\boldsymbol{x})$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space \hspace*{}
\length 1cm
\end_inset

calcolare il passo di discesa 
\begin_inset Formula $\eta_{k}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space \hspace*{}
\length 1cm
\end_inset


\begin_inset Formula $\boldsymbol{x}_{k+1}=\boldsymbol{x}_{k}+\eta_{k}\boldsymbol{p}_{k}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space \hspace*{}
\length 1cm
\end_inset


\begin_inset Formula $k=k+1$
\end_inset


\end_layout

\begin_layout Plain Layout
end.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Discesa del gradiente
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Algoritmi di ottimizzazioni della discesa del gradiente
\end_layout

\begin_layout Standard
Nelle librerie di apprendimento (es: keras) esistono varie implementazioni
 di algoritmi per ottimizzare la discesa del gradiente.
 Questi algoritmi sono generalmente usati come black-box, in questa sezione
 forniremo una panoramica e le intuizioni dietro ad essi.
 Una prima differenziazione della discesa del gradiente è sulla quantità
 di dati usati per i calcoli.
 In questa sezione ci riferiamo alla funzione di costo (loss function, error
 function etc) da minimizzare come 
\begin_inset Formula $J(\boldsymbol{x}|\boldsymbol{\theta}):\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
\end_inset

 funzione di 
\begin_inset Formula $\boldsymbol{x}\in\mathbb{R}^{n}$
\end_inset

 parametrizzata da 
\begin_inset Formula $\boldsymbol{\theta}\in\mathbb{R}^{d}.$
\end_inset

 Per brevità, visto che minimiziamo rispetto a 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 indichiamo la funzione di costo semplicemente come 
\begin_inset Formula $J(\boldsymbol{\theta})$
\end_inset

.
\end_layout

\begin_layout Paragraph
Batch gradient descent
\end_layout

\begin_layout Standard
Batch gradient descent calcola il gradiente rispetto ai parametri 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 della funzione di costo sull'intero insieme di dati di allenamento (training
 dataset) ed esegue la media aritmetica:
\begin_inset Formula 
\begin{equation}
\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}-\eta\cdot\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})
\end{equation}

\end_inset

osserviamo che per eseguire un singolo aggiornamento dei parametri (epoch)
 dobbiamo calcolare il gradiente su tutto il dataset, questo rende l'algoritmo
 estremamente lento e intrattabile nel caso in cui la dimensione del dataset
 è maggiore del quantitativo di memoria del calcolatore.
 Inoltre non permette l'aggiornamento online del modello aggiungendo o modifican
do gli esempi nel training set durante la fase di allenamento.
 Batch gradient descent converge sicuramente al minimo globale se la funzione
 da ottimizzare è convessa (caso ottimo, altamente improbabile nella realtà)
 altrimenti converge ad un minimo locale o ad un punto di sella (quest'ultimo
 non voluto).
 In codice python:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

for i in range(nb_epochs):
\end_layout

\begin_layout Plain Layout

  params_grad = gradient(loss_function, data, params)
\end_layout

\begin_layout Plain Layout

  params = params - learning_rate * params_grad
\end_layout

\end_inset

In formule il gradiente è calcolato come:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{x}|\boldsymbol{\theta})=(\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{1}},\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{2}},...,\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{d}})
\end{equation}

\end_inset

dove:
\begin_inset Formula 
\begin{equation}
\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{i}}=\frac{1}{N}\sum_{n=1}^{N}\frac{\partial J(x_{n}|\boldsymbol{\theta})}{\partial\theta_{i}}
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Stochastic gradient descent
\end_layout

\begin_layout Standard
(SGD) al contrario aggiorna i parametri per ogni esempio di allenamento
 
\begin_inset Formula $x_{i}$
\end_inset

 e il relativo output 
\begin_inset Formula $y_{i}$
\end_inset

:
\begin_inset Formula 
\begin{equation}
\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}-\eta\cdot\nabla_{\boldsymbol{\theta}}J(x_{i},y_{i}|\boldsymbol{\theta}).
\end{equation}

\end_inset

L'idea alla base di SGD è che batch gradient descent esegue calcoli ridondanti
 su interi dataset molto grandi, come ricalcolare il gradiente per esempi
 simili prima di aggiornare i pesi.
 SGD elimina questa ridondanza aggiornando i pesi ad ogni computazione del
 gradiente, questo tuttavia porta ad avere un' alta varianza del gradiente,
 con conseguenti fluttuazioni nella loss function da minimizzare.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Stogra.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Fluttuazioni SGD
\end_layout

\end_inset


\end_layout

\end_inset

Tuttavia è stato dimostrato che decrementando lentamente e gradualmente
 il learnig rate, SGD mostra le stesse caratteristiche di convergenza di
 batch gradient descent, convergendo certamente ad un minimo locale o gloabale
 per una funzione non convessa o convessa rispettivamente.
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

for i in range(nb_epochs):
\end_layout

\begin_layout Plain Layout

  for example in data:
\end_layout

\begin_layout Plain Layout

	params_grad = gradient(loss_function, data, params)
\end_layout

\begin_layout Plain Layout

	params = params - learning_rate * params_grad
\end_layout

\end_inset

In formule:
\begin_inset Formula 
\begin{equation}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{x}|\boldsymbol{\theta})=(\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{1}},\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{2}},...,\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{d}})
\end{equation}

\end_inset

dove:
\begin_inset Formula 
\begin{equation}
\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{i}}\approx\frac{\partial J(x_{n}|\boldsymbol{\theta})}{\partial\theta_{i}}
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Mini-batch gradient descent
\end_layout

\begin_layout Standard
prende il meglio dei due metodi aggiornando i pesi per ogni mini batch di
 dimensione 
\begin_inset Formula $n$
\end_inset

 esempi di training:
\begin_inset Formula 
\begin{equation}
\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}-\eta\cdot\nabla_{\boldsymbol{\theta}}J(x_{i:i+n},y_{i:i+n}|\boldsymbol{\theta}).
\end{equation}

\end_inset

In questo modo risulta ridotta la varianza del gradiente e quindi degli
 aggiornamenti dei pesi con la conseguenza di una maggiore stabilità della
 convergenza e permette di velocizzare il calcolo rispetto a SGD sfruttando
 le librerie ottimizzate del calcolo matriciale.
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

for i in range(nb_epochs):
\end_layout

\begin_layout Plain Layout

  for batch in get_batches(data, batch_size=n):
\end_layout

\begin_layout Plain Layout

	params_grad = gradient(loss_function, batch, params)
\end_layout

\begin_layout Plain Layout

	params = params - learning_rate * params_grad
\end_layout

\end_inset

In formule:
\begin_inset Formula 
\begin{equation}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{x}|\boldsymbol{\theta})=(\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{1}},\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{2}},...,\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{d}})
\end{equation}

\end_inset

dove:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial J(\boldsymbol{x}|\boldsymbol{\theta})}{\partial\theta_{i}}\approx\frac{1}{M}\sum_{n\in minibatch}\frac{\partial J(x_{n}|\boldsymbol{\theta})}{\partial\theta_{i}}
\end{equation}

\end_inset

Minibatch è un sottoinsieme del training set di cardinalità 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_layout Paragraph
Sfide.
 
\end_layout

\begin_layout Standard
Mini-batch gradient descent non garantisce ancora ottime proprietà di convergenz
a, ma offre delle sfide che devono essere affrontate:
\end_layout

\begin_layout Itemize
Scegliere l'adatto 
\series bold
learning rate
\series default
 (l.r.) può essere difficile.
 Un l.r.
 troppo piccolo porta ad una convergenza dolorosamente lenta, mentre un
 l.r.
 troppo alto causa fluttuazioni intorno ad un minimo della loss function
 o addirittura la divergenza.
\end_layout

\begin_layout Itemize
Programmare diversi l.r.
 durante le fasi di allenamento seguendo schemi predefiniti oppure adattandolo
 ai risulati intermedi dell'allenamento (
\series bold
learning rate adattivo
\series default
).
\end_layout

\begin_layout Itemize
Usare differenti l.r.
 contemporaneamente.
 Se i dati sono sparsi e le features si presentano con differenti frequenze
 potremmo considerare di utilizzare valori di l.r.
 elevati quando si presentano le features più rare.
\end_layout

\begin_layout Itemize
Uscire dai minimi sub ottimi e punti di sella.
\end_layout

\begin_layout Standard
Ora vediamo una rapida carrellata degli algoritmi più noti.
\end_layout

\begin_layout Subsubsection
Momentum
\end_layout

\begin_layout Standard
SGD ha problemi a navigare attraverso i 
\begin_inset Quotes qld
\end_inset

burroni
\begin_inset Quotes qrd
\end_inset

, per esempio zone in cui la superficie della funzione curva molto più rapidamen
te rispetto alle altre, situazione molto comune vicino ai minimi locali.
 In questo scenario, SGD oscilla lungo le pendenze del burrone e avanza
 lentamente nella direzione del minimo.
 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename SGD_momentum.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Momento
\end_layout

\end_inset


\end_layout

\end_inset


\series bold
Momentum
\series default
 è un metodo che aiuta ad accelerare SGD nella direzione rilevante e riduce
 le oscillazioni come mostrato in Figura 2.6b.
 Per fare ciò introduce un termine 
\begin_inset Formula $\gamma$
\end_inset

 nell'aggiornamento del vettore direzione come mostrato:
\begin_inset Formula 
\begin{equation}
\begin{cases}
\boldsymbol{v}_{t}=\gamma\boldsymbol{v}_{t-1}+\eta\cdot\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})\\
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\boldsymbol{v}_{t}
\end{cases}
\end{equation}

\end_inset

Il termine 
\begin_inset Formula $\gamma$
\end_inset

 comunemente usato è 0.9 o un valore simile.
 Essendo la direzione seguita al tempo t dipendente dal tempo t-1 e quindi
 ricorsivamente da tutti i tempi precedenti, permette di mantenere una sorta
 di memoria, inoltre è facile convincersi dalla formula che i cambi di direzioni
 isolati verranno attenuati mentre la direzione principale, cioè quella
 che più volte ricorre verrà incrementata.
 Un'analogia utile a comprendere è immaginare una palla che rotola lungo
 un sentiero di montagna, la palla tende ad accelerare nella direzione di
 discesa ma subisce anche altre accelerazioni isolate dovute a parziali
 intralci nel percorso, come ad esempio un sasso che la fa deviare momentaneamen
te verso destra, momentum si occupa di smorzare tali deviazioni e incrementare
 nella direzione di discesa.
\end_layout

\begin_layout Subsubsection
Nesterov accelerated gradient (NAG).
\end_layout

\begin_layout Standard
Questo metodo è un evoluzione di momentum, tornando all'analogia con la
 palla che rotola giù da un pendio, l'idea è quella di guardarsi attorno
 prima di calcolare la prossima direzione e cercare di aggirare preventivamente
 ostacoli e avvallamenti.
 Notiamo che in momentum calcoliamo preventivamente il vettore 
\begin_inset Formula $\boldsymbol{\theta}_{t-1}-\gamma\boldsymbol{v}_{t-1}$
\end_inset

 che fornisce un' approssimazione della prossima posizione dei parametri
 (i parametri sono raggruppati in un vettore, possono quindi essere visti
 come punti nello spazio).
 Allora possiamo effettivamente guardarci attorno calcolando il gradiente
 non nella posizione attuale dei parametri ma rispetto all'approssimazione
 futura.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{cases}
\boldsymbol{v}_{t}=\gamma\boldsymbol{v}_{t-1}+\eta\cdot\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}_{t-1}-\gamma\boldsymbol{v}_{t-1})\\
\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{t-1}-\boldsymbol{v}_{t}
\end{cases}
\end{equation}

\end_inset

Ancora, 
\begin_inset Formula $\gamma$
\end_inset

 comunemente usato è 0.9 o un valore simile.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename NAG.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
NAG update
\end_layout

\end_inset


\end_layout

\end_inset

In figura 2.7 vediamo un confronto tra Momentum e NAG.
 Momentum prima calcola il gradiente corrente (vettore blu piccolo) dopo
 esegue un grande passo nella direzione accumulata nei passi precedenti
 (vettore blu grande).
 NAG invece prima esegue un grande passo nella direzione predetta (vettore
 grande marrone) poi calcola il gradiente e corregge il passo (vettore rosso
 piccolo).
 Questo aggiornamento anticipato ci impedisce di accelerare troppo e aumenta
 la reattività durante l'allenamento.
 
\end_layout

\begin_layout Standard
Adesso siamo capaci di adattare gli aggiornamenti dei pesi alla pendenza
 della funzione di errore e di accelerare SGD.
 Il prossimo passo è adattare i nostri aggiornamenti specificamente per
 ogni peso in modo da eseguire aggiornamenti minori o maggiori a seconda
 dell'importanza del peso.
\end_layout

\begin_layout Subsubsection
Adagrad 
\end_layout

\begin_layout Standard
Adatta il learning rate ai parametri, eseguendo aggiornamenti maggiori ai
 pesi meno frequenti e aggiornamenti minori ai pesi più frequenti.
 Per questa ragione è adatto nel caso in cui i dati siano sparsi .
 Sia:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g_{t}=\nabla_{\theta_{t}}J(\theta_{t})
\end{equation}

\end_inset

inoltre definiamo:
\begin_inset Formula 
\begin{equation}
G_{t}=\begin{bmatrix}\begin{array}{ccccc}
\sum_{j=0}^{t}(\frac{\partial j(\boldsymbol{\theta})}{\partial\theta_{1}})^{2} & 0 & \cdots &  & 0\\
0 & \ddots &  &  & \vdots\\
\vdots &  & \sum_{j=0}^{t}(\frac{\partial j(\boldsymbol{\theta})}{\partial\theta_{i}})^{2} &  & \vdots\\
 &  &  & \ddots & \vdots\\
0 & \cdots & \cdots & \cdots & \sum_{j=0}^{t}(\frac{\partial j(\boldsymbol{\theta})}{\partial\theta_{d}})^{2}
\end{array}\end{bmatrix}
\end{equation}

\end_inset

la matrice diagonale appartenente a 
\begin_inset Formula $\mathbb{R}^{dxd}$
\end_inset

 in cui ogni elemento in posizione 
\begin_inset Formula $i,i$
\end_inset

 è la somma dei quadrati della derivata parziale rispetto al parametro 
\begin_inset Formula $i-esimo$
\end_inset

 dal tempo 0 al tempo 
\begin_inset Formula $t$
\end_inset

.
 La regola di aggiornamento diventa:
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\odot g_{t}
\end{equation}

\end_inset

dove 
\begin_inset Formula $\odot$
\end_inset

 è il prodotto elemento per elemento tra matrici.
 In questo modo risulta evidente che il l.r.
 risulta inversamente proporzionale agli aggiornamenti precedenti, più un
 peso viene aggiornato più il termine corrispondente 
\begin_inset Formula $G_{t,ii}$
\end_inset

 cresce e di conseguenza il learning rate decresce.
 Il termine 
\begin_inset Formula $\epsilon$
\end_inset

 è necessario per evitare divisioni per zero (comunemente nell'ordine di
 
\begin_inset Formula $10^{-8}$
\end_inset

).
 I ricercatori inoltre hanno notato che senza l'operatore di radice quadrata
 l'algoritmo funziona molto peggio, probabilmente dovuta a una rapida decadenza
 dei coefficienti.
 Uno dei benefici di 
\series bold
Adagrad
\series default
 è l'eliminazione della necessità di modificare manualmente il learning
 rate.
 D'altra parte introduce un' altra debolezza, anche estraendo la radice
 quadrata, a denominatore sommiamo sempre quantità positive, questo alla
 lunga porta il learning rate a valori infinitesimi, bloccando di fatto
 l'apprendimento.
 Il prossimo algoritmo ha lo scopo di risolvere questo problema.
\end_layout

\begin_layout Subsubsection
Adadelta 
\end_layout

\begin_layout Standard
È un estensione di Adagrad che cerca di ridurre l'aggressività con cui monotonic
amente decresce il learning rate.
 Invece di accumulare tutti i quadrati dei precedenti gradienti, 
\series bold
Adadelta
\series default
 restringe l'accumulo con una finestra di una fissata dimensione 
\begin_inset Formula $w.$
\end_inset

 Inoltre, invece di salvare inefficientemente tutti i 
\begin_inset Formula $w$
\end_inset

 gradienti al quadrato salva una media decadente dei precedenti gradienti.
 La media è definita nel modo seguente:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E[g^{2}]_{0}=\boldsymbol{0}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E[g^{2}]_{t}=\gamma E[g^{2}]_{t-1}+(1-\gamma)g_{t}^{2}
\end{equation}

\end_inset

con ancora:
\begin_inset Formula 
\[
g_{t}=\nabla_{\theta_{t}}J(\theta_{t})
\]

\end_inset

e 
\begin_inset Formula $\gamma$
\end_inset

 un termine simile al momento (circa 0.9).
 Abbiamo affermato precedentemente che Adadelta è l'evoluzione di Adagrad,
 richiamiamo il vettore aggiornamento di Adagrad:
\begin_inset Formula 
\begin{equation}
\Delta\theta_{t}=-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\odot g_{t}
\end{equation}

\end_inset

adesso sostituiamo semplicemente la matrice diagonale 
\begin_inset Formula $G_{t}$
\end_inset

 con la media definita:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\Delta\theta_{t}=-\frac{\eta}{\sqrt{E[g^{2}]_{t}+\epsilon}}g_{t}
\end{equation}

\end_inset

notiamo che adesso a denominatore abbiamo un vettore e non una matrice.
 Definiamo, per brevità di scrittura
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
RMS[g]_{t}=\sqrt{E[g^{2}]_{t}+\epsilon}
\end{equation}

\end_inset

dove RMS sta per root mean squared, allora
\begin_inset Formula 
\begin{equation}
\Delta\theta_{t}=-\frac{\eta}{RMS[g]_{t}}g_{t}
\end{equation}

\end_inset

il prossimo passo consiste nel rendere anche il numeratore dipendente in
 qualche modo dai parametri, definiamo quindi un'altra media, questa volta
 una media degli aggiornamenti precedenti:
\begin_inset Formula 
\begin{equation}
E[\Delta\theta^{2}]_{0}=\boldsymbol{0}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
E[\Delta\theta^{2}]_{t}=\gamma E[\Delta\theta^{2}]_{t-1}+(1-\gamma)\Delta\theta^{2}.
\end{equation}

\end_inset

La root mean squared degli aggiornamenti dei parametri è:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
RMS[\Delta\theta]_{t}=\sqrt{E[\Delta\theta^{2}]_{t}+\epsilon}.
\end{equation}

\end_inset

Siccome all'aggiornamento al tempo 
\begin_inset Formula $t$
\end_inset

 non possiamo conoscere 
\begin_inset Formula $RMS[\Delta\theta]_{t}$
\end_inset

, lo approssimiamo usando RMS al tempo precedente.
 Siamo giunti alla formula finale di Adadelta:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{cases}
\Delta\theta_{t}=-\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_{t}}g_{t}\\
\theta_{t+1}=\theta_{t}-\Delta\theta_{t}
\end{cases}.
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
RMSprop
\end_layout

\begin_layout Standard
È molto simile ad Adadelta, infatti i due metodi sono stati sviluppati indipende
ntemente nello stesso periodo e con lo scopo di risolvere i problemi di
 Adagrad.
 L'aggiornamento dei pesi segue questi passi:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E[g^{2}]_{t}=\gamma E[g^{2}]_{t-1}+(1-\gamma)g_{t}^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E[g^{2}]_{t}+\epsilon}}g_{t}
\end{equation}

\end_inset

gli autori suggeriscono di usare 
\begin_inset Formula $\gamma=0.9$
\end_inset

 e 
\begin_inset Formula $\eta=0.001.$
\end_inset


\end_layout

\begin_layout Subsubsection
Adam
\end_layout

\begin_layout Standard

\series bold
Adaptive Moment Estimation
\series default
 (
\series bold
Adam
\series default
) è un altro metodo per adattare il learning rate ad ogni parametro.
 In aggiunta alla media decadente dei gradienti precedenti al quadrato 
\begin_inset Formula $v_{t}$
\end_inset

 come Adadelta e RMSprop, Adam mantiene anche una media decadente dei gradienti
 passati 
\begin_inset Formula $m_{t}$
\end_inset

 (NB non il quadrato):
\begin_inset Formula 
\begin{equation}
m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}.
\end{equation}

\end_inset

Notiamo che il termine 
\begin_inset Formula $v_{t}$
\end_inset

 è del tutto analogo a 
\begin_inset Formula $E[g^{2}]_{t}$
\end_inset

 di Adadelta e RMSprop.
 
\begin_inset Formula $m_{t}$
\end_inset

 e 
\begin_inset Formula $v_{t}$
\end_inset

 sono stime del momento primo (media) e momento secondo (varianza) dei gradienti
 rispettivamente, da qui il nome del metodo.
 Entrambi i termini vengono inizializzati a 0, gli autori hanno notato però
 che questa inizializzazione porta un bias che fa tendere l'aggiornamento
 a 0, soprattutto con valori di 
\begin_inset Formula $\beta_{1},\beta_{2}$
\end_inset

 vicini a 1.
 Per contrastare questo bias introduciamo una correzione a entrambi i termini:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}
\end{equation}

\end_inset

L'aggiornamento diventa allora:
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\hat{m}_{t}
\end{equation}

\end_inset

Gli autori propongono come valori di default: 
\begin_inset Formula $\beta_{1}=0.9$
\end_inset

, 
\begin_inset Formula $\beta_{2}=0.999$
\end_inset

 e 
\begin_inset Formula $\epsilon=10^{-8}.$
\end_inset

 Hanno mostrato empiricamente che Adam lavora bene e con prestazioni simili
 a Adadelta e RMSProp.
\end_layout

\begin_layout Subsubsection
AdaMax
\end_layout

\begin_layout Standard
In Adam calcoliamo 
\begin_inset Formula $v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$
\end_inset

, notiamo che 
\begin_inset Formula $v_{t}$
\end_inset

 è direttamente proporzionale alla norma 
\begin_inset Formula $l_{2}$
\end_inset

 del gradiente e quindi l'aggiornamento è inversamente proporzionale alla
 norma del gradiente.
 Possiamo generalizzare l'aggiornamento usando 
\begin_inset Formula $l_{p}$
\end_inset

 norma.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})|g_{t}|^{p}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
=(1-\beta_{2})\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p}.
\end{equation}

\end_inset

La norma 
\begin_inset Formula $l_{p}$
\end_inset

 è numericamente instabile per grandi valori di 
\begin_inset Formula $p$
\end_inset

, questo è il motivo per cui generalmente si usa 
\begin_inset Formula $l_{1},l_{2}$
\end_inset

.
 Tuttavia la norma 
\begin_inset Formula $l_{\infty}$
\end_inset

 mostra ancora un comportamento molto stabile, per questo gli autori di
 
\series bold
AdaMax
\series default
 la propongono.
 Definiamo 
\begin_inset Formula $u_{t}=\lim_{p\rightarrow\infty}(v_{t})^{1/p}$
\end_inset

 allora:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
u_{t}=\lim_{p\rightarrow\infty}(v_{t})^{1/p}=\lim_{p\rightarrow\infty}((1-\beta_{2})\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p})^{1/p}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\lim_{p\rightarrow\infty}(1-\beta_{2})^{1/p}(\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p})^{1/p}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\lim_{p\rightarrow\infty}(\sum_{i=1}^{t}\beta_{2}^{(t-i)}|g_{i}|^{p})^{1/p}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=max(\beta_{2}^{t-1}|g_{1}|,\beta_{2}^{t-2}|g_{2}|,...,\beta_{2}|g_{t-1}|,|g_{t}|)
\end{equation}

\end_inset

che può essere riscritta ricorsivamente come:
\begin_inset Formula 
\begin{equation}
u_{t}=max(\beta_{2}u_{t-1},|g_{t}|).
\end{equation}

\end_inset

Al solito, 
\begin_inset Formula $u_{0}=0.$
\end_inset

 Sostituiamo nella formula di Adam 
\begin_inset Formula $\sqrt{\hat{v}_{t}}+\epsilon$
\end_inset

 con 
\begin_inset Formula $u_{t}$
\end_inset

, otteniamo così la regola di aggiornamento AdaMax:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{u_{t}}\hat{m}_{t}
\end{equation}

\end_inset

I valori consigliati di default sono 
\begin_inset Formula $\eta=0.002$
\end_inset

, 
\begin_inset Formula $\beta_{1}=0.9$
\end_inset

, 
\begin_inset Formula $\beta_{2}=0.999$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Nadam 
\end_layout

\begin_layout Standard
Come visto prima, Adam può essere visto come una combinazione di RMSprop
 e Momentum: RMSprop contribuisce tramite il termine 
\begin_inset Formula $v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$
\end_inset

 e momentum tramite il termine 
\begin_inset Formula $m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}$
\end_inset

.
 Abbiamo visto anche che Nesterov accelerated gradient (NAG) è superiore
 a momentum.
 
\series bold
Nadam
\series default
 (
\series bold
Nesterov-accelerated Adaptive Moment Estimation
\series default
) combina così Adam e NAG.
 In NAG calcoliamo il gradiente non nella posizione attuale ma nella posizione
 stimata a priori, in cui arriveremo dopo l' aggiornamento.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g_{t}=\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}_{t}-\gamma\boldsymbol{m}_{t-1})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
m_{t}=\gamma m_{t-1}+\eta g_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{t+1}=\theta_{t}-m_{t}
\]

\end_inset

Gli autori di Nadam propongono di modificare NAG in questo modo: piuttosto
 che applicare il momento due volte, una volta per guardarsi attorno nel
 calcolo del gradiente 
\begin_inset Formula $g_{t}$
\end_inset

 e una seconda volta nel calcolo di 
\begin_inset Formula $\theta_{t+1}$
\end_inset

, usiamo il momento corrente 
\begin_inset Formula $m_{t}$
\end_inset

 per guardarci attorno direttamente nell'aggiornamento dei pesi e non nel
 gradiente, allora NAG modificato diventa:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g_{t}=\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}_{t})
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
m_{t}=\gamma m_{t-1}+\eta g_{t}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-(\gamma m_{t}+\eta g_{t})
\end{equation}

\end_inset

Richiamiamo brevemente anche il metodo Adam:
\begin_inset Formula 
\[
m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\hat{m}_{t}
\]

\end_inset

Espandiamo l'ultima equazione:
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\frac{m_{t}}{1-\beta_{1}^{t}})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\frac{\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\frac{\beta_{1}m_{t-1}}{1-\beta_{1}^{t}}+\frac{(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}

\end_inset

Notiamo che 
\begin_inset Formula $\frac{m_{t-1}}{1-\beta_{1}^{t}}=\hat{m}_{t-1}$
\end_inset

 sostituendo:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\beta_{1}\hat{m}_{t-1}+\frac{(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}

\end_inset

Notiamo che questa regola è ancora Adam, per giungere a Nadam dobbiamo inserire
 NAG modificato in precedenza, notiamo che in NAG modificato usiamo il momento
 corrente nell' aggiornamento e non il momento del passo precedente, modifichiam
o di conseguenza la formula di aggiornamento:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\beta_{1}\hat{m}_{t}+\frac{(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename confronto_discesa.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Confronto algoritmi
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Backpropagation
\end_layout

\begin_layout Standard
In questa sezione svilupperemo una comprensione intuitiva della 
\series bold
backpropagation
\series default
 che permette di calcolare agilmente il gradiente della funzione di costo
 (error function o loss function) di una rete neurale, tramite l'applicazione
 ricorsiva della regola di derivazione di funzioni composte, nota anche
 come 
\series bold
regola della catena
\series default
.
\end_layout

\begin_layout Paragraph
Derivazione funzione composta.
\end_layout

\begin_layout Standard
Consideriamo due funzioni reali di varibile reale (la regola è valida in
 generale per tutte le funzioni differenziabili, anche a più variabili con
 tutti gli adattamenti del caso) 
\begin_inset Formula $f:\mathbb{R}\rightarrow\mathbb{R}$
\end_inset

 e 
\begin_inset Formula $g:\mathbb{R}\rightarrow\mathbb{R}$
\end_inset

 e chiamiamole 
\begin_inset Formula $y=g(x)$
\end_inset

 e 
\begin_inset Formula $z=f(y)$
\end_inset

.
 Sia poi 
\begin_inset Formula $z=f(y)=f(g(x))$
\end_inset

 la loro composizione.
 Allora vale:
\begin_inset Formula 
\begin{equation}
\frac{dz}{dx}=\frac{dz}{dy}\cdot\frac{dy}{dx}
\end{equation}

\end_inset

(come se il differenziale 
\begin_inset Formula $dy$
\end_inset

 si semplificasse nella moltiplicazione, con buona pace dei matematici).
 Sostituendo:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{dz}{dx}=\frac{d}{dy}f(y)\cdot\frac{d}{dx}g(x)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
=f'(y)\cdot g'(x)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
=f'(g(x))\cdot g(x)
\end{equation}

\end_inset

In parole povere, la derivata della funzione composta 
\begin_inset Formula $z=f(g(x))$
\end_inset

 è data dalla derivata della funzione più esterna, con argomento invariato,
 moltiplicata per la derivata della funzione più interna.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Rete_per_regressione.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Rete neurale multistrato
\end_layout

\end_inset


\end_layout

\end_inset

In Figura 2.10 è mostrata una rete neurale che computa la funzione 
\begin_inset Formula $g_{1}:\mathbb{R}^{I}\rightarrow\mathbb{R}$
\end_inset

 parametrizzata da 
\begin_inset Formula $w$
\end_inset

.
 La funzione da minimizzare è 
\begin_inset Formula $E(w|t_{n},x_{n})$
\end_inset

, notiamo che 
\begin_inset Formula $E$
\end_inset

 è funzione di 
\begin_inset Formula $w$
\end_inset

 e parametrizzata da 
\begin_inset Formula $t_{n}$
\end_inset

 e 
\begin_inset Formula $x_{n}$
\end_inset

.
 Usando la regola della catena calcoliamo la derivata: 
\begin_inset Formula 
\begin{equation}
\frac{\partial E(w)}{\partial w_{ji}}=-2\sum_{n=1}^{N}(t_{n}-g_{1}(x_{n},w))\cdot g'_{1}(x_{n},w)\cdot w_{1j}^{(2)}\cdot h'_{j}(\sum_{j=0}^{J}w_{1j}^{(1)}\cdot x_{i,n})\cdot x_{i}
\end{equation}

\end_inset

Infatti notiamo:
\begin_inset Formula 
\begin{equation}
\frac{\partial E(w)}{\partial g_{1}(x_{n},w)}=-2\sum_{n=1}^{N}(t_{n}-g_{1}(x_{n},w))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial g_{1}(x_{n},w)}{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}=g'_{1}(x_{n},w)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}{\partial h{}_{j}(\bullet)}=w_{1j}^{(2)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial h{}_{j}(\bullet)}{\partial w_{1j}^{(1)}\cdot x_{i,n}}=h'_{j}(\sum_{j=0}^{J}w_{1j}^{(1)}\cdot x_{i,n})
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\frac{\partial w_{1j}^{(1)}\cdot x_{i,n}}{\partial w_{1j}^{(1)}}=x_{i}
\end{equation}

\end_inset

Allora:
\begin_inset Formula 
\begin{equation}
\frac{\partial E(w)}{\partial w_{ji}}=\frac{\partial E(w)}{\partial g_{1}(x_{n},w)}\cdot\frac{\partial g_{1}(x_{n},w)}{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}\cdot\frac{\partial w_{1j}^{(2)}\cdot h{}_{j}(\bullet)}{\partial h{}_{j}(\bullet)}\cdot\frac{\partial h{}_{j}(\bullet)}{\partial w_{1j}^{(1)}\cdot x_{i,n}}\cdot\frac{\partial w_{1j}^{(1)}\cdot x_{i,n}}{\partial w_{1j}^{(1)}}.
\end{equation}

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename backprop.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Backpropagation
\end_layout

\end_inset


\end_layout

\end_inset

Dal punto di vista pratico, la regola della catena è molto utile, permette
 facilmente di parallelizzare ed eseguire localmente il calcolo del gradiente
 e l'aggiornamento dei pesi.
 Ogni neurone ha la propria 
\shape italic
funzione di attivazione
\shape default
 con la propria derivata.
 Possiamo calcolare le derivate singolarmente e propagare all'indietro semplicem
ente tramite un prodotto.
 Per esempio nella rete in Figura 2.10 il neurone di output calcola tutte
 le derivate parziali della sua funzione di attivazione rispetto ai propri
 parametri di ingresso, il risultato così ottenuto viene propagato allo
 strato precedente, in cui ogni neurone ha già calcolato le proprie derivate
 rispetto ai suoi parametri di input, le moltiplica con la derivata ricevuta
 dallo strato successivo e propaga all'indietro e così via fino allo strato
 di input.
 Inoltre quando un neurone esegue il prodotto tra le sue derivate e quella
 ricevuta dallo strato successivo, è in possesso del gradiente della funzione
 costo rispetto a tutti i suoi parametri e quindi procede ad aggiornare
 i pesi secondo una delle strategie viste nella precedente sezione.
 
\end_layout

\begin_layout Subsection
Scomparsa del gradiente
\end_layout

\begin_layout Standard
I problemi che si possono incontrare usando la backpropagation sono molteplici
 e dipendono da vari fattori, come profondità della rete, funzioni di attivazion
e scelte, inizializzazione dei pesi e altri.
 Alcuni sono già stati accennati e trattati nei vari algoritmi di discesa
 del gradiente visti.
 Il problema della 
\series bold
scomparsa del gradiente
\series default
 (in lingua inglese 
\series bold
vanishing gradient
\series default
 problem) è un fenomeno che crea difficoltà nell'addestramento delle reti
 neurali profonde tramite retropropagazione dell'errore mediante discesa
 stocastica del gradiente.
 Come visto, ogni parametro del modello riceve ad ogni iterazione un aggiornamen
to proporzionale alla derivata parziale della funzione di perdita rispetto
 al parametro stesso.
 Una delle principali cause è la presenza di funzioni di attivazione non
 lineari classiche, come la 
\shape italic
tangente iperbolica
\shape default
 o la 
\shape italic
funzione logistica
\shape default
, che hanno gradiente a valori nell'intervallo 
\begin_inset Formula ${\displaystyle (0,1)}$
\end_inset

.
 Poiché nell'algoritmo di retropropagazione i gradienti ai vari livelli
 vengono moltiplicati tramite la regola della catena, il prodotto di 
\begin_inset Formula ${\displaystyle n}$
\end_inset

 numeri in 
\begin_inset Formula ${\displaystyle (0,1)}$
\end_inset

 decresce esponenzialmente rispetto alla profondità 
\begin_inset Formula ${\displaystyle n}$
\end_inset

 della rete.
 Quando invece il gradiente delle funzioni di attivazione può assumere valori
 elevati, un problema analogo che può manifestarsi è quello dell'esplosione
 del gradiente.
\end_layout

\begin_layout Section
Overfitting
\end_layout

\begin_layout Standard
In statistica e in informatica, si parla di 
\series bold
overfitting
\series default
 (in italiano: adattamento eccessivo, sovradattamento) quando un modello
 statistico molto complesso si adatta ai dati osservati (il campione) perché
 ha un numero eccessivo di parametri rispetto al numero di osservazioni.
 Un modello assurdo e sbagliato può adattarsi perfettamente se è abbastanza
 complesso rispetto alla quantità di dati disponibili.
 Si sostiene che l'overfitting sia una violazione del principio del Rasoio
 di Occam.
 
\end_layout

\begin_layout Paragraph
Apprendimento automatico
\end_layout

\begin_layout Standard
Il concetto di overfitting è molto importante anche nell'apprendimento automatic
o e nel data mining.
 Di solito un algoritmo di apprendimento viene allenato usando un certo
 insieme di esempi (il training set appunto), ad esempio situazioni tipo
 di cui è già noto il risultato che interessa prevedere (output).
 Si assume che l'algoritmo di apprendimento (il learner) raggiungerà uno
 stato in cui sarà in grado di predire gli output per tutti gli altri esempi
 che ancora non ha visionato, cioè si assume che il modello di apprendimento
 sarà in grado di 
\series bold
generalizzare
\series default
.
 Tuttavia, soprattutto nei casi in cui l'apprendimento è stato effettuato
 troppo a lungo o dove c'era uno scarso numero di esempi di allenamento,
 il modello potrebbe adattarsi a caratteristiche che sono specifiche solo
 del training set, ma che non hanno riscontro nel resto dei casi; perciò,
 in presenza di overfitting, le prestazioni (cioè la capacità di adattarsi/preve
dere) sui dati di allenamento aumenteranno, mentre le prestazioni sui dati
 non visionati saranno peggiori.
\end_layout

\begin_layout Paragraph
Contromisure
\end_layout

\begin_layout Standard
Sia nella statistica sia nel apprendimento automatico, per prevenire ed
 evitare l'overfitting è necessario mettere in atto particolari accorgimenti
 tecnici, come la convalidazione incrociata e l'arresto anticipato, che
 indicano quando un ulteriore allenamento non porterebbe a una migliore
 generalizzazione.
\end_layout

\begin_layout Paragraph
Cross validation 
\end_layout

\begin_layout Standard
La convalidazione incrociata è un accorgimento che non permette direttamente
 di evitare l'overfitting ma ci permette di riconoscere quando avviene.
 L'idea è molto semplice e quasi sempre applicabile, a patto di avere un
 training set abbastanza grande, consiste nel rimuovere dal training set
 un certo numero di esempi e spostarli nel validation set.
 La rete si allena ancora normalmente sul training set, che risulterà ridotto,
 successivamente ad ogni epoca vengono sottoposti gli esempi del validation
 set che vengono processati dalla rete, quest' ultima durante la fase di
 validazione non si allena (non aggiorna i pesi) e viene monitorata la funzione
 di errore che in assenza di overfitting mostrerà un andamento simile alla
 funzione di errore valutata durante la fase di allenamento, nel momento
 in cui la rete inizierà ad adattarsi eccessivamente noteremo che la funzione
 di errore valutata nel validation set iniziera ad aumentare mentre quando
 la valutiamo nel training set continuerà a decrescere.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Overfitting_svg.svg.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overfitting: La curva blu mostra l'andamento dell'errore nel classificare
 i dati di training, mentre la curva rossa mostra l'errore nel classificare
 i dati di test o validazione.
 Una situazione in cui il secondo aumenta mentre il primo diminuisce è indice
 della possibile presenza di un caso di overfitting.
 
\end_layout

\end_inset


\end_layout

\end_inset

Esistono varie implementazioni della cross validation.
\end_layout

\begin_layout Subsection
Holdout cross validation
\end_layout

\begin_layout Standard
È il metodo più semplice per implementare la 
\series bold
validazione incrociata
\series default
.
 Il data set viene diviso staticamente in due parti, 
\series bold
training set
\series default
 e 
\series bold
validation set
\series default
 e i due sottoinsiemi vengono usati come descritto prima.
\end_layout

\begin_layout Subsection
K-fold cross validation
\end_layout

\begin_layout Standard
Il data set viene suddiviso in 
\series bold
k
\series default
 sotto insiemi di egual dimensione (escluso al più l'ultimo), il training
 del modello viene ripetuto k volte ed ogni volta viene usato un sotto insieme
 come validation set, ogni volta ripartendo dallo stato iniziale.
 Alla fine delle k iterazioni i risultati delle validazioni vengono mediati
 per ottenere il risultato finale.
 Il vantaggio di questo metodo è che importa meno come i dati vengono divisi
 in quanto dopo k iterazioni tutto il dataset è stato usato almeno una volta
 come validation set (meno varianza).
 Lo svantaggio è l'enorme peso computazionale (training ripetuto k volte)
 per questo è quasi mai usato.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kfoldvalidation.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
K-fold cross validation: in giallo il sottoinsieme di validazione
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Leave-one-out cross validation
\end_layout

\begin_layout Standard
È una variante di k-fold dove k viene posto uguale a N cardinalità del dataset.
 Quindi la rete si allena su N-1 esempi e un solo esempio è usato per la
 validazione.
\end_layout

\begin_layout Subsection
Iperparametri
\end_layout

\begin_layout Standard
Fino ad ora abbiamo visto molti tasselli che compongono una rete neurale
 ma non come combinarli assieme.
 Purtroppo ancora non è stata sviluppata una metodologia di progettazione
 che possa fornire linee guida solide per la creazione di una rete neurale
 per un determinato compito.
 Alcuni strumenti matematici possono aiutarci ad esempio come abbiamo fatto
 per trovare la funzione di errore per la regressione.
 Per altre grandezze invece va fatto ricorso all'esperienza, ad altri risultati
 pubblicati riguardanti reti che svolgono un compito simile e tanti, tantissimi
 tentativi ed errori.
 Per valutare la bontà di un modello possiamo usare l'
\series bold
insieme di test
\series default
 e ancora la cross validation.
 Quanti e quali sono quindi gli 
\series bold
iperparametri
\series default
 che il progettista della rete deve scegliere? Molti, per esempio in una
 rete feedforward bisogna definire:
\end_layout

\begin_layout Itemize

\series bold
numero dei neuroni
\series default
 e loro disposizione in strati;
\end_layout

\begin_layout Itemize

\series bold
funzione di attivazione
\series default
 dei neuroni;
\end_layout

\begin_layout Itemize

\series bold
funzione di errore
\series default
;
\end_layout

\begin_layout Itemize
algoritmo di 
\series bold
discesa del gradiente
\series default
;
\end_layout

\begin_layout Standard
eventualmente anche:
\end_layout

\begin_layout Itemize

\series bold
dimensione batch
\series default
;
\end_layout

\begin_layout Itemize

\series bold
dimensione validation set
\series default
 per la cross validation;
\end_layout

\begin_layout Itemize
molti altri a seconda della complessità della rete.
\end_layout

\begin_layout Standard
Come scegliere questi iperparametri?
\end_layout

\begin_layout Itemize
usare tutte le conoscenze pregresse e ipotesi sulla distribuzione dei dati
 da processare;
\end_layout

\begin_layout Itemize
sfruttare le conoscienze acquisite da altri nella risoluzione di compiti
 simili;
\end_layout

\begin_layout Itemize
usare la creatività!
\end_layout

\begin_layout Standard
Anche per la scelta degli iperparametri del modello è di aiuto la cross
 validation.
 Infatti possiamo usare (ed è altamente consigliato) la validazione per
 monitorare l'andamento della loss function e determinare la bontà del modello
 al variare degli iperparametri.
 È utile usare i dati di validazione al posto di quelli di test per evitare
 che le nostre scelte degli iperparametri siano condizionate e sovra adattate
 al test set, perdendo generalità.
 Modifichiamo gli iperparametri in base ai risultati sul validation set,
 se questi sono confermati anche dal test set possiamo ragionevolmente affermare
 che il modello funzioni.
 Se il test set non conferma allora abbiamo sovra adattato gli iperparametri
 al validation set (una qualche forma di overfitting manuale).
 
\end_layout

\begin_layout Subsection
Weight decay
\end_layout

\begin_layout Standard
La regolarizzazione riguarda il vincolamento della “libertà” di un modello,
 sulla base di un’assunzione a-priori sul modello stesso, per ridurre l’overfitt
ing.
 Finora abbiamo massimizzato la verosimiglianza dei dati
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w_{MLE}=argmax_{w}\hphantom{}P(D|w)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
questo approccio è detto frequentista.
 Usando, invece, un approccio 
\series bold
Bayesiano
\series default
, possiamo ridurre la libertà del modello.
 Usiamo, cioè, una 
\series bold
Maximum A-Posteriori verosimiglianza
\series default
:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w_{MAP}=argmax_{w}P(w|D)\approx argmax_{w}P(D|w)·P(w)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
dove 
\begin_inset Formula $P(w)$
\end_inset

 è la probabilità a-priori, che definisce lo spazio di ricerca dell’algoritmo.
 Si è osservato che piccoli pesi migliorano la capacità di generalizzazione
 delle reti neurali.
 Weight decay in pratica aggiunge una penalità proporzionale alla norma
 L2 dei pesi alla loss function, con il fine di imprimere una direzione
 al gradiente che riduca la norma dei pesi.
 Oltre all'evidenza empirica in taluni casi weigt decay è supportato dalla
 teoria matematica usando 
\series bold
Maximum A-Posteriori verosimiglianza
\series default
.
 Analiziamo il caso già visto della regressione, ma utilizzando 
\shape italic
MAP
\shape default
 e supponiamo che i pesi vengano inizializzati con una distribuzione 
\begin_inset Formula $P(w)\approx N(0,\sigma_{w})$
\end_inset

 che come vedremo più avanti è assolutamente ragionevole come ipotesi.
 
\begin_inset Formula 
\[
argmax_{w}P(D|w)·P(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t_{i}-g(x_{i}|w))^{2}}{2\sigma^{2}}}*\prod_{q=1}^{Q}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(w_{q})^{2}}{2\sigma^{2}}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=argmin_{w}(\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2})+\sum_{q=1}^{Q}\frac{(w_{q})^{2}}{2\sigma^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=argmin_{w}(\sum_{i=1}^{N}(t_{i}-g(x_{i}|w))^{2})+\gamma\sum_{q=1}^{Q}(w_{q})^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Come scegliere il parametro gamma? Una possibile strategia è tramite cross-valid
ation.
\end_layout

\begin_layout Section
Altre tecniche di ottimizzazione
\end_layout

\begin_layout Subsection
Shuffling e Curriculum Learning
\end_layout

\begin_layout Standard
Generalmente allenare una rete fornendole gli esempi sempre nello stesso
 ordine può portare ad un bias nell'algoritmo di ottimizzazione e quindi
 all'overfitting.
 Di conseguenza una buona idea è fornire gli esempi mescolati ad ogni epoca.
 In altri casi, in cui lo scopo della rete è risolvere via via problemi
 sempre più complessi, allora è meglio fornire gli esempi in ordine di complessi
tà.
 Quest'ultimo è il caso del 
\series bold
Curriculum learning
\series default
.
\end_layout

\begin_layout Subsection
Preprocessamento dei dati
\end_layout

\begin_layout Standard
Ci sono tre operazioni principali che possono essere eseguite sui dati di
 input prima di essere processati dalla rete (sia durante il training che
 durante il test).
 Ricordiamo che per noi l'input è in linea generale un vettore, quindi può
 essere visto come un punto nello spazio e l'insieme dei dati, rappresentato
 da una matrice 
\begin_inset Formula $X\in\mathbb{R}^{N\times D}$
\end_inset

 dove 
\begin_inset Formula $N$
\end_inset

 è il numero dei dati, 
\begin_inset Formula $D$
\end_inset

 la loro dimensione.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Dati_originali.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Dati originali
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Sottrazione della media
\end_layout

\begin_layout Standard
Questa operazione ha il compito di centrare la media dell'insieme dei dati
 in zero.
 Ad ogni coordinata di tutti i vettori viene sottratta la media rispettiva.
 In formule:
\end_layout

\begin_layout Standard
sia 
\begin_inset Formula $\boldsymbol{x}_{i}$
\end_inset

 l'i-esimo vettore appartente a 
\begin_inset Formula $X$
\end_inset

 di dimensione 
\begin_inset Formula $D$
\end_inset

, 
\begin_inset Formula $\boldsymbol{x}_{i}=(x_{i,1},...,x_{i,D}$
\end_inset

)
\begin_inset Formula 
\begin{equation}
\boldsymbol{y}_{i}=\boldsymbol{x}_{i}-\widehat{\boldsymbol{x}}
\end{equation}

\end_inset

dove
\begin_inset Formula 
\begin{equation}
\widehat{\boldsymbol{x}}=(\frac{1}{N}\sum_{i=1}^{N}x_{i,1},\frac{1}{N}\sum_{i=1}^{N}x_{i,2},...,\frac{1}{N}\sum_{i=1}^{N}x_{i,D})
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Normalizzazione
\end_layout

\begin_layout Standard
Ci riferiamo alla 
\series bold
normalizzazione
\series default
 dei dati non in senso geometrico classico, che consiste nel dividire ogni
 vettore per la sua norma, ma come l'operazione di riscalamento di ogni
 dimensione in modo tale che ognuna di esse abbia circa la stessa scala.
 Per far ciò possiamo operare in due modi:
\end_layout

\begin_layout Enumerate
Dividiamo ogni dimensione di ogni vettore per la deviazione standard di
 quella dimensione (analogo a quanto fatto per la media).
 In formule:
\begin_inset Formula 
\begin{equation}
\boldsymbol{y}_{i}=\frac{\boldsymbol{x}_{i}}{\boldsymbol{\sigma}}
\end{equation}

\end_inset

dove
\begin_inset Formula 
\begin{equation}
\boldsymbol{\sigma}=(\sqrt{\frac{1}{N}\sum(x_{i,1}-\widehat{\boldsymbol{x}}_{1})^{2}},\sqrt{\frac{1}{N}\sum(x_{i,2}-\widehat{\boldsymbol{x}}_{2})^{2}},...,\sqrt{\frac{1}{N}\sum(x_{i,D}-\widehat{\boldsymbol{x}}_{D})^{2}})
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
Dividiamo ogni dimensione di ogni vettore per la differenza tra la coordinata
 massima e minima rispettiva.
 In formule:
\begin_inset Formula 
\begin{equation}
\boldsymbol{y}_{i}=\frac{\boldsymbol{x}_{i}}{\boldsymbol{\Delta}}
\end{equation}

\end_inset

dove 
\begin_inset Formula 
\begin{equation}
\boldsymbol{\Delta}=(\max_{i}(x_{i,1}),\max_{i}(x_{i,2}),...,\max_{i}(x_{i,D}))
\end{equation}

\end_inset

in questo modo l'input normalizzato avrà ogni componente compresa nell'intervall
o 
\begin_inset Formula $[-1,1]$
\end_inset


\end_layout

\begin_layout Standard
Generalmente 
\series bold
sottrazione della media
\series default
 e 
\series bold
normalizzazione
\series default
 vengono effettuate in coppia.
 Ha senso normalizzare se si ha ragione di credere che l'input abbia features
 in scale differenti ma equamente importanti.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename preprocessing_data.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Preprocessamento completo
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Attenzione: 
\series default
un punto importante da sottolineare nelle preelaborazioni è che qualsiasi
 statistica (es.
 media, varianza etc) deve essere calcolata solo sui dati di addestramento
 e quindi applicata ai tutti i dati (allenamento, validazione, test).
 Ad esempio calcolare la media su tutti i dati e poi suddividere in allenamento,
 validazione e test sarebbe un errore.
 Invece le statistiche vanno calcolate solo sui dati di allenamento e poi
 applicate a tutti i dati.
\end_layout

\begin_layout Subsection
Batch Normalization
\end_layout

\begin_layout Standard
Con l’aumento del numero di strati delle reti neurali permesso dalla sempre
 crescente potenza di calcolo dei computer odierni sono sorti vari problemi
 soprattutto durante l’addestramento, tra cui l’esplosione e la 
\shape italic
scomparsa del gradiente
\shape default
.
 Una delle possibili soluzioni a questi problemi consiste nel layer di 
\series bold
batch normalization
\series default
.
 Come già detto precedentemente, solitamente l’input di una rete viene 
\shape italic
normalizzato
\shape default
 (cioè tutti gli input vengono riscalati per avere valori compresi in un
 range scelto, solitamente [0,1]) o 
\shape italic
standardizzato
\shape default
 (cioè ai valori degli input gli viene sottratta la media e vengono divisi
 per la deviazione standard del dataset, per avere dei dati distribuiti
 con media 0 e deviazione standard 1).
 Ciò aiuta l’addestramento in quanto riduce il range dinamico dei dati in
 input a un range fisso, permettendo alla rete di estrarre feature più robuste
 e più velocemente.
 Tuttavia se la rete ha un elevato numero di layer, a seconda dei valori
 dei pesi l’output dei layer potrebbe tornare ad avere range dinamici ampi.
 Per ovviare a questo problema, si interpone un layer di batch normalization
 dopo il layer della rete da normalizzare.
 In questo modo non solo l’input della rete ma anche l’output dei vari layer
 viene standardizzato.
 Ogni layer di batch normalization ha 
\series bold
due pesi
\series default
 (
\shape italic
per ogni batch
\shape default
), un 
\series bold
fattore di scala
\series default
 e un 
\series bold
bias
\series default
, che modificano l’output standardizzato permettendo di cambiarne media
 e deviazione standard.
 Questi pesi possono venire aggiustati durante l’addestramento.
 Il termine 
\series bold
batch
\series default
 nel nome deriva dal gruppo di dati su cui viene effettuata la normalizzazione
 nel layer, che in questo caso è appunto un batch utilizzato durante l’addestram
ento con 
\series bold
Stochastic Gradient Descent
\series default
 (
\series bold
SGD
\series default
).
 Per implementare la batch normalization:
\end_layout

\begin_layout Standard
si trova il valore medio per il batch corrente 
\begin_inset Formula $B=\{x_{1},...,x_{m}\}$
\end_inset

, ovvero il valore medio prodotto da un particolare sub-strato della rete,
 prima di passare dalla funzione di attivazione non-lineare, quindi
\begin_inset Formula 
\[
\mu_{B}=\frac{1}{m}\sum_{i=1}^{m}x_{i}
\]

\end_inset

si trova la varianza per il batch corrente
\begin_inset Formula 
\[
\sigma_{B}^{2}=\frac{1}{m}\sum_{i=1}^{m}(x_{i}-\mu_{B})^{2}
\]

\end_inset

si normalizza il valore con l’equazione
\begin_inset Formula 
\[
\widehat{x_{i}}=\frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}}
\]

\end_inset

Quindi, ad ogni valore si sottrae la media e si divide per una deviazione
 standard a cui si aggiunge il valore 
\begin_inset Formula $ϵ$
\end_inset

.
 Il valore 
\begin_inset Formula $ϵ$
\end_inset

 è una costante positiva, ad esempio 0.001, che conferisce maggiore stabilità
 numerica (evita divisioni per zero) e incrementa, di poco, la varianza
 per ogni batch.
 Incrementare la varianza ha lo scopo di tenere in considerazione che la
 varianza della popolazione è maggiore di ogni campione preso dalla popolazione
 stessa.
 Il valore normalizzato viene poi moltiplicato per Gamma a sommato al parametro
 Beta, entrambi parametri che la rete apprenderà in fase di training:
\begin_inset Formula 
\begin{equation}
y_{i}=\gamma\widehat{x_{i}}+\beta
\end{equation}

\end_inset


\begin_inset Formula $y_{i}$
\end_inset

 è il valore normalizzato prodotto da ogni sub-strato della rete che passerà
 attraverso la funzione di attivazione, quali Sigmoide, Relu, Tanh ecc ecc.
 Durante il training il gradiente dovrà propagarsi a ritroso (backpropagation)
 attraverso questa trasformazione, affinchè Beta e Gamma ricevano il segnale
 di errore e vengano ottimizzati.
 La fase di inferenza, rispetto alla fase di training, presenta alcune differenz
e: la rete, infatti, non viene esposta ad un batch di input ma ad un solo
 valore di input, per il quale dovrà produrre un output.
 Se utilizzassimo la medesima tecnica applicata durante il training dovremmo
 calcolare media e varianza su un singolo valore e non si produrrebbe quindi
 nessun risultato sensato.
 Per superare questo problema, la rete, durante la fase di testing, normalizza
 i valori di input utilizzando media e varianza stimati durante la fase
 di training.
 La Batch normalization si può utilizzare su 
\series bold
reti feed forward
\series default
, come su 
\series bold
reti convoluzionali
\series default
 e 
\series bold
reti ricorrenti
\series default
.
 Per le reti ricorrenti, 
\shape italic
lstm, gru o vanilla
\shape default
, media e varianza vengono calcolate per ogni step di tempo anzichè per
 ogni strato.
 Per le reti convoluzionali media e varianza vengono calcolate per ogni
 filtro.
 I vantaggi introdotti dalla batch normalization sono molteplici: 
\end_layout

\begin_layout Itemize
Training della rete più veloce 
\end_layout

\begin_layout Itemize
Si possono utilizzare tassi di apprendimento più alti 
\end_layout

\begin_layout Itemize
L’inizializzazione dei pesi della rete può essere fatta con meno cautela
 
\end_layout

\begin_layout Itemize
Le funzioni di attivazione quali Sigmoide e Relu sono utilizzabili anche
 con reti maggiormente profonde 
\end_layout

\begin_layout Itemize
Fornisce una sorta di regolarizzazione aggiuntiva e potrebbe ridurre la
 necessità di Dropout 
\end_layout

\begin_layout Itemize
Miglioramento delle Performance in generale
\end_layout

\begin_layout Subsection
Early Stopping
\end_layout

\begin_layout Standard
Consiste nel monitorare l'errore durante la fase di validazione.
 Quando alleniamo una rete decidiamo a priori per quante epoche allenarla
 (un'epoca corrisponde a sottoporre alla rete tutto il training set), dopo
 un certo numero di epoche osserviamo che l'errore di validazione che prima
 diminuiva inizia ad aumentare, magari oscillando anche, questo è il caso
 dell'overfitting.
 
\series bold
Early stopping
\series default
 è un metodo parametrizzato da tre fattori, quantità da monitorare, un delta
 che indica il valore assoluto o percentuale di cambiamento della quantità
 monitorata per determinare se c'è stato un miglioramento o peggioramento
 ed infine un parametro patience che determina dopo quante epoche senza
 miglioramenti fermare l'allenamento.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename earlystopping.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Early stopping
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Regolarizzazione loss function
\end_layout

\begin_layout Standard
Le reti neurali, durante il loro processo di apprendimento, sfruttano, come
 abbiamo visto, la funzione costo per decidere come sistemare i propri parametri
, ovvero pesi e bias.
 Abbiamo visto che c'è anche un grande problema, che è quello dell'overfitting.
 Sono state sviluppate alcune tecniche che, operando sulla funzione costo,
 aiutano a ridurre gli effetti del sovraallenamento di una rete neurale.
 Tali tecniche prendono il nome di tecniche di 
\series bold
regolarizzazione
\series default
.
 Generalmente, tali tecniche prevedono l'aggiunta di un fattore, dipendente
 dai pesi, dopo l'espressione della funzione costo.
 Tale fattore ha lo scopo di introdurre un termine di penalità sulla norma
 di 
\begin_inset Formula $w$
\end_inset

 che ha l’effetto di restringere l’insieme entro cui vengono scelti i parametri.
 Ciò equivale, essenzialmente, ad imporre condizioni di regolarità sulla
 classe di funzioni realizzata dalla rete.
 Sia 
\begin_inset Formula $E_{p}(w)$
\end_inset

 la funzione costo (o loss function, error function) non regolarizzata,
 la funzione costo regolarizzata assume la forma:
\begin_inset Formula 
\begin{equation}
E(w)=E_{p}(w)+\gamma||w||
\end{equation}

\end_inset

dove 
\begin_inset Formula $||\bullet||$
\end_inset

 denota la norma euclidea 
\begin_inset Formula $L_{2}$
\end_inset

.
 Per completezza aggiungiamo che è possibile usare anche altri tipi di norma,
 ma comunemente le più usate sono la norma 
\begin_inset Formula $L_{1},L_{2}$
\end_inset

.
 Il termine 
\begin_inset Formula $\gamma>0$
\end_inset

 è anch'esso un iperparametro.
 La regolarizzazione può essere vista come un 
\shape italic
compromesso
\shape default
 tra trovare pesi piccoli e minimizzare la funzione costo.
 Il parametro 
\begin_inset Formula $\gamma$
\end_inset

 viene detto 
\series bold
tasso di regolarizzazione
\series default
 e serve per determinare il bilanciamento di tale compromesso: quando 
\begin_inset Formula $\gamma$
\end_inset

 è piccolo, allora preferiamo minimizzare la funzione costo, mentre quando
 è grande, cerchiamo di trovare pesi piccoli.
 La regolarizzazione aiuta le reti neurali a generalizzare meglio, in quanto
 una rete con pesi piccoli non varia il proprio comportamento se cambiano
 alcuni dei dati di input.
 Questo le rende particolarmente difficile memorizzare le peculiarità dei
 dati, mentre la aiuta ad apprendere meglio quelli che sono i modelli e
 gli schemi dei dati di allenamento.
 Il principale problema della regolarizzazione è che non si è ancora capito
 esattamente il perché essa aiuti a migliorare le prestazioni di una rete
 neurale, ma abbiamo a disposizione solo evidenze pratiche di questo fatto.
 Nonostante questo, la regolarizzazione è ampiamente utilizzata e ci aiuta
 a migliorare le prestazioni delle nostre reti neurali.
\end_layout

\begin_layout Subsection
Dropout
\end_layout

\begin_layout Standard
La tecnica di 
\series bold
dropout
\series default
 invece funziona diversamente, in quanto modifica non la funzione di costo
 della rete, ma la rete stessa.
 Abbiamo visto il principio di funzionamento di una rete neurale e come
 essa riesca ad allenarsi.
 Ecco, questa tecnica prevede di applicare il solito procedimento togliendo
 prima una certa percentuale di neuroni in ogni hidden layer! Per ogni epoca
 di allenamento si sceglie casualmente con una data probabilità (iperparametro)
 quali neuroni tenere e quali scartare e si allena la rete così ottenuta.
 Si ripete quindi il procedimento, tenendo e scartando neuroni diversi ad
 ogni epoca: una volta che si ritiene che la rete sia pronta, si prende
 la rete originale e si aggiustano i pesi uscenti dai neuroni nascosti:
 abbiamo ottenuto una rete pronta a svolgere il proprio compito.
 In poche parole, è come se usassimo tante reti diverse e poi prendessimo
 come risultato la media di tutti i risultati di queste reti.
 Va tenuto ben presente che questo procedimento è applicato solo in fase
 di allenamento: durante il funzionamento vero e proprio, la rete è considerata
 nella sua interezza.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Droout_.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Dropout: ad ogni epoca, poi, faremo in modo di avere una rete diversa ogni
 volta, considerando neuroni diversi.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Inizializzazione dei pesi
\end_layout

\begin_layout Standard
Prima di partire con l'allenamento i pesi vanno inizializzati.
 Osserviamo che non sappiamo quale sarà il valore finale di ogni peso al
 termine dell'allenamento, ma se ad esempio normalizziamo i dati di input
 è ragionevole supporre che circa la metà dei pesi sarà positiva e la metà
 negativa, quindi con media tendente a zero.
 
\end_layout

\begin_layout Itemize
Una prima idea di 
\series bold
inizializzazione
\series default
 è quella di porre tutti i pesi a 
\series bold
zero
\series default
.
 Pessima idea, infatti avendo tutti i pesi uguali, ogni neurone calcolerà
 lo stesso output e lo stesso gradiente, quindi i pesi subiranno tutti gli
 stessi aggiornamenti e i pesi finali resteranno tutti uguali tra loro.
 Questo è un problema, supponiamo che la nostra rete sia un classificatore
 di immagini, avere tutti i pesi uguali significa che ogni pixel di ogni
 immagine abbia la stessa importanza degli altri, quindi la rete non impara
 ad estrarre e riconoscere nessuna feature.
\end_layout

\begin_layout Itemize

\series bold
Inizializzazione con numeri
\series default
 grandi: anche questa è una pessima idea, a seconda della funzione di attivazion
e del neurone abbiamo diversi comportamenti ma tutti problematici.
 Come detto prima, i pesi grandi peggiorano la capacità di generalizzazione
 della rete, inoltre se usassimo funzioni di attivazione come softmax o
 tangente iperbolica potremmo essere fin da subito nelle zone di saturazione,
 zone in cui il gradiente tende a zero bloccando di fatto l'apprendimento.
\end_layout

\begin_layout Itemize

\series bold
Inizializzare i pesi intorno allo zero
\series default
: abbandonata l'idea di inizializzare con numeri grandi, vogliamo che i
 pesi siano molto vicini allo zero, non identicamente nulli e distribuiti
 in modo che circa la metà sia positiva e metà sia negativa.
 Una pratica comune è inizializzare i pesi campionandoli da una distribuzione
 gaussiana a media nulla e varianza piccola (0.01).
 Questo tipo di inizializzazione può andare bene se la rete neurale dispone
 di pochi strati nascosti, ma in reti neurali più profonde le attivazioni
 diventano sempre più piccole mano a mano che si processano i vari strati,
 fino ridursi a quantità praticamente nulle.
 Questo diventa un problema in quanto durante la fase di back propagation
 il gradiente accumulato continua ad essere moltiplicato per quantità piccolissi
me che portano alla sua dissolvenza.
\end_layout

\begin_layout Itemize

\series bold
Inizializzazione di Xavier
\series default
: l'idea è quindi quella di avere una distribuzione delle attivazioni tale
 che la rete neurale sia in grado di apprendere in maniera efficiente.
 In quest'ottica, Xavier (2010) ha proposto una inizializzazione dei pesi
 secondo una normale a media nulla con deviazione standard tale che la varianza
 delle attivazioni 
\begin_inset Formula $a^{(k)}$
\end_inset

 risulti essere unitaria.
 Sotto l'assunzione di attivazioni lineari e simmetriche (plausibile dal
 momento che anche la tangente iperbolica ha proprio questo comportamento
 intorno allo zero) questo si traduce nel rendere unitaria la varianza degli
 input 
\begin_inset Formula $z^{(k)}$
\end_inset

.
 L'inizializzazione di Xavier va eseguita neurone per neurone a partire
 dallo strato iniziale, per questo è necessario preprocessare l'input della
 rete come descritto sopra (input a media nulla e varianza unitaria) quando
 vogliamo utilizzare questa inizializzazione.
 Supponiamo di avere un input 
\begin_inset Formula $X$
\end_inset

 di dimensione 
\begin_inset Formula $n$
\end_inset

 e lo strato, composto da 
\begin_inset Formula $n$
\end_inset

 neuroni sia solamente connesso tramite pesi casuali 
\begin_inset Formula $W$
\end_inset

 di dimensione 
\begin_inset Formula $n\times n$
\end_inset

, l'output 
\begin_inset Formula $Y$
\end_inset

 dell'i-esimo neurone, di dimensione 
\begin_inset Formula $n$
\end_inset

 anch'esso sarà:
\begin_inset Formula 
\begin{equation}
Y_{i}=X_{1}W_{i,1}+X_{2}W_{i,2}+...+X_{n}W_{i,n}
\end{equation}

\end_inset

dove 
\begin_inset Formula $i\in[1,n]$
\end_inset

 e indica l'i-esimo neurone e 
\begin_inset Formula $W_{i}$
\end_inset

 il vettore dei pesi associati.
 La varianza del prodotto di due variabili aleatorie è:
\begin_inset Formula 
\[
Var(W_{i}X_{i})=E[X_{i}]^{2}Var(W_{i})+E[W_{i}]^{2}Var(X_{i})+Var(W_{i})Var(X_{i})
\]

\end_inset

Per ipotesi sia l'input sia i pesi sono a media nulla, semplificando:
\begin_inset Formula 
\[
Var(W_{i}X_{i})=Var(W_{i})Var(X_{i})
\]

\end_inset

Inoltre assumiamo che 
\begin_inset Formula $X_{i},W_{i}$
\end_inset

 siano indipendenti ed identicamente distribuite (iid), allora:
\begin_inset Formula 
\[
Var(Y_{i})=Var(X_{1}W_{i,1}+X_{2}W_{i,2}+...+X_{n}W_{i,n})
\]

\end_inset


\begin_inset Formula 
\begin{equation}
=nVar(W_{i})Var(X_{i})
\end{equation}

\end_inset

Assumento 
\begin_inset Formula $Var(X_{1})=1$
\end_inset

 perché l'input può essere o i dati di allenamento già normalizzati o essere
 l'output dello strato precedente in cui i pesi sono già stati inizializzati
 correttamente.
 Imponiamo quindi:
\begin_inset Formula 
\[
nVar(W_{i})=1
\]

\end_inset


\begin_inset Formula 
\[
Var(W_{i})=\frac{1}{n}=\frac{1}{n_{in,i}}
\]

\end_inset

Quindi in definitiva Xavier propone di inizializzare i pesi campionandoli
 nel modo seguente:
\begin_inset Formula 
\begin{equation}
W_{i}\sim N(0,\frac{1}{n_{in,i}})
\end{equation}

\end_inset

Fino ad adesso abbiamo assunto l'ipotesi che la funzione di attivazione
 sia lineare e simmetrica intorno allo 0.
 Usando la funzione ReLU cade l'ipotesi di simmetria e la distribuzione
 va adattata.
 A tale scopo recentemente è stata proposta l'inizializzazione: 
\begin_inset Formula 
\begin{equation}
W_{i}\sim N(0,\frac{2}{n_{in,i}})
\end{equation}

\end_inset

Il fattore di correzione 2 ha senso: la ReLU dimezza di fatto gli input,
 e pertanto bisogna raddoppiare la varianza dei pesi per mantenere la stessa
 varianza delle attivazioni.
\end_layout

\begin_layout Itemize

\series bold
Glorot & Bengio
\series default
 seguendo un simile ragionamento a quello di Xavier ma applicato alla backpropag
ation, quindi procedendo a ritroso dall'ultimo strato al primo, invertendo
 anche il ruolo di input e output, hanno trovato che deve valere:
\begin_inset Formula 
\[
n_{in,i}Var(W_{i})=1
\]

\end_inset


\begin_inset Formula 
\[
n_{out,i}Var(W_{i})=1
\]

\end_inset

Imporre le due condizioni simultaneamente risulta essere troppo restrittivo,
 imporrebbe infatti 
\begin_inset Formula $n_{in,i}=n_{out,i}$
\end_inset

 che nel caso di una rete feedforward totalmente connessa significa che
 tutti gli strati hanno la medesima ampiezza e anche in altre tipologie
 di reti è una forte imposizione sull'architettura.
 Si è raggiunto un compromesso tra i due vincoli usando una distribuzione
\begin_inset Formula 
\begin{equation}
W_{i}\sim N(0,\frac{2}{n_{in,i}+n_{out,i}})
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Data augmentation
\end_layout

\begin_layout Standard
Si può anche pensare di 
\series bold
ampliare
\series default
 artificialmente
\series bold
 i dati di allenamento
\series default
, in quanto ottenere nuovi dati per allenare la rete è sempre una buona
 idea.
 Il problema è che non sempre è possibile, oppure è troppo costoso ottenerne
 di nuovi.
 Quindi se ne generano di nuovi a partire da quelli che abbiamo già a disposizio
ne.
 Ad esempio, se la nostra rete dovesse riconoscere delle cifre scritte a
 mano, potremmo applicare delle piccole rotazioni o delle lievi dilatazioni
 o restrizioni ai dati che abbiamo già in possesso, creando delle immagini
 nuove da fornire alla nostra rete neurale.
 In generale, si cerca di espandere il set di allenamento cercando di riprodurre
 quelle che sono le variazioni che di solito hanno nella pratica.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Data_aug.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Data augmentation: Nonostante la differenza sia minima, per l'analisi svolta
 dalla rete sono due immagini significativamente diverse.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Scelte di non linearità
\end_layout

\begin_layout Standard
Possiamo ora analizzare più nel dettaglio funzioni di attivazioni accennate
 nel capitolo 1 avendo ora maggiori conoscienze sull'ottimizzazione.
\end_layout

\begin_layout Subsection
Funzione Sigmoide
\end_layout

\begin_layout Standard
Tale soluzione è stata progressivamente accantonata negli ultimi anni per
 via di alcune problematiche che comporta a livello pratico.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename grafico_sig.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Grafico Sigmoide
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Formula 
\[
f(x)=\frac{1}{1+e^{-x}}
\]

\end_inset

La prima e più importante è quella relativa alla dissolvenza del gradiente
 in seguito alla 
\shape italic
saturazione
\shape default
 dei neuroni, ossia quei neuroni che presentano valori di output agli estremi
 del codominio della funzione di attivazione, in questo caso 
\begin_inset Formula $(0,1)$
\end_inset

.
 É facile infatti notare che
\begin_inset Formula 
\[
\lim_{x\rightarrow+\infty}f(x)=1
\]

\end_inset


\begin_inset Formula 
\[
\lim_{x\rightarrow-\infty}f(x)=0
\]

\end_inset

Tale saturazione diventa problematica durante le fase di back propagation,
 in quanto il gradiente locale assume valori prossimi allo zero, che per
 la 
\shape italic
regola della catena
\shape default
 vanno a moltiplicare tutti i gradienti calcolati in precedenza e quelli
 successivi, conducendo così all'annullamento del gradiente globale.
\begin_inset Formula 
\[
f'(x)=f(x)(1-f(x))
\]

\end_inset


\begin_inset Formula 
\[
\lim_{x\rightarrow+\infty}f'(x)=0
\]

\end_inset


\begin_inset Formula 
\[
\lim_{x\rightarrow-\infty}f'(x)=0
\]

\end_inset

In pratica, si ha un flusso utile del gradiente solo per valori di input
 che rimangono all'interno di una zona di sicurezza, cioè nei dintorni dello
 zero.
 Il secondo problema deriva invece dal fatto che gli output della funzione
 sigmoide non sono centrati intorno allo zero, e di conseguenza gli strati
 processati successivamente riceveranno anch'essi valori con una distribuzione
 non centrata sullo zero.
 Questo influisce in maniera significativa sulla discesa del gradiente,
 in quanto gli input in ingresso ai neuroni saranno sempre positivi, e pertanto
 il gradiente dei pesi associati diventerà, durante la fase di back propagation,
 sempre positivo o sempre negativo.
 Tale risultato si traduce in una dinamica a zig-zag negli aggiornamenti
 dei pesi che rallenta in maniera signicativa il processo di convergenza.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename zigzag gradiente.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Esempio di dinamica a zig-zag nell'aggiornamento di due pesi 
\begin_inset Formula $W_{1}$
\end_inset

 e 
\begin_inset Formula $W_{2}$
\end_inset

.
 La freccia blu indica indica l'ipotetico vettore ottimale per la discesa
 del gradiente, mentre le frecce rosse i passi di aggiornamento compiuti:
 gradienti tutti dello stesso segno comportano due sole possibili direzioni
 di aggiornamento.
\end_layout

\end_inset


\end_layout

\end_inset

É importante comunque notare che una volta che i gradienti delle singole
 osservazioni vengono sommati all'interno dello stesso batch di dati l'aggiornam
ento finale dei pesi può avere segni diversi, permettendo quindi di muoversi
 lungo un insieme più ampio di direzioni.
 
\end_layout

\begin_layout Standard
Il terzo e ultimo difetto della funzione sigmoide è che l'operazione exp(·)
 al denominatore è molto costosa dal punto dal punto di vista computazionale,
 soprattutto rispetto alle alternative che verranno presentate di seguito.
\end_layout

\begin_layout Subsection
Tangente iperbolica
\end_layout

\begin_layout Standard
Il problema degli output non centrati sullo zero della sigmoide può essere
 risolto ricorrendo all'utilizzo della tangente iperbolica, la quale presenta
 codominio (−1, 1) centrato sull'origine degli assi.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename tangente-iperbolica.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Grafico tangente iperbolica
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\[
f(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
\]

\end_inset

Tuttavia, rimane il problema della 
\shape italic
saturazione
\shape default
 dei neuroni, anzi viene addirittura accentuato, dal momento che la zona
 di sicurezza risulta ancora più ristretta.
 Rimane anche il problema della complessità computazionale della funzione
 esponenziale.
\end_layout

\begin_layout Subsection
ReLU
\end_layout

\begin_layout Standard
La 
\series bold
Rectifier Linear Unit
\series default
 (ReLU) è diventata popolare negli ultimi anni per via dell'incremento prestazio
nale che offre nel processo di convergenza: velocizza infatti di circa 6
 volte la discesa del gradiente rispetto alle alternative viste finora.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Grafico_relu.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Grafico ReLU
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Formula 
\[
f(x)=x^{+}=max(0,x)
\]

\end_inset

Questo risultato è da attribuire in larga parte al fatto che la ReLU risolve
 il 
\shape italic
problema della dissolvenza
\shape default
 del gradiente, non andando a saturare i neuroni.
 Durante la fase di back propagation infatti, se il gradiente calcolato
 fino a quel punto è positivo questo viene semplicemente lasciato passare,
 perchè la derivata locale per il quale viene moltiplicato è pari ad uno.
\begin_inset Formula 
\[
f'(x)=\begin{cases}
1 & x>0\\
0 & x<0
\end{cases}
\]

\end_inset


\series bold
NB
\series default
 durante la back propagation l'input 
\begin_inset Formula $x$
\end_inset

 è il gradiente proveniente dagli strati successivi.
\end_layout

\begin_layout Standard
Eventuali problemi sorgono invece quando il gradiente accumulato ha segno
 negativo, in quanto questo viene azzerato (la derivata locale è nulla lungo
 tutto il semiasse negativo) con la conseguenza che i pesi non vengono aggiornat
i.
 Fortunatamente questo problema può essere alleviato attraverso l'utilizzo
 di un algoritmo SGD: considerando più dati alla volta c'è infatti la speranza
 che non tutti gli input del batch provochino l'azzeramento del gradiente,
 tenendo così in vita il processo di apprendimento del neurone.
 Al contrario, se per ogni osservazione la ReLU riceve valori negativi,
 allora il neurone "muore", e non c'è speranza che i pesi vengano aggiornati.
 Valori elevati del learning rate amplificano questo problema, dal momento
 che cambiamenti più consistenti dei pesi si traducono in una maggiore probabili
tà che questi affondino nella "zona morta".
 
\end_layout

\begin_layout Subsection
Leaky ReLU
\end_layout

\begin_layout Standard
La 
\series bold
leaky ReLU
\series default
 è un tentativo di risolvere il problema della disattivazione dei neuroni
 comportato dalla ReLU classica, e consiste nell'introdurre una piccola
 
\series bold
pendenza negativa
\series default
 (di circa 0.01) 
\begin_inset Formula $\alpha$
\end_inset

 nella regione dove la ReLU è nulla, dove 
\begin_inset Formula $\alpha$
\end_inset

 è costante.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Leaky_relu.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Grafico Leaky ReLU
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\[
f(x)=\max(\alpha x,x)
\]

\end_inset

In alcune varianti, α può essere un parametro da stimare, al pari degli
 altri pesi della rete (si parla di 
\series bold
Parametric ReLU
\series default
 ), oppure una variabile casuale: è il caso della 
\series bold
Randomized ReLU
\series default
, dove ad ogni iterazione la pendenza della parte negativa della funzione
 viene scelta casualmente all'interno di un range pressato.
 In alcuni recenti esperimenti, Bing Xu et al.
 (2015) hanno mostrato come le varianti della ReLU classica siano in grado
 di aumentare le performance finali del modello in termini di accuratezza,
 prima su tutte la RReLU, che grazie alla sua natura casuale sembra particolarme
nte portata alla riduzione del sovradattamento.
\end_layout

\begin_layout Chapter
Classificazioni di immagini
\end_layout

\begin_layout Standard
La visione artificiale (nota anche come 
\series bold
computer vision
\series default
) è l'insieme dei processi che mirano a creare un modello approssimato del
 mondo reale (3D) partendo da immagini bidimensionali (2D).
 Vedere è inteso non solo come l'acquisizione di una fotografia bidimensionale
 di un'area ma soprattutto come l'interpretazione del contenuto di quell'area.
 L'informazione è intesa in questo caso come qualcosa che implica una decisione
 automatica.
 Un problema classico nella visione artificiale è quello di determinare
 se l'immagine contiene o no determinati oggetti (Object recognition) o
 attività.
 Il problema può essere risolto efficacemente e senza difficoltà per oggetti
 specifici in situazioni specifiche per esempio il riconoscimento di specifici
 oggetti geometrici come poliedri, riconoscimento di volti o caratteri scritti
 a mano.
 Le cose si complicano nel caso di oggetti arbitrari in situazioni arbitrarie.
 
\end_layout

\begin_layout Standard
Nella letteratura troviamo differenti varietà del problema:
\end_layout

\begin_layout Itemize

\series bold
Recognition
\series default
 (riconoscimento): uno o più oggetti prespecificati o memorizzati possono
 essere ricondotti a classi generiche usualmente insieme alla loro posizione
 2D o 3D nella scena.
\end_layout

\begin_layout Itemize

\series bold
Identification
\series default
 (identificazione): viene individuata un'istanza specifica di una classe.
 Es.
 Identificazione di un volto, impronta digitale o veicolo specifico.
\end_layout

\begin_layout Itemize

\series bold
Detection
\series default
 (rilevamento): l'immagine è scandita fino all'individuazione di una condizione
 specifica.
 Es.
 Individuazione di possibili cellule anormali o tessuti nelle immagini mediche.
\end_layout

\begin_layout Standard
Altro compito tipico è la ricostruzione dello scenario: dati 2 o più immagini
 2D si tenta di ricostruire un modello 3D dello scenario.
 Nel caso più semplice si parla di un insieme di singoli punti in uno spazio
 3D o intere superfici.
 Generalmente è importante trovare la matrice fondamentale che rappresenta
 i punti comuni provenienti da immagini differenti.
\end_layout

\begin_layout Standard
Il problema della 
\series bold
classificazione di immagini
\series default
 è il compito di assegnare ad un'immagine di input una e una sola etichetta
 proveniente da un insieme fissato di output.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename esempio_classificazione.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Classificazione di un'immagine
\end_layout

\end_inset


\end_layout

\end_inset

La classificazione presenta alcuni problemi:
\end_layout

\begin_layout Itemize
Variazione punto di vista (
\shape italic
Viewpoint variation
\shape default
): una singola istanza di un oggetto può essere orientata in modi diversi
 rispetto alla camera, producendo immagini diverse;
\end_layout

\begin_layout Itemize
Variazione scala (
\shape italic
Scale variation
\shape default
): oggetti diversi appartenenti alla medesima classe possono differire nelle
 loro dimensioni reali;
\end_layout

\begin_layout Itemize
Deformazione (
\shape italic
Deformation
\shape default
): alcuni oggetti non sono corpi rigidi e possono apparire deformati in
 modi diversi;
\end_layout

\begin_layout Itemize
Occlusione (
\shape italic
Occlusion
\shape default
): parti dell'oggetto da riconoscere è nascosto e non visibile;
\end_layout

\begin_layout Itemize
Condizioni di illuminazione (
\shape italic
Illumination conditions
\shape default
): l'illuminazione ha un ruolo decisivo nell'informazione codificata all'interno
 dei pixel che compongono l'immagine;
\end_layout

\begin_layout Itemize
Disordine di sfondo (
\shape italic
Background clutter
\shape default
): gli oggetti di interesse possono mescolarsi e confondersi nell'ambiente
 circostante, rendendo difficile l'identificazione;
\end_layout

\begin_layout Itemize
Variazione intra-classe (
\shape italic
Intra-class variation
\shape default
): oggetti appartenti alla stessa classe possono differire significativamente
 l'uno dall'altro.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Problem_image_class.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Problemi nella classificazione di immagini
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Dataset CIFAR-10
\end_layout

\begin_layout Standard
In queste note si farà riferimento al dataset
\series bold
 CIFAR-10
\series default
 che consiste in 60000 immagini di dimensione 
\begin_inset Formula $32\times32$
\end_inset

 pixels, ogni pixel ha associato 3 numeri, uno per ogni colore (RGB).
 Ogni immagine è etichettata con una di 10 classi, Queste 60000 immagini
 sono partizionate in 
\shape italic
training set
\shape default
 di 50000 immagini e 
\shape italic
test set
\shape default
 di 10000 immagini.
 Ogni immagine può quindi essere vista come un vettore appartenente allo
 spazio 
\begin_inset Formula $\mathbb{R}^{32\times32\times3}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename cifar10.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
CIFAR-10 daset
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Nearest Neighbor Classifier
\end_layout

\begin_layout Standard
Questo classificatore non ha nulla a che fare con le reti neurali e non
 viene quasi mai usato nella pratica, ma ci permetterà di avere un'idea
 dell'approccio di base a un problema di classificazione delle immagini.
 L'idea di questo classificatore è molto semplice, la fase di training consiste
 nel memorizzare tutte le immagini del training set, la classificazione
 avviene confrontando l'immagine di test con ogni immagine del training
 set, quindi si etichetta l'immagine in accordo con l'etichetta dell'immagine
 che più assomiglia.
 Cosa intendiamo quando diciamo 
\begin_inset Quotes qld
\end_inset


\shape italic
che più assomiglia
\shape default

\begin_inset Quotes qrd
\end_inset

? Occorre quindi formalizzare questo concetto secondo una qualche metrica.
 Ogni immagine può essere vista come una matrice in cui ogni elemento è
 un valore numerico che rappresenta l'intensità di un colore appartenente
 allo spazio RGB di un singolo pixel.
 Definiamo quindi tre distanze:
\end_layout

\begin_layout Enumerate

\series bold
L1 distance: 
\begin_inset Formula 
\begin{equation}
d_{1}\left(I_{1},I_{2}\right)=\sum_{p}\mid I_{1}^{p}-I_{2}^{p}\mid
\end{equation}

\end_inset


\series default
la distanza è calcolata sommando il modulo delle differenze elemento per
 elemento.
 è anche nota come distanza di Manhattan.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename L1Distance.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
L1 distance
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Enumerate

\series bold
L2 distance:
\series default

\begin_inset Formula 
\begin{equation}
d_{2}\left(I_{1},I_{2}\right)=\sqrt{\sum_{p}\left(I_{1}^{p}-I_{2}^{p}\right)^{2}}
\end{equation}

\end_inset

 è la distanza euclidea classica.
\end_layout

\begin_layout Enumerate

\series bold
L
\begin_inset Formula $_{k}$
\end_inset

 distance:
\series default

\begin_inset Formula 
\begin{equation}
d_{k}\left(I_{1},I_{2}\right)=\left(\sum_{p}\left(\mid I_{1}-I_{2}\mid\right)^{k}\right)^{\frac{1}{k}}
\end{equation}

\end_inset

con 
\begin_inset Formula $k\in[1,+\infty).$
\end_inset

 E' una generalizzazione delle precedenti.
\end_layout

\begin_layout Standard
Le distanze comunemente usate sono le prime due, L1 e L2.
 In altre parole 
\series bold
Nearest Neihbor Classifier
\series default
 è ricondotto ad un problema di minimizzazione riformulando il problema
 di classificazione come segue:
\end_layout

\begin_layout Itemize
chiamiamo 
\begin_inset Formula $\boldsymbol{x}_{i}$
\end_inset

 l'i-esima immagine di test (da etichettare);
\end_layout

\begin_layout Itemize
chiamiamo 
\begin_inset Formula $\boldsymbol{x}_{j}$
\end_inset

 la j-esima immagine del training set (già etichettata);
\end_layout

\begin_layout Itemize
chiamiamo 
\begin_inset Formula $y_{j}$
\end_inset

 l'etichetta della j-esima immagine del training set;
\end_layout

\begin_layout Itemize
poniamo 
\begin_inset Formula $y_{i}=y_{j^{*}}$
\end_inset

 con 
\begin_inset Formula $j^{*}=\mathrm{argmin}\left(d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Da test effettuati risulta che Nearest Neihbor Classifier, usando la distanza
 L1, ha classificato correttamente il 38,6% delle immagini del dataset CIFAR-10.
 Un risultato ragguardevole se comparato alla probabilità di una corretta
 classificazione assegnando casualmente un'etichetta (10% nel nostro caso)
 ma ben lontano dalle prestazioni umane e dalle migliori reti neurali convuluzio
nali.
 Utilizzando la distanza L2 invece si è ottenuto un'accuratezza del 35,4%.
 
\end_layout

\begin_layout Section
K-Nearest Neighbor Classifier
\end_layout

\begin_layout Standard
Questo classificatore è un estensione del precedente e si basa su un'idea
 molto semplice: invece di assegnare l'etichetta dell'immagine più vicina
 (rispetto ad una definita distanza), troviamo le 
\series bold
k immagini più vicine
\series default
 e assegnamo l'etichetta che compare maggiormente nelle 
\shape italic
k
\shape default
 etichette.
 In particolare per 
\begin_inset Formula $k=1$
\end_inset

 otteniamo 
\shape italic
Nearest Neihbor Classifier
\shape default
 precedente.
 Intuituivamente un alto valore di 
\shape italic
k 
\shape default
ha un effetto 
\begin_inset Quotes qld
\end_inset

levigante
\begin_inset Quotes qrd
\end_inset

 sui confini decisionali e rende il classificatore più resistente ai valori
 anomali.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename k-NN.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
confronto NN classifier e 5-NN classifier
\end_layout

\end_inset


\end_layout

\end_inset

 La figura 2.5 mostra un confronto tra K-NN e NN, usando punti bidimensionali
 come dati da classificare in 3 classi (rosso, blue, verde).
 Le regioni colorate evidenziano i confini decisionali.
 Le regioni bianche mostrano punti la cui classificazione è ambigua (per
 esempio in K-NN nel caso in cui ci sia parità tra due o più classi).
 Notiamo come nel caso di punti anomali, NN crea piccole isole di probabili
 previsioni errate, mentre 5-NN smussa queste irregolarità.
\end_layout

\begin_layout Paragraph
Vantaggi e svantaggi K-NN
\end_layout

\begin_layout Standard
Tra i vantaggi si sottolinea:
\end_layout

\begin_layout Itemize
Facile da capire e implementare
\end_layout

\begin_layout Itemize
Training in tempo costante 
\begin_inset Formula $O(1)$
\end_inset

 difatti basta salvare il riferimento al training set.
\end_layout

\begin_layout Standard
Tra gli svantaggi si sottolinea:
\end_layout

\begin_layout Itemize
Complessità temporale: richiede il confronto di ogni immagine di test con
 tutte quelle appartenti al trainin set.
 La complessità dipende linearmente dalla dimensione del training set e
 test set;
\end_layout

\begin_layout Itemize
Elevata complessità spaziale: richiede che tutto il training set sia memorizzato.
\end_layout

\begin_layout Itemize
La distanza tra immagini non è sempre signicativa come mostrato dalla seguente
 immagine.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename distance_image.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Distanza tra immagini, le 3 immagini a sinistra hanno tutte la stessa distanza
 dall'originale
\end_layout

\end_inset


\end_layout

\end_inset

si pone ora il problema di come scegliere il valore k, come scegliere il
 tipo di distanza da usare.
 Soluzione 
\series bold
cross-validation.
 
\end_layout

\begin_layout Section
Classificatore Lineare (Linear Classifier)
\end_layout

\begin_layout Standard
Un 
\series bold
classificatore
\series default
 può anche essere visto come una funzione che associa ad un'immagine 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 un vettore le cui componenti sono il punteggio associato ad ogni classe.
 Intuitivamente un buon classificatore associa alla classe corretta un punteggio
 più alto rispetto alle classi incorrette.
 Più formalmente:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathcal{\boldsymbol{F}:\mathbb{\mathbb{\mathbb{R^{\mathrm{\mathit{d}}}}}}\rightarrow}\mathbb{R}^{L}
\end{equation}

\end_inset

 dove 
\begin_inset Formula $d$
\end_inset

 è la dimensione di una immagine (
\begin_inset Formula $32\times32\times3$
\end_inset

 nel caso CIFAR-10), 
\begin_inset Formula $L$
\end_inset

 è la cardinalità dell'insieme delle etichette.
 Quindi 
\begin_inset Formula $\boldsymbol{F}\left(\boldsymbol{\text{𝑥}}\right)$
\end_inset

 è un vettore 
\begin_inset Formula $L$
\end_inset

- dimensionale e la 
\begin_inset Formula $i$
\end_inset

-esima componente 
\begin_inset Formula $s_{i}=\left[\boldsymbol{F}\left(\boldsymbol{\text{𝑥}}\right)\right]_{i}$
\end_inset

 contiente il punteggio di quanto probabile sia l'appartenenza di 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 alla classe 
\begin_inset Formula $i$
\end_inset

.
 Per il momento non abbiamo detto nulla su come è fatta la funzione vettoriale
 
\begin_inset Formula $\mathcal{\boldsymbol{F}:\mathbb{\mathbb{\mathbb{R^{\mathrm{\mathit{d}}}}}}\rightarrow}\mathbb{R}^{L}$
\end_inset

.
 Nel classificatore lineare 
\begin_inset Formula $\boldsymbol{F}$
\end_inset

 è una funzione lineare:
\begin_inset Formula 
\begin{equation}
\boldsymbol{F}\left(\boldsymbol{\text{𝑥}\mid}W,b\right)=W\boldsymbol{x}+b
\end{equation}

\end_inset

dove 
\begin_inset Formula $W\in\mathbb{\mathbb{R}}^{L\times d}$
\end_inset

 è chiamata 
\shape italic
matrice dei pesi
\shape default
, 
\begin_inset Formula $b\in\mathbb{R}^{L}$
\end_inset

è chiamato 
\shape italic
bias
\shape default
 e sono entrambi parametri.
 Prima di essere classificata un'immagine necessita di una pre elaborazione,
 denominata 
\shape italic
unroll
\shape default
 che consiste nel espandere la matrice di pixel 
\begin_inset Formula $I\in\mathbb{R}^{h\times w}$
\end_inset

 in un vettore 
\begin_inset Formula $\boldsymbol{x}\in\mathbb{R}^{(h\cdot w\cdot3)}$
\end_inset

 in cui ogni componente del vettore rappresenta l'intensità di uno specifico
 colore RGB di un singolo pixel.
 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Unrollpicture.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Unroll immagine
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename linear_class.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Linear Classifier
\end_layout

\end_inset


\end_layout

\end_inset

Il 
\series bold
classificatore lineare
\series default
 assegna ad un'immagine di input la classe corrispondente al più alto punteggio:
 
\begin_inset Formula 
\begin{equation}
\hat{y_{j}}=arg\max_{i=1,...,L}[s_{j}]_{i}.
\end{equation}

\end_inset

 Nel caso CIFAR-10 ogni immagine viene trasformata in un vettore di 
\begin_inset Formula $[3072\times1]$
\end_inset

, la matrice 
\begin_inset Formula $W$
\end_inset

 ha una dimensione di 
\begin_inset Formula $[10\times3072]$
\end_inset

, 
\begin_inset Formula $b$
\end_inset

 ha dimensione 
\begin_inset Formula $[10\times1]$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Notiamo che una singola moltiplicazione 
\begin_inset Formula $W\boldsymbol{x}_{i}$
\end_inset

 corrisponde (nel caso specifico) a 10 classificazioni parallele, in cui
 ogni riga della matrice 
\begin_inset Formula $W$
\end_inset

 corrisponde a un classificatore, quindi all'importanza che ogni pixel possiede
 per la i-esima classe.
 
\end_layout

\begin_layout Itemize
Durante la fase di training abbiamo i dati di input 
\begin_inset Formula $\left(\boldsymbol{x}_{i},y_{i}\right)$
\end_inset

 già classificati e fissati, ma noi abbiamo il controllo sui parametri 
\begin_inset Formula $W,b$
\end_inset

 e il nostro obiettivo è trovare quei lavori che massimizzano la classificazione
 corretta.
\end_layout

\begin_layout Itemize
Rispetto a Nearest Neihbor Classifier una volta terminata la fase di training
 il set di immagini di training può essere eliminato, è necessario solo
 memorizzare i parametri 
\begin_inset Formula $W,b.$
\end_inset


\end_layout

\begin_layout Itemize
Inoltre questo classificatore lineare coinvolge solo un prodotto matriciale
 che è molto più veloce del confronto di un'immagine con tutto il training
 set.
 
\end_layout

\begin_layout Paragraph
Loss Function
\end_layout

\begin_layout Standard
Dopo aver visto formalmente cosa è un Linear Classifier, cioè una funzione
 lineare 
\begin_inset Formula $\boldsymbol{F}\left(\boldsymbol{\text{𝑥}\mid}W,b\right)=W\boldsymbol{x}+b$
\end_inset

, ci poniamo ora il problema di trovare i giusti parametri 
\begin_inset Formula $W,b$
\end_inset

 che rendano consistente e migliore la classificazione.
 Per fare ciò introduciamo una funzione in grado di fornirci una misura
 di quanto la nostra classificazione sia 
\begin_inset Quotes fld
\end_inset

infelice
\begin_inset Quotes frd
\end_inset

 (unhappiness), cioè lontana da un risultato corretto.
 Si tratterà quindi di una funzione dell'input 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 e parametrizzata dai parametri 
\begin_inset Formula $W,b$
\end_inset

 che andrà minimizzata.
 In letteratura è nota come 
\series bold
loss function
\series default
 (o altre volte 
\series bold
cost function
\series default
).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathcal{L}\left(\boldsymbol{x},y_{i}|W,b\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Bias trick
\end_layout

\begin_layout Standard
Modifichiamo ora la score function alleggerendo la notazione eliminando
 il termine 
\begin_inset Formula $\boldsymbol{b}$
\end_inset

.
 Richiamiamo la (9) 
\begin_inset Formula $\boldsymbol{F}\left(\boldsymbol{\text{𝑥}\mid}W,b\right)=W\boldsymbol{x}+b$
\end_inset

, come evidente risulta scomodo mantenere due set di parametri (il bias
 
\begin_inset Formula $\boldsymbol{b}$
\end_inset

 e i pesi 
\begin_inset Formula $W$
\end_inset

) separati.
 L'idea è di includere il vettore di bias nella matrice 
\begin_inset Formula $W$
\end_inset

.
 Sia:
\begin_inset Formula 
\begin{equation}
W=\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n}
\end{bmatrix},\boldsymbol{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{bmatrix},\boldsymbol{b}=\begin{bmatrix}b_{1}\\
b_{2}\\
\vdots\\
b_{n}
\end{bmatrix}
\end{equation}

\end_inset

allora la (9) diventa:
\begin_inset Formula 
\begin{equation}
\boldsymbol{F}\left(\boldsymbol{\text{𝑥}\mid}W,b\right)=W\boldsymbol{x}+b=\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n}
\end{bmatrix}\cdot\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{bmatrix}+\begin{bmatrix}b_{1}\\
b_{2}\\
\vdots\\
b_{n}
\end{bmatrix}=\begin{bmatrix}s_{1}\\
s_{2}\\
\vdots\\
s_{n}
\end{bmatrix}
\end{equation}

\end_inset

 dove 
\begin_inset Formula $s_{i}$
\end_inset

 rappresenta il punteggio della classe i-esima.
 Dalla definizione di prodotto riga per colonna notiamo che la (11) è equivalent
e a:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n} & b_{1}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n} & b_{2}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n} & b_{n}
\end{bmatrix}\cdot\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}\\
1
\end{bmatrix}=\begin{bmatrix}s_{1}\\
s_{2}\\
\vdots\\
s_{n}
\end{bmatrix}
\end{equation}

\end_inset

dove abbiamo esteso la matrice 
\begin_inset Formula $W$
\end_inset

 aggiungendo una colonna contenente 
\begin_inset Formula $\boldsymbol{b}$
\end_inset

 e esteso il vettore 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 aggiungendo 1 in posizione 
\begin_inset Formula $(n+1)$
\end_inset

.
 Rinominando i componenti della matrice 
\begin_inset Formula $W$
\end_inset

 giungiamo alla formula definitiva:
\begin_inset Formula 
\begin{equation}
\boldsymbol{F}\left(\boldsymbol{\text{𝑥}\mid}W\right)=\begin{bmatrix}w_{1,1} & w_{1,2} & \cdots & w_{1,n} & w_{1,n+1}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,n} & w_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
w_{m,1} & w_{m,2} & \cdots & w_{m,n} & w_{m,n+1}
\end{bmatrix}\cdot\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}\\
1
\end{bmatrix}=\begin{bmatrix}s_{1}\\
s_{2}\\
\vdots\\
s_{n}
\end{bmatrix}
\end{equation}

\end_inset

nel nostro esempio CIFAR-10, 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 è adesso 
\begin_inset Formula $[3073\times1]$
\end_inset

 invece di 
\begin_inset Formula $[3072\times1]$
\end_inset

 e 
\begin_inset Formula $W$
\end_inset

 è 
\begin_inset Formula $[10\times3073]$
\end_inset

 invece di 
\begin_inset Formula $[10\times3072]$
\end_inset

.
\end_layout

\begin_layout Subsection
Interpretazione di un Classificatore Lineare.
\end_layout

\begin_layout Standard
Linear classifier, data in input una immagine (nel caso più generale dato
 un generico vettore di dati) calcola il punteggio di una classe come una
 somma pesata di ogni pixel attraverso tutti e tre i canali colori (RGB).
 Ogni riga della matrice 
\begin_inset Formula $W$
\end_inset

 contiene i pesi per mappare i tre canali colore di ogni pixel nel punteggio
 di una classe (una riga per classe), quindi in definitiva la funzione lineare
 ha la capacità di approvare o disapprovare (dipendente dal segno di ciascun
 
\begin_inset Formula $w_{i,j})$
\end_inset

 un certo colore in una certa posizione.
 Per esempio, generalmente, un'immagine della classe 
\begin_inset Quotes qld
\end_inset

ship
\begin_inset Quotes qrd
\end_inset

 ci aspettiamo abbia un'alta presenza di colore blu ai lati dell'immagine,
 mentre abbia molto meno altri colori.
\end_layout

\begin_layout Subsubsection
Interpretazione Geometrica
\end_layout

\begin_layout Standard
Poichè trattiamo le nostre immagini di input come vettori colonna possiamo
 considerare ogni immagine come un punto nello spazio, nell'esempio CIFAR-10
 lo spazio è 
\begin_inset Formula $\mathbb{R}^{3072}$
\end_inset

 (escludendo il bias trick).
 Inoltre dall'algebra lineare sappiamo che l'equazione generica di un 
\series bold
iperpiano
\series default
 (sottospazio affine di 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 di dimensione 
\begin_inset Formula $n-1$
\end_inset

 ) è 
\begin_inset Formula $\varSigma:a_{1}x_{1}+a_{2}x_{2}+\ldots+a_{n}x_{n}+b=0$
\end_inset

 che riscritto in forma più compatta: 
\begin_inset Formula $\varSigma:<\boldsymbol{a},\boldsymbol{x}>+b=0$
\end_inset

 dove 
\begin_inset Formula $<\cdot,\cdot>$
\end_inset

 è il prodotto scalare.
 Osserviamo che il prodotto 
\begin_inset Formula $W\boldsymbol{x}$
\end_inset

 corrisponde a eseguire 
\begin_inset Formula $m$
\end_inset

 prodotti scalari, quindi 
\begin_inset Formula $\boldsymbol{F}\left(\boldsymbol{\text{𝑥}\mid}W,b\right)=W\boldsymbol{x}+b=0$
\end_inset

 definisce 
\begin_inset Formula $m$
\end_inset

 iperpiani nello spazio.
 Ricordando ora che la distanza di un generico punto 
\begin_inset Formula $\boldsymbol{P}$
\end_inset

 da un iperpiano 
\begin_inset Formula $\varSigma$
\end_inset

 è: 
\begin_inset Formula 
\begin{equation}
d(\varSigma,\boldsymbol{P})=\frac{|<\boldsymbol{a},\boldsymbol{P}>+b|}{∥\boldsymbol{a}∥}
\end{equation}

\end_inset

osserviamo che la distanza è direttamente proporzionale al prodotto scalare
 dei coefficienti dell'iperpiano con il punto (in modulo, il segno determina
 se siamo 
\shape italic
sopra
\shape default
 o 
\shape italic
sotto
\shape default
 l'iperpiano).
 Possiamo vedere ogni iperpiano come il confine di una regione di accettazione,
 il punteggio 
\begin_inset Formula $s_{i}$
\end_inset

 esprime tramite il segno se siamo nella regione di accettazione o no, e
 tramite il modulo quanto siamo lontani dal confine, quindi un alto punteggio
 positivo indica un'alta confidenza che quella immagine appartenga alla
 classe i-esima, viceversa un alto punteggio negativo indica una bassissima
 confidenza.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename interpretazione_geometrica.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Interpretazione geomtrica nello spazio bidimensionale
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Template
\end_layout

\begin_layout Standard
Un'altra possibile interpretazione per i pesi 
\begin_inset Formula $W$
\end_inset

 è che ogni riga della matrice corrisponda a un 
\series bold
template
\series default
 (o prototipo) per ogni classe.
 Il punteggio di ogni classe è ottenuto tramite prodotto scalare e indica
 il grado di somiglianza tra l'immagine test e il template.
\end_layout

\begin_layout Chapter
Reti Convoluzionali
\end_layout

\begin_layout Standard
Le 
\series bold
reti neurali convoluzionali
\series default
 (
\series bold
Convolutional Neural Networks, CNN
\series default
) sono di fatto delle reti neurali artificiali.
 Esattamente come queste ultime infatti, anche le reti neurali convoluzionali
 sono costituite da neuroni collegati fra loro tramite dei rami pesati (weight);
 i parametri allenabili delle reti sono sempre quindi i 
\series bold
weight
\series default
 ed i 
\series bold
bias
\series default
.
 Tutto quanto detto in precedenza sull’allenamento di una rete neurale,
 cioè forward/backward propagation e aggiornamento dei pesi, vale anche
 in questo contesto; inoltre un’intera rete neurale convoluzionale utilizza
 sempre una singola funzione di costo differenziabile.
 Tuttavia le reti neurali convoluzionali fanno la specifica assunzione che
 il loro input abbia una precisa struttura di dati, come ad esempio un’immagine
 nel nostro caso, e ciò permette ad esse di assumere delle specifiche proprietà
 nella loro architettura al fine di elaborare al meglio tali dati.
 Ad esempio di poter effettuare delle forward propagation più efficienti
 in modo da ridurre l’ammontare di parametri della rete.
 Il nome Convolutional Neural Networks deriva dal fatto che tali reti utilizzano
 un'operazione matematica lineare chiamata 
\series bold
convoluzione
\series default
.
 Gli strati composti da operazioni di convoluzione prendono il nome di 
\series bold
Convolutional Layers
\series default
, ma non sono gli unici strati che compongono una CNN: la tipica architettura
 prevede infatti l'alternarsi di 
\series bold
Convolutional Layers, Pooling Layers e Fully Connected Layers.
\end_layout

\begin_layout Section
Inadeguatezza della struttura fully-connected
\end_layout

\begin_layout Standard
Come visto nel capitolo precedente, le reti neurali tradizionali ricevono
 in input un singolo vettore, e lo trasformano attraverso una serie di strati
 nascosti, dove ogni neurone è connesso ad ogni singolo neurone sia dello
 strato precedente che di quello successivo (ovvero "
\shape italic
fully-connected
\shape default
") e funziona quindi in maniera completamente indipendente, dal momento
 che non vi è alcuna condivisione delle connessioni con i nodi circostanti.
 Nel caso l'input sia costituito da immagini di dimensioni ridotte, ad esempio
 
\begin_inset Formula $32\times32\times3$
\end_inset

 (32 altezza, 32 larghezza, 3 canali colore), un singolo neurone connesso
 in questa maniera comporterebbe un numero totale di 32 × 32 × 3 = 3072
 pesi, una quantità abbastanza grande ma ancora trattabile.
 Le cose si complicano però quando le dimensioni si fanno importanti: salire
 ad appena 256 pixel per lato comporterebbe un carico di 
\begin_inset Formula $256\times256\times3=196'608$
\end_inset

 pesi per singolo neurone, ovvero quasi 2 milioni di parametri per una semplice
 rete con un singolo strato nascosto da dieci neuroni.
 L'architettura fully-connected risulta perciò troppo esosa in questo contesto,
 comportando una quantità enorme di parametri che condurrebbe velocemente
 a casi di sovradattamento.
 Inoltre una rete FC è del tutto generica e non ha nessun tipo di accorgimento
 per l'elaborazione di immagini che presentano una struttura ben determinata.
 Le Convolutional Neural Networks prendono invece vantaggio dall'assunzione
 che gli input hanno proprio una struttura di questo tipo, e questo permette
 loro la costruzione di un'architettura su misura attraverso la formalizzazione
 di tre fondamentali proprietà: 
\series bold
l'interazione sparsa
\series default
 (
\series bold
sparse interaction
\series default
), l
\series bold
'invarianza rispetto a traslazioni
\series default
 (
\series bold
invariant to translation
\series default
), e la 
\series bold
condivisione dei parametri
\series default
 (
\series bold
weight sharing
\series default
).
 Il risultato è una rete più effcace e allo stesso tempo parsimoniosa in
 termini di parametri.
\end_layout

\begin_layout Section
Operazione di convoluzione
\end_layout

\begin_layout Standard
In problemi discreti l'operazione di convoluzione non è altro che la somma
 degli elementi del prodotto di Hadamard fra un set di parametri (che prende
 il nome di filtro o kernel) e una porzione dell'input di pari dimensioni.
 L'operazione di convoluzione viene quindi ripetuta spostando il filtro
 lungo tutta la supercie dell'input, sia in altezza che in larghezza.
 Questo produce quella che viene chiamata mappa di attivazioni (o 
\series bold
features map
\series default
), la quale costituisce di fatto il primo strato nascosto della rete.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename convoluzione.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Operazione di convoluzione nel caso bidimensionale: un filtro di dimensione
 2 × 2 viene moltiplicato elemento per elemento per una porzione dell'input
 di uguali dimensioni.
 L'output dell'operazione è costituito dalla somma di tali prodotti.
 L'operazione di convoluzione viene infine ripetuta spostando il filtro
 lungo le due dimensioni dell'input.
\end_layout

\end_inset


\end_layout

\end_inset

 Nel caso in cui gli input siano rappresentati da volumi tridimensionali
 la procedura rimane la stessa, ma è importante notare che il filtro, pur
 mantendo una ridotta estensione spaziale (larghezza e altezza), viene esteso
 in profondità in misura uguale all'input.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Conv_3d.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Operazione di convoluzione di due differenti filtri (W0 e W1) di dimensione
 
\begin_inset Formula $3\times3\times3$
\end_inset

 su un volume di input 
\begin_inset Formula $7\times7\times3$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

É importante notare che la singola operazione di convoluzione produce sempre
 uno scalare, indipendentemente da quali siano le dimensioni del volume
 di input, sia esso bidimensionale o tridimensionale.
 Di conseguenza, una volta che il filtro viene fatto convolvere lungo tutta
 la supercie dell'input, si ottiene sempre una mappa di attivazioni bidimensiona
le.
 
\end_layout

\begin_layout Section
Strati di una CNN
\end_layout

\begin_layout Subsection
Convolutional Layer
\end_layout

\begin_layout Standard
Come si può intuire questo è il principale tipo di layer: l’utilizzo di
 uno o più di questi layer in una rete neurale convoluzionale è indispensabile.
 I
\series bold
 parametri di un convolutional laye
\series default
r in pratica riguardano un insieme di filtri allenabili.
 Ciascun 
\series bold
filtro
\series default
 è spazialmente ridotto, lungo le dimensioni di larghezza ed altezza, ma
 si estende per l’intera profondità del volume di input a cui viene applicato.
 Durante la forward propagation si trasla, o più precisamente si convolve,
 ciascun filtro lungo la larghezza e l’altezza del volume di input, producendo
 una 
\series bold
activation map
\series default
 (o 
\series bold
feature map
\series default
) bidimensionale per quel filtro.
 Intuitivamente, la rete avrà come obbiettivo quello di apprendere (tramite
 
\series bold
backpropagation
\series default
 e 
\series bold
discesa del gradiente
\series default
) dei filtri che si attivano in presenza di un qualche specifico tipo di
 feature in una determinata regione spaziale dell’input.
 L’accodamento di tutte queste activation map, per tutti i filtri, lungo
 la dimensione della profondità forma il volume di output di un layer convoluzio
nale.
 Ciascun elemento di questo volume può essere interpretato come l’output
 di un neurone (con la sua funzione di attivazione) che osserva solo un
 piccola regione dell’input e che condivide i suoi parametri con gli altri
 neuroni nella stessa activation map, dato che questi valori provengono
 tutti dall’applicazione dello stesso filtro.
 In altre parole, ragionando dalla prospettiva opposta, la mappa di attivazioni
 è formata da neuroni connessi localmente allo strato di input attraverso
 i parametri del filtro che li ha generati.
 Attualmente le cose potrebbero non apparire ancora del tutto chiare.
 Vediamo dunque più dettagliatamente i concetti appena espressi, suddividendoli
 per meglio comprendere l’intero processo.
\end_layout

\begin_layout Paragraph
Proprietà di connettività locale
\end_layout

\begin_layout Standard
Era già stato menzionato in precedenza il fatto che, avendo a disposizione
 degli input di considerevoli dimensioni come appunto delle immagini, connettere
 ciascun neurone di un layer con tutti i neuroni del layer precedente (o
 del volume di input) nella pratica non è conveniente.
 Infatti qui ciascun neurone è connesso solo ad una piccola regione locale
 del volume di input.
 L’estensione spaziale (sempre larghezza e altezza) di questa regione è
 un parametro del layer convoluzionale e viene detto 
\series bold
receptive field
\series default
 del neurone.
 È bene ricordare quanto già detto in precedenza: l’estensione lungo l’asse
 della profondità della regione considerata è sempre uguale alla profondità
 del volume di input.
 Ciò significa che rispetto alla profondità non si esclude nulla dell’input
 per ciascuna regione locale, limitata invece in larghezza ed altezza.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Layer_conv.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Layer convoluzionale applicato ad un’immagine di CIFAR-10, si noti come
 un singolo neurone non sia connesso a tutto l'input ma solo ad una parte
 di dimensione uguale al filtro applicato
\end_layout

\end_inset


\end_layout

\end_inset

Un semplice esempio può chiarire questi concetti: prendiamo la solita immagine
 di CIFAR-10, quindi un volume 
\begin_inset Formula $[32\times32\times3]$
\end_inset

; se il receptive field è di dimensioni 
\begin_inset Formula $5\times5$
\end_inset

, allora ciascun neurone del layer convoluzionale avrà dei weight associati
 ad una regione locale 
\begin_inset Formula $5\times5\times3$
\end_inset

 del volume di input, per un totale di 
\begin_inset Formula $5*5\ast3=75$
\end_inset

 pesi, numero decisamente inferiore rispetto a quanto visto nella sezione
 precedente (
\begin_inset Formula $3072$
\end_inset

 pesi) per un neurone in un’architettura fully connected.
\end_layout

\begin_layout Paragraph
Output dello strato
\end_layout

\begin_layout Standard
Fin qui si è parlato della connettività di ciascun neurone di un layer convoluzi
onale rispetto al volume di input.
 In questa sottosezione si parlerà invece della quantità di neuroni presenti
 nel volume di output di un layer convoluzionale e di come essi sono organizzati.
 In questo caso vi sono 3 iperparametri:
\end_layout

\begin_layout Itemize

\series bold
Profondità (depth)
\series default
: corrisponde al numero di filtri 
\begin_inset Formula $N_{F}$
\end_inset

 che compongono lo strato, ognuno in cerca di caratteristiche differenti
 nel volume di input.
 Il set di neuroni (appartanenti a filtri diversi) connessi alla stessa
 regione di input prende invece il nome di colonna profondità (depth column).
\end_layout

\begin_layout Itemize

\series bold
Stride
\series default
: specifica il numero di pixel di cui si vuole traslare il filtro ad ogni
 spostamento.
 Poiché come detto eseguendo la convoluzione tra filtro e parte dell'input
 otteniamo un singolo scalare, questo parametro permette di specificare
 in che maniera allocare le depth column in tutto lo spazio, cioè larghezza
 e altezza, dell’input.
 Quando lo stride è pari ad uno significa che stiamo muovendo il filtro
 un pixel alla volta, e di conseguenza viene scannerizzata ogni possibile
 posizione dell'input.
 Valori più alti muovono il filtro con salti maggiori, e pertanto viene
 generato un output di dimensioni minori.
\end_layout

\begin_layout Itemize

\series bold
Zero padding
\series default
: a volte può risultare conveniente aggiungere un bordo di zeri al volume
 di input, in modo così da controllare le dimensioni dell'output ed evitare
 incongruenze durante le operazioni.
 Lo spessore di questo bordo è determinato dall'iperparametro di zero-padding,
 ed è spesso utilizzato per far combaciare la dimensione dell'input con
 quella dell'output.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Stride1.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Stride uguale a 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Stride2.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Stride uguale a 2
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Padding.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Padding uguale a 1
\end_layout

\end_inset


\end_layout

\end_inset

Larghezza e altezza del volume di output sono determinati in funzione del
 volume di input, dimensione del campo recettivo (dimensione del filtro),
 dello stride, quantità di zero-padding.
 La profondità invece dipende dal numero di filtri 
\begin_inset Formula $N_{F}$
\end_inset

 applicati.
 Siano 
\begin_inset Formula $F$
\end_inset

 la dimensione di un lato di un filtro (i filtri son quadrati), 
\begin_inset Formula $P$
\end_inset

 la quantità di padding applicata, 
\begin_inset Formula $S$
\end_inset

 il valore di stride allora, dato in input un volume di dimensione 
\begin_inset Formula $[W_{1}\times H_{1}\times D_{1}]$
\end_inset

 lo strato convoluzionale produrra un output di dimensione 
\begin_inset Formula $[W_{2}\times H_{2}\times D_{2}]$
\end_inset

 dove:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{cases}
W_{2}=\frac{(W_{1}-F+2P)}{S}+1\\
H_{2}=\frac{(H_{1}-F+2P)}{S}+1\\
D_{2}=N_{F}
\end{cases}
\end{equation}

\end_inset

Chiaramente, i valori di stride e zero-padding devono essere scelti in modo
 tale che 
\begin_inset Formula $W_{2}$
\end_inset

 e 
\begin_inset Formula $H_{2}$
\end_inset

 siano valori interi.
 Ad esempio, l'architettura AlexNet di Krizhevsky et al.
 (2012) accetta in input immagini di dimensione [227 × 227 × 3] e utilizza
 nel suo primo convolutional layer 96 filtri di dimensione 11, stride pari
 a 4 e nessun zero-padding.
 Dal momento che 
\begin_inset Formula $(227−11)/4+1=55$
\end_inset

 il volume finale dell'output del primo strato avrà dimensione [55 × 55
 × 96].
 Ognuno dei 55 × 55 × 96 neuroni di questo volume è connesso ad una regione
 di dimensione [11 × 11 × 3] del volume di input, e tutti i 96 neuroni appartene
nti alla stessa colonna profondità sono connessi alla stessa regione [11
 × 11 × 3], ma ovviamente attraverso set diversi di pesi.
 
\series bold
Osservazione: 
\series default
è importante che durante la convoluzione i filtri coprano tutto l'input,
 non è una buona idea quindi adottare un valore di stride superiore alla
 larghezza dei filtri.
 
\end_layout

\begin_layout Paragraph
Condivisione dei parametri e invarianza alla traslazione
\end_layout

\begin_layout Standard
Nell'esempio sopra riportato, ognuno dei 
\begin_inset Formula $55\times55\times96=290400$
\end_inset

 neuroni del primo convolutional layer possiede 
\begin_inset Formula $11\times11\times3=363$
\end_inset

 pesi, più uno relativo alla distorsione.
 In un'architettura come quella delle reti neurali classiche che non prevede
 la condivisione dei pesi questo comporterebbe un numero totale di parametri
 pari a 
\begin_inset Formula $290400\times364=105705600$
\end_inset

, solamente per il primo strato.
 Tale quantità, chiaramente intrattabile nella realtà, viene drasticamente
 ridotta dalla proprietà di condivisione dei parametri (
\series bold
weight sharing
\series default
) delle CNN, la quale si fonda su una ragionevole assunzione: se la rilevazione
 di una caratteristica in una determinata posizione spaziale risulta utile,
 lo sarà anche in differenti posizioni spaziali.
 Si assume cioè una struttura dell'immagine
\series bold
 invarante rispetto a traslazioni
\series default
.
 Questo è il motivo per cui facciamo scorrere lo stesso filtro su tutto
 il volume di input.
 In pratica, chiamando una singola “
\shape italic
sezione
\shape default
” bidimensionale lungo l’asse di profondità del volume di output con depth
 slice (o mappa di attivazione, ad esempio un volume 55 x 55 x 96 ha 96
 depth slice, ciascuna ovviamente di dimensioni 55 x 55), si farà in modo
 che tutti i neuroni di ciascuna 
\series bold
depth slice
\series default
 (attenzione a non confondere con 
\shape italic
depth column
\shape default
) utilizzino gli stessi weight e bias.
 Tornando all'esempio, la proprietà di weight sharing comporta una diminuzione
 dei parametri per il primo strato fino a 
\begin_inset Formula $(11\times11\times3)\times96+96=34944$
\end_inset

, rispetto ai precedenti 105 milioni, ovvero circa tremila volte meno.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 96filtri.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Set di 96 filtri 11 × 11 × 3 appresi dall'architettura di Krizhevsky et
 al.
 (2012) in un problema di classicazione di immagini.
 L'assunzione di weight sharing è ragionevole dal momento che individuare
 una linea o uno spigolo è importante in qualsiasi posizione dell'immagine,
 e di conseguenza non c'è la necessità di imparare ad localizzare la stessa
 caratteristica in tutte le possibili zone.
\end_layout

\end_inset


\end_layout

\end_inset

E' importante notare che in determinati contesti l'assunzione di weight
 sharing perde di senso.
 Un caso particolare è quando le immagini in input presentano una struttura
 specifica e centrata, e ci si aspetta di apprendere caratteristiche differenti
 in una determinata area dell'immagine piuttosto che in un'altra.
 Ad esempio, in un problema di riconoscimento facciale dove i volti sono
 stati ritagliati e centrati, risulterà più efficace apprendere nella parte
 superiore dell'immagine caratteristiche relative ad occhi o capelli, mentre
 nella parte inferiore quelle speciche di bocca e naso.
\end_layout

\begin_layout Standard
\noindent
Riassumendo, un layer convoluzionale:
\end_layout

\begin_layout Itemize
accetta in ingresso un volume 
\begin_inset Formula $[W_{1}\times H_{1}\times D_{1}]$
\end_inset

;
\end_layout

\begin_layout Itemize
richiede il settaggio dei seguenti parametri:
\end_layout

\begin_deeper
\begin_layout Itemize
numero di kernel 
\begin_inset Formula $N_{F}$
\end_inset

 (il parametro depth);
\end_layout

\begin_layout Itemize
il lato dell’area del receptive field F; 
\end_layout

\begin_layout Itemize
lo stride S; 
\end_layout

\begin_layout Itemize
l’ammontare dello zero-padding P;
\end_layout

\end_deeper
\begin_layout Itemize
produce un volume di output di dimensioni 
\begin_inset Formula $[W_{2}\times H_{2}\times D_{2}]$
\end_inset

 dove:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $W_{2}=\frac{(W_{1}-F+2P)}{S}+1$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $H_{2}=\frac{(H_{1}-F+2P)}{S}+1$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $D_{2}=N_{F}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
con la tecnica della condivisione dei parametri, si hanno 
\begin_inset Formula $N_{F}\times N_{F}\times D_{1}$
\end_inset

 pesi per ogni kernel, per un totale di 
\begin_inset Formula $N_{F}\times N_{F}\times D_{1}$
\end_inset

 pesi e 
\begin_inset Formula $N_{F}$
\end_inset

 bias;
\end_layout

\begin_layout Itemize
nel volume di output l’i-esimo depth slice (di taglia 
\begin_inset Formula $W_{2}\times H_{2}$
\end_inset

) è il risultato dell’operazione di convoluzione dell’i-esimo kernel sul
 volume di input con uno stride S e con la successiva aggiunta dell’i-esimo
 bias.
\end_layout

\begin_layout Paragraph
Campo recettivo
\end_layout

\begin_layout Standard
\noindent
Un concetto base nelle CNN è il 
\series bold
campo recettivo
\series default
 (
\series bold
receptive field
\series default
).
 A causa della connettività sparsa, in una CNN l'output dipende solo dalla
 regione dell'input, a differenza delle reti FC dove il valore di ogni output
 dipende da tutto l'input.
\end_layout

\begin_layout Standard
\noindent
Questa regione dell'input è il campo recettivo per quell'output.
 Più si va in profondità, più largo è il campo recettivo.
 Di solito il campo recettivo si riferisce all'unità di output finale della
 rete in relazione all'input della rete, ma la stessa definizione vale per
 volumi intermedi.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\size small
\begin_inset Graphics
	filename receptiveFieldFC.png
	lyxscale 50
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\size small
\begin_inset Graphics
	filename receptiveFieldCNN.png
	lyxscale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Campi recettivi di FCN (in alto) e CNN (in basso)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Pooling layer
\end_layout

\begin_layout Standard
Nell'architettura di una CNN è pratica comune inserire fra due o più convolution
al layers uno strato di 
\series bold
Pooling
\series default
, la cui funzione è quella di ridurre progressivamente la dimensione spaziale
 degli input (larghezza e altezza), in modo da diminuire numero di parametri
 e carico computazionale, e di conseguenza controllare anche il sovradattamento.
 Per il ridimensionamento si utilizza una semplice funzione, come ad esempio
 un’operazione di 
\series bold
max (max-pooling)
\series default
 oppure di 
\series bold
media (average-pooling)
\series default
 e pertanto non comporta la presenza di pesi allenabili.
 L'assenza di pesi allenabili comporta anche che questo layer svolga le
 sue mansioni solo durante la forward propagation, nelle fasi di backward
 propagation retropropaga gli errori e basta senza calcolare alcuna derivata.
 I pooling layer hanno alcuni iperparametri settabili:
\end_layout

\begin_layout Itemize
il 
\series bold
lato
\series default
 F dell’estensione spaziale della selezione quadrata che verrà di volta
 in volta considerata sull’input in ogni suo depth slice; 
\end_layout

\begin_layout Itemize
il parametro di 
\series bold
stride
\series default
 S;
\end_layout

\begin_layout Standard
Si può notare una certa somiglianza con i parametri di un layer convoluzionale:
 questo perchè anche qui si ha una sorta di receptive field che viene spostato
 di volta in volta, con un passo specificato dal parametro di stride, su
 ciascun depth slice del volume di input.
 In realtà oltre a questo, la situazione in questo caso è completamente
 diversa, dato che qui non vi è alcuna operazione di convoluzione.
 Come per il layer convoluzionale, il volume di output 
\begin_inset Formula $[W_{2}\times H_{2}\times D_{2}]$
\end_inset

 è funzione del volume di input 
\begin_inset Formula $[W_{1}\times H_{1}\times D_{1}]$
\end_inset

 e dei due iperparametri secondo formule analoghe a quelle della sezione
 precedente:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{cases}
W_{2}=\frac{(W_{1}-F)}{S}+1\\
H_{2}=\frac{(H_{1}-F)}{S}+1
\end{cases}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Max_pooling.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Fully Connected layer
\end_layout

\begin_layout Standard
Questo tipo di layer è esattamente uguale ad uno qualsiasi dei layer di
 una classica rete neurale artificiale con architettura fully connected:
 semplicemente in un 
\series bold
layer Fully Connected (FC)
\series default
 ciascun neurone è connesso a tutti i neuroni del layer precedente, nello
 specifico alle loro attivazioni.
 L'architettura fully-connected implica il rilassamento dell'assunzione
 di weight sharing.
 Valgono quindi la maggior parte delle considerazioni fatte nel capitolo
 2, ad eccezione ovviamente di quelle che non hanno senso applicate ad un
 singolo strato o un sottoinsieme degli strati della rete (es.
 l'algoritmo di discesa del gradiente è unico per tutta la rete, non ha
 senso applicarne uno diverso per uno strato).
 Solitamente si utilizza più di un layer FC in serie e l’ultimo di questi
 avrà un numero di neuroni 
\begin_inset Formula $K$
\end_inset

 pari al numero di classi presenti nel dataset (nel caso della classificazione)
 o in generale il numero di neuroni è determinato dall'output della rete
 desiderato.
 La funzione principale dei FC layer nell’ambito delle reti neurali convoluziona
li è quello di effettuare una sorta di raggruppamento delle informazioni
 ottenute fino a quel momento, esprimendole con un singolo numero (l’attivazione
 di uno dei suoi neuroni), il quale servirà nei successivi calcoli per la
 classificazione finale.
\end_layout

\begin_layout Section
Architettura generale di una CNN
\end_layout

\begin_layout Standard
Finora abbiamo visto i singoli strati che possono essere impiegati nell'architet
tura di una Convolutional Neural Network.
 Come per le reti neurali tradizionali, anche qui non esiste una linea guida
 per la scelta degli iperparametri, ne tantomeno per la sequenza da adottare
 nella scelta degli strati.
 Ci sono però alcune considerazioni ragionevoli che si possono fare per
 cercare quantomeno di muoversi lungo la giusta direzione.
 La dimensione dei filtri dovrebbe, almeno in linea teorica, essere motivata
 dalla correlazione spaziale dell'input che un particolare strato riceve,
 mentre il numero di fiitri utilizzati (quindi di feature maps generate)
 si riflette sulla capacità della rete di cogliere rappresentazioni gerarchiche:
 questo signica che gli strati finali necessitano di un numero di filtri
 maggiore di quello destinato agli strati iniziali, altrimenti si limitano
 le possibilità di combinare features di basso livello per rappresentare
 features di alto livello, che sono di fatto quelle più vicine alla classicazion
e finale.
 La rappresentazione gerarchica sottintende che generalmente non è un sufficient
e un singolo strato convoluzionale, in quanto features estratte nello stesso
 strato sono gerarchicamente di pari importanza.
 Inoltre anche per le CNN valgono le tecniche per affrontare l'overfitting
 quali dropout, batch normalization e altre trattate in precedenza con gli
 opportuni accorgimenti del caso.
 La maggior parte delle architetture CNN seguono il seguente schema di composizi
one dei layer: 
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC
\end_layout

\end_inset


\end_layout

\begin_layout Standard
dove:
\end_layout

\begin_layout Itemize
* indica la ripetizione
\end_layout

\begin_layout Itemize
POOL? indica l'opzionalità del pooling layer
\end_layout

\begin_layout Itemize
N > 0 e solitamente N <= 3
\end_layout

\begin_layout Itemize
M >= 0
\end_layout

\begin_layout Itemize
k >= 0 e solitamente k < 3
\end_layout

\begin_layout Standard
Nelle prossime pagine mostreremo alcune architetture note di reti CNN e
 varianti per compiti specifici (es.
 segmentation)
\end_layout

\begin_layout Section
Esempi architetture CNN
\end_layout

\begin_layout Standard
Architetture note di CNN sono:
\end_layout

\begin_layout Enumerate

\series bold
LeNet-5
\series default
: sviluppato nel 1998 per identificare cifre scritte a mano per il riconosciment
o dello zip code nel servizio postale (60000 parametri).
 L'idea era quella di usare una sequenza di 3 layers: convoluzione, pooling,
 non-linearità.
 I pixel in un'immagine sono altamente correlati, la convoluzione può essere
 usata come modo di estrarre le features spaziali e per avere connessioni
 sparse, mentre il subsample viene fatto usando la media spaziale delle
 mappe (average pooling).
 Le non-linearità usate in questa rete sono Tanh o sigmoidi.
 Infine, l'ultimo layer ha un MLP come classificatore.
\end_layout

\begin_layout Enumerate

\series bold
AlexNet
\series default
: sviluppata nel 2012, è simile a LeNet-5 (60 milioni di parametri, conv.:
 3.7 milioni, FC: 58.6 milioni).
 La dimensione dell'input è di 224x224x3, mentre i layer sono:
\end_layout

\begin_deeper
\begin_layout Itemize
5 layer convoluzionali
\end_layout

\begin_layout Itemize
3 MLP
\end_layout

\begin_layout Standard
Per evitare l'overfitting la rete usa ReLU, Dropout (0.5), weight decay e
 maxpooling.
 Il primo layer convoluzionale ha 96 filtri 11x11 con stride=4.
 L'output è costituito da 2 volumi da 55x55x48.
 
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
VGG16
\series default
: introdotta nel 2014 come variante della AlexNet (138 milioni di parametri,
 conv.:11%, FC:89%).
 La dimensione dell'input è di 224x224x3.
 L'idea è quella di usare convoluzioni multiple 3x3 in sequenza per ottenere
 campi recettivi più larghi con meno parametri e più non-liearità anzichè
 con filtri più larghi in un layer singolo.
\end_layout

\begin_layout Enumerate

\series bold
GoogleNet
\series default
: rete con alta efficienza computazionale (5 milioni di parametri, 22 layers
 di moduli Inception).
 Questa rete è basata su moduli Inception, ovvero una sorta di 
\begin_inset Quotes fld
\end_inset

rete nella rete
\begin_inset Quotes frd
\end_inset

 o 
\begin_inset Quotes fld
\end_inset

moduli locali
\begin_inset Quotes frd
\end_inset

.
 Scegliere la giusta dimensione del kernel per le operazioni di convoluzione
 è difficile, dato che l'immagine potrebbe mostrare features rilevanti a
 scalature diverse.
 Le reti troppo dense tendono inoltre a overfittare e a diventare computazionalm
ente molto costose.
 La soluzione è quella di sfruttare dimensioni multiple del filtro allo
 stesso livello e poi fare un merge tramite concatenazione delle mappe di
 attivazione dell'output (con zero padding).
\end_layout

\begin_layout Enumerate

\series bold
Inception Module
\series default
: per ridurre il carico computazionale della rete, il numero dei canali
 di input è ridotto aggiungendo un layer convoluzionale 1x1 prima delle
 convoluzioni 3x3 e 5x5.
 Il volume dell'output ha dimensione simile, ma il numero di operazioni
 richieste è ridotto significativamente grazie alla convoluzione 1x1 (che
 incrementa quindi il numero di non-linearità).
 Tale layer convoluzionale è anche chiamato ''bottleneck layer''.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename lenet.png
	lyxscale 80
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
LeNet-5
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename alexnet.png
	lyxscale 130
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
AlexNet
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename vgg16.png
	lyxscale 70
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
VGG16
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename GoogleNet.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
GoogleNet
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Inception.png
	lyxscale 60
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Inception module 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Data Augmentation
\end_layout

\begin_layout Standard
La 
\series bold
Data Augmentation
\series default
 è tipicamente effettuata tramite:
\end_layout

\begin_layout Itemize

\series bold
trasformazioni geometriche
\end_layout

\begin_deeper
\begin_layout Itemize
shift/rotazioni/distorsioni
\end_layout

\begin_layout Itemize
shear
\end_layout

\begin_layout Itemize
scaling
\end_layout

\begin_layout Itemize
flip
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
trasformazioni fotometriche
\end_layout

\begin_deeper
\begin_layout Itemize
aggiunta di rumore
\end_layout

\begin_layout Itemize
modifica dell'intensità media
\end_layout

\begin_layout Itemize
superimposizione di altre immagini
\end_layout

\begin_layout Itemize
modificazione del contrasto
\end_layout

\end_deeper
\begin_layout Standard

\series bold
Test Time Augmentation
\end_layout

\begin_layout Standard
\noindent
Anche se la CNN è allenata usando Data Augmentation, non raggiungerà un'invarian
za perfetta rispetto alle trasformazioni considerate.
\end_layout

\begin_layout Standard
\noindent
La 
\series bold
Test Time Augmentation (TTA)
\series default
 può essere performata a test time per migliorare la precisione delle predizioni.
 TTA è particolarmente utile per le immagini di test in cui il modello è
 parecchio insicuro.
\end_layout

\begin_layout Standard
\noindent
Gli step della TTA sono:
\end_layout

\begin_layout Enumerate
performare un'augmentation randomica su ogni immagine di test 
\emph on
I
\end_layout

\begin_layout Enumerate
fare una media delle predizioni per ogni 
\emph on
I
\end_layout

\begin_layout Enumerate
prendere il vettore delle medie per la definizione della predizione finale
\end_layout

\begin_layout Standard

\series bold
Confusion matrix
\end_layout

\begin_layout Standard
\noindent
L'idea della 
\series bold
confusion matrix
\series default
 sta nel fatto che l'elemento 
\emph on
C(i, j) 
\emph default
della matrice corrisponda alla percentuale degli elementi appartenenti alla
 classe 
\emph on
i 
\emph default
classificati come elementi della classe 
\emph on
j
\emph default
.
 Quindi la confusion matrix ideale ha tutti 1 sulla diagonale principale.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename confusion matrix.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Esempio di confusion matrix
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\series bold
Classificazione a due classi
\end_layout

\begin_layout Standard
\noindent
La performance della classificazione nel caso di classificatori binari può
 anche essere misurata in termini della 
\series bold
ROC (Receiver Operating Characteristic) curve
\series default
.
 
\end_layout

\begin_layout Standard
\noindent
Il classificatore ideale raggiungerebbe: 
\end_layout

\begin_layout Itemize
FPR = 0%
\end_layout

\begin_layout Itemize
TPR = 100%
\end_layout

\begin_layout Standard
Quindi, più larga è l'
\series bold
Area Under Curve (AUC)
\series default
, meglio è.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename roc.png
	lyxscale 50
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ROC (ogni punto rosso è specifico per un parametro)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Transfer Learning
\end_layout

\begin_layout Standard
L'output del fully connected layer ha la stessa dimensione del numero di
 classi L, e ogni componente fornisce un punteggio di appartenenza a una
 certa classe per l'immagine in input.
\end_layout

\begin_layout Standard
\noindent
L'input del fully connected layer può essere visto come un descrittore dell'imma
gine in input, cioè un vettore di features, le quali sono definite per massimizz
are la performance di classificazione e sono allenate con backpropagation.
\end_layout

\begin_layout Standard
\noindent
Come sappiamo, più ci muoviamo in profondità, più la risoluzione spaziale
 è ridotta e il numero di mappe (relativo al campo percettivo) aumenta,
 quindi cerchiamo dei pattern di alto livello senza preoccparci troppo della
 loro posizione esatta nell'immagine.
 
\end_layout

\begin_layout Standard
\noindent
La parte convoluzionale può quindi essere vista come un estrattore di feature
 molto generale, mentre i FC layers sono molto personalizzati, dato che
 sono pensati per risolvere obiettivi di classificazione specifici.
\end_layout

\begin_layout Standard
\noindent
L'idea alla base del 
\series bold
transfer learning
\series default
 è quindi quella di:
\end_layout

\begin_layout Itemize
\noindent
prendere un modello pre-allenato (es.: VGG)
\end_layout

\begin_layout Itemize
rimuovere e modificare i FC layers 
\end_layout

\begin_layout Itemize
congelare i pesi nella CNN
\end_layout

\begin_layout Itemize
allenare l'intera rete sui nuovi dati di training
\end_layout

\begin_layout Standard
Ci sono due opzioni di congelamento della CNN:
\end_layout

\begin_layout Itemize

\series bold
Transfer Learning
\series default
: solo i layers della FCN vengono allenati (una buona opzione quando si
 hanno pochi dati di training e ci si aspetta che la CNN pre-allenata sia
 compatibile col problema)
\end_layout

\begin_layout Itemize

\series bold
Fine tuning
\series default
: l'intera CNN è riallenata, ma i layer convoluzionali sono inizializzati
 al modello pre-allenato (una buona opzione quando si hanno abbastanza dati
 di training o quando non ci si aspetta che la CNN pre-allenata sia compatibile
 col problema)
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename transf1.png
	lyxscale 50
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architettura tipica di una CNN (verde: estrattore feature, giallo: classificator
e feature
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Segmentazione di immagini
\end_layout

\begin_layout Standard
La 
\series bold
segmentazione semantica
\series default
 di un immagine ha come obiettivo quella di assegnare ad ogni pixel un'etichetta
 proveniente da un insieme fissato.
 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Segmentazione.png
	scale 43

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Segmentazione di una camera da letto
\end_layout

\end_inset


\end_layout

\end_inset

In figura 4.9 viene mostrato un esempio di segmentazione, ogni etichetta
 assegnata ad un pixel viene rappresentata come un colore differente.
 Più formalmente:
\end_layout

\begin_layout Standard
\noindent
Data un'immagine 
\shape italic
I, 
\shape default
associamo ad ogni pixel identificato dalle sue coordinate (r,c) dove r e
 c identificano riga e colonna rispettivamente, un'etichetta proveniente
 da un insieme 
\begin_inset Formula $\Lambda$
\end_inset

.
\end_layout

\begin_layout Subsection
Fully-Convolutional Neural Networks per la segmentazione semantica
\end_layout

\begin_layout Standard
Le CNN sono pensate per processare imput di dimensione fissata.
 I layer convoluzionali e di subsampling operano in maniera scorrevole sull'imma
gine, mentre i FC layers vincolano l'input a una dimensione fissata.
 Dando in pasto un'immagine di dimensione più larga alla rete, quindi, non
 si possono calcolare i punteggi delle varie classi, ma si possono comunque
 estrarre le features.
\end_layout

\begin_layout Standard
\noindent
Ad ogni modo, dato che la FCN è lineare, può essere rappresentata come una
 convoluzione, i cui pesi sono associati al neurone di output 
\begin_inset Formula $o_{i}$
\end_inset

.
 Si ha che:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
o_{i}\mathrel{=\mathop{\left(\boldsymbol{w_{i}}\mathbin{\odot}s\right)(0,0)}+}b_{i}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove 
\begin_inset Formula $\boldsymbol{w_{i}}=$
\end_inset

{
\emph on
w
\begin_inset Formula $_{i,j}\}_{i=j:N}$
\end_inset


\emph default
 sono i pesi associati a 
\emph on

\begin_inset Formula $o_{i}$
\end_inset


\emph default
.
\end_layout

\begin_layout Standard
\noindent
Quindi, dato che la FCN è lineare, può essere rappresentata come una convoluzion
e di L filtri di dimensione 1x1xN.
 Ognuno di questi filtri convoluzionali contiene i pesi della FCN per il
 corrispondente neurone in output.
 Per ogni classe in output, otteniamo quindi un'immagine (
\series bold
heatmap
\series default
) avente una risoluzione più bassa rispetto a quella dell'immagine in input
 e probabilità delle classi per il campo recettivo di ogni pixel.
\end_layout

\begin_layout Standard
\noindent
In un'immagine più grande, ogni pixel nella heatmap corrisponde a un campo
 recettivo nell'immagine in input.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename heatmap.png
	lyxscale 70
	scale 95

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename heatmap_explan.png
	lyxscale 60
	scale 52

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Esempio di heatmap (sopra), Direct Heatmap Predictions (sotto)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Le 
\series bold
Fully-Convolutional Neural Networks
\series default
 sono dunque un mezzo per effettuare predizioni su immagini di dimensione
 arbitraria.
 Ci sono varie soluzioni per segmentare le immagini tramite FCNN:
\end_layout

\begin_layout Enumerate

\series bold
Direct Heatmap Predictions
\series default
: possiamo assegnare l'etichetta predetta nella heatmap all'intero campo
 recettivo, nonostante questa sarebbe una stima molto grossolana
\end_layout

\begin_layout Enumerate

\series bold
Shift and Stich
\series default
: assumiamo che ci sia un ratio 
\emph on
f
\emph default
 tra la dimensione dell'input e la dimensione della heatmap in output.
 Compiamo i seguenti step:
\end_layout

\begin_deeper
\begin_layout Itemize
calcoliamo le heatmap per tutti i possibili shift 
\emph on

\begin_inset Formula $f^{2}$
\end_inset


\emph default
dell'input (
\begin_inset Formula $0\leq r,c<f$
\end_inset

)
\end_layout

\begin_layout Itemize
mappiamo le predizioni dalle 
\emph on

\begin_inset Formula $f^{2}$
\end_inset

 
\emph default
heatmap all'immagine (ogni pixel della heatmap fornisce la predizione del
 pixel centrale del campo recettivo)
\end_layout

\begin_layout Itemize
frapponiamo le heatmap per formare un immagine larga quanto l'input
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
solo convoluzioni
\series default
: campo recettivo piccolo e metodo molto inefficiente
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename shift_and_stich.png
	lyxscale 70
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Shift and Stich
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
La segmentazione semantica affronta una tensione tra semantica e locazione,
 ovvero si pone l'obiettivo di fare predizioni locali rispetto alla struttura
 globale.
 Un modo per fare ciò è rappresentato nella figura seguente.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename semant_segment.png
	lyxscale 70
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Rete con convoluzione e upsampling
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Come possiamo notare, la prima parte della rete è una normale CNN (convoluzioni
 + pooling/downsampling), mentre la seconda metà è pensata per fare un upsamplin
g delle predizioni per coprire ogni pixel dell'immagine (convoluzioni +
 upsampling).
\end_layout

\begin_layout Standard
\noindent
Per fare 
\series bold
l'upsampling
\series default
 possiamo usare 2 metodi:
\end_layout

\begin_layout Itemize

\series bold
Nearest Neighbor
\series default
: si riempiono tutti i pixel del layer di output col valore del corrispettivo
 pixel del layer di input
\end_layout

\begin_layout Itemize

\series bold
Bed of Nails
\series default
: si riempiono tutti i pixel del layer di output con degli 0, eccetto un
 pixel che avrà il valore del corrispettivo pixel del layer in input
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename upsampling.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
UpSampling
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Per mantenere anche le informazioni delle posizioni dei pixel, usiamo un
 meccanismo chiamato 
\series bold
Max Unpooling
\series default
 (l'inverso del Max Pooling, vedi figura sotto).
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename maxUnpooling.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Max Unpooling
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Inoltre possiamo anche fare una convoluzione trasposta, che è esattamente
 l'operazione reciproca della convoluzione.
 L'unico problema sta nei pixel che si sovrappongono, ma la soluzione consiste
 nel fare una somma pesata tra i pixel in input e la parte di filtro compresa
 nella regione di overlap.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename transpose_conv.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename transpose_conv(1).png
	lyxscale 150

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Convoluzione trasposta (stride = 2)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
I filtri di upsampling possono essere appresi durante il training con un'inizial
izzazione uguale all'interpolazione bilineare.
 Le predizioni derivate dall'upsampling sono comunque troppo grossolane.
 La soluzione a questo problema è quella di usare delle 
\series bold
Skip Connections
\series default
.
 Queste integrano una tradizionale rete di contrazione dove la convoluzione
 è rimpiazzata dalla convoluzione trasposta.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename skip_connections.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Skip connections
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Metodi per allenare una FCNN
\end_layout

\begin_layout Enumerate

\series bold
Patch-Based
\series default
: eseguiamo i seguenti step:
\end_layout

\begin_deeper
\begin_layout Itemize
prepariamo un set di training per una rete di classificazione
\end_layout

\begin_layout Itemize
ritagliamo delle patch 
\begin_inset Formula $x_{i}$
\end_inset

 dalle immagini e assegnamo a ogni patch l'etichetta corrispondente al punto
 centrale della patch
\end_layout

\begin_layout Itemize
alleniamo una CNN per la classificazione da zero o facciamo fine-tuning
 di un modello pre-allenato sulle classi di segmentazione
\end_layout

\begin_layout Itemize
spostiamo i FC layers nelle convoluzioni 1x1
\end_layout

\begin_layout Itemize
disegnamo la parte di upsampling della rete e ne alleniamo i filtri
\end_layout

\begin_layout Itemize
la rete di classificazione è allenata per minimizzare l'errore di classificazion
e 
\emph on
l
\emph default
 su un mini-batch
\begin_inset Formula 
\begin{equation}
\hat{\theta}=min_{\theta}\sum_{x_{j}}l(\boldsymbol{x_{j}},\theta)
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
i batch sono poi randomicamente assemblati durante il training
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Full-Image
\series default
: è possibile allenare direttamente una FCNN che include i layer di upsampling,
 quindi il learning diventa la seguente minimizzazione 
\begin_inset Formula 
\begin{equation}
min_{x_{j}\in I}\sum_{x_{j}}l(\boldsymbol{x_{j}},\theta)
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
dove 
\emph on

\begin_inset Formula $\boldsymbol{x_{j}}$
\end_inset


\emph default
sono i pizel in una regione dell'immagine di input e la funzione di errore
 è valutata sulle etichette corrispondenti nell'annotazione.
 Tale metodo è chiamato anche ''
\shape italic
metodo end-to-end
\shape default
'' ed è più efficiente del Patch-Based.
\end_layout

\end_deeper
\begin_layout Subsection
U-Net
\end_layout

\begin_layout Standard
La 
\series bold
U-Net
\series default
 è formata da una parte contrattiva e una parte espansiva e non ha FC layers
 (è 
\series bold
simmetrica
\series default
).
 Essa usa un gran numero di feature maps nella parte di downsampling e usa
 molta data augmentation.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UNet.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
U-Net
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Nella parte di 
\series bold
contrazione
\series default
, la rete ripete blocchi aventi:
\end_layout

\begin_layout Itemize
2 layer convoluzionali 3x3 + ReLU (opzione 'valid', no padding)
\end_layout

\begin_layout Itemize
maxpooling 2x2
\end_layout

\begin_layout Standard
A ogni downsampling, inoltre, il numero di feature maps è raddoppiato.
 La particolarità di questa rete è che implementa le skip connections, tramite
 le quali aggrega tramite concatenazione l'ultimo layer convoluzionale,
 per ogni blocco nella rete di contrazione, col primo layer convoluzionale
 del corrispettivo blocco della rete di espansione.
 
\end_layout

\begin_layout Standard
\noindent
Nella parte di 
\series bold
espansione
\series default
, la rete ripete blocchi aventi:
\end_layout

\begin_layout Itemize
convoluzione trasposta 2x2, dimezzando il numero di feature maps (ma raddoppiand
o la risoluzione spaziale)
\end_layout

\begin_layout Itemize
concatenazione delle cropped features corrispondenti
\end_layout

\begin_layout Itemize
2 layer convoluzionali 3x3 + ReLU
\end_layout

\begin_layout Standard
Il risultato è un'immagine in output più piccola di quella in input.
\end_layout

\begin_layout Standard
\noindent
La rete U-Net usa il metodo di training Full-Image con 
\series bold
una funzione di errore
\series default
 pesata (
\series bold
NB
\series default
.
 È la funzione appositamente formulata per la segmentazioni di immagini
 biomediche per cui U-Net è stata pensata 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://arxiv.org/pdf/1505.04597.pdf
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{\theta}=min_{\theta}\sum_{x_{j}}w(\boldsymbol{x_{j}})log(\boldsymbol{x_{j}},\theta)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove il peso è dato dalla relazione
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w(\boldsymbol{x})=w_{c}(\boldsymbol{x})+w_{0}e^{-\frac{(d_{1}(\boldsymbol{x})+d_{2}(\boldsymbol{x}))^{2}}{2\sigma^{2}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In quest'ultima relazione, 
\begin_inset Formula $w_{c}$
\end_inset

è usato per bilanciare le proporzioni delle classi, 
\begin_inset Formula $d_{1}$
\end_inset

è la distanza tra il bordo e la cellula più vicina, mentre 
\begin_inset Formula $d_{2}$
\end_inset

è la distanza tra il bordo e la seconda cellula più vicina (i pesi sono
 più grandi quando la distanza dalle due celle più vicine al bordo è piccola).
 Inoltre, il primo termine dell'equazione tiene conto dello sbilanciamento
 delle classi nel training set, mentre il secondo migliora la performance
 della classificazione ai bordi di oggetti diversi tra loro.
\end_layout

\begin_layout Subsection
Global Averaging Pooling
\end_layout

\begin_layout Standard
L'idea principale consiste nell'usare, anzichè le convoluzioni tradizionali,
 uno stack di convoluzioni 1x1 + ReLU, che corrisponde alle reti di MLP
 usate in maniera scorrevole su tutta l'immagine.
 Viene quindi introdotto il 
\series bold
Global Averaging Pooling layer
\series default
, che calcola la media di ogni feature map al posto del FC layer alla fine
 della rete e predire usando una semplice softmax.
 L'importante è che il numero di feature maps sia uguale al numero di classi
 in output.
 Tra i vantaggi di questo layer vi è quello di riuscire a classificare immagini
 di dimensioni diverse.
\end_layout

\begin_layout Standard
\noindent
In pratica è come se avessimo una
\series bold
 rete nella rete (NiN)
\series default
 composta da:
\end_layout

\begin_layout Itemize
MLP convolutional layers + ReLU + dropout
\end_layout

\begin_layout Itemize
maxpooling
\end_layout

\begin_layout Itemize
GAP layer
\end_layout

\begin_layout Itemize
softmax
\end_layout

\begin_layout Standard
Delle semplici NiN raggiungono la miglior performance su piccoli dataset
 grazie al fatto che il GAP riduce efficacemente l'overfitting rispetto
 al FC.
 Possiamo quindi affermare che il GAP agisce come un regolarizzatore strutturale.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename GAP.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Network in Network
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Weakly-Supervised Localization
\end_layout

\begin_layout Standard
\noindent
La 
\series bold
Weakly-Supervised Localization
\series default
 effettua una localizzazione su un'immagine senza box di contorno annotati.
 Il training set è fornito come per la classificazione con coppie <immagine,
 etichetta> (
\emph on
I
\emph default
, 
\emph on
l
\emph default
) dove non è fornita alcun'informazione sulla localizzazione.
\end_layout

\begin_layout Paragraph
CAM
\end_layout

\begin_layout Standard
\noindent
I vantaggi del GAP layer vanno oltre al semplice agire come regolarizzatore
 che previene l'overfitting.
 Infatti, la rete può conservare una notevole abilità di localizzazione
 fino al layer finale.
 Una CNN allenata sulla categorizzazione di un oggetto è quindi capace di
 localizzare con successo le regioni discriminative per la classificazione
 contenenti gli oggetti con cui gli umani interagiscono piuttosto che gli
 umani stessi.
 Ciò viene fatto tramite una tecnica chiamata 
\series bold
Class Activation Mapping (CAM)
\series default
, che permette di identificare quali regioni di un'immagine verranno usate
 per la discriminazione.
 Essa richiede solo un FC layer dopo il GAP e una piccola regolarizzazione.
 Tale layer calcola lo score 
\begin_inset Formula $S_{c}$
\end_inset

 per ogni classe 
\emph on
c
\emph default
 come 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S_{c}=\sum_{k}w_{k}^{c}F_{k}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove 
\begin_inset Formula 
\begin{equation}
F_{k}=\sum_{(x,y)}f_{k}(x,y)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename CAM(1).png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
CAM
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Dopodichè, calcola la probabilità 
\emph on

\begin_inset Formula $P_{c}$
\end_inset

 
\emph default
della classe 
\emph on
c 
\emph default
come
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P_{c}=\frac{e^{S_{c}}}{\sum_{i}e^{S_{i}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Ad ogni modo, unendo la (128) e la (129), e sapendo che il CAM è definito
 come
\begin_inset Formula 
\begin{equation}
M_{c}(x,y)=\sum_{k}w_{k}^{c}f_{k}(x,y)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
otteniamo che
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S_{c}=\sum_{(x,y)}M_{c}(x,y)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove 
\begin_inset Formula $M_{c}(x,y)$
\end_inset

 indica direttamente l'importanza delle attivazioni a (x,y) per la classe
 
\emph on
c
\emph default
.
 Inoltre, grazie alla softmax, la profondità dell'ultimo layer convoluzionale
 può essere diversa dal numero di classi.
 Potrebbe anche essere necessario un upsampling per poter matchare l'immagine
 in input (vedi figura sotto).
 I pesi rappresentano quindi l'importanza di ogni feature map per portare
 alla predizione finale.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename CAM.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Class Activation Mapping
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Grad-CAM
\end_layout

\begin_layout Standard
\noindent
CAM ha il grande svantaggio di dover modificare la rete per poter predire
 un'altra classe.
 Una tecnica che risolve questo problema è la 
\series bold
Grad-CAM
\series default
, la quale usa il 
\series bold
gradiente
\series default
 per calcolare la heatmap a bassa risoluzione 
\begin_inset Formula $\boldsymbol{g^{c}}$
\end_inset

, dove 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{g^{c}=}RELU\left(\sum_{k}w_{k}^{c}\boldsymbol{a_{k}}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w_{k}^{c}=\frac{1}{nm}\sum_{i}\sum_{j}\frac{\partial y_{c}}{\partial\boldsymbol{a_{k}}(\boldsymbol{i},\boldsymbol{j})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Grad-CAM.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Grad-CAM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Augmented Grad-CAM
\end_layout

\begin_layout Standard
\noindent
Consideriamo l'operatore di augmentation 
\begin_inset Formula $\mathcal{A_{\mathit{l}}}:\mathbb{R^{\mathrm{N\mathbin{x}M}}\rightarrow\mathbb{R^{\mathrm{N\mathbin{x}M}}}}$
\end_inset

, che include rotazioni e traslazioni randomiche dell'immagine di input
 
\series bold
x
\series default
.
 L
\series bold
'Augmented Grad-CAM
\series default
 incrementa la risoluzione delle heatmap tramite 
\series bold
image augmentation
\series default
.
 Tutte le risposte generate dalla CNN alle versioni multiple augmentate
 della stessa immagine di input hanno molte informazioni per la ricostruzione
 della heatmap 
\series bold
h
\series default
 ad alta risoluzione.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename aug-grad-cam.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Augmented Grad-CAM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Viene fatta la cosiddetta 
\series bold
Super-Resolution (SR)
\series default
 della heatmap sfruttando le informazioni condivise nelle heatmap a bassa
 risoluzione calcolate dallo stess input sotto diverse, ma note, trasformazioni.
 Le CNN sono in generale invarianti alle rototraslazioni, in termini di
 predizioni, ma ogni heatmap a bassa risoluzione
\series bold
 
\begin_inset Formula $\boldsymbol{g_{l}}$
\end_inset


\series default
 contiene di fatto diverse informazioni.
 Modelliamo le heatmap calcolate col Grad-CAM come risultato di un operatore
 di downsampling sconosciuto 
\begin_inset Formula $\mathcal{\boldsymbol{D}}:\mathbb{R}^{\mathrm{N\mathbin{x}M}}\rightarrow\mathbb{R^{\mathrm{N\mathbin{x}M}}}$
\end_inset

.
 L'alta risoluzione della heatmap 
\series bold
h
\series default
 è recuperata risolvendo un problema inverso
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
argmin_{h}\frac{1}{2}\sum_{l=1}^{L}\bigparallel\mathcal{DA\mathit{_{l}}\mathrm{\boldsymbol{h}-g_{\mathit{l}}\bigparallel}_{\mathrm{2}}^{\mathrm{2}}}+\lambda TV_{l_{1}}(\boldsymbol{h})+\frac{\mu}{2}\bigparallel\boldsymbol{h}\bigparallel{}_{\mathrm{2}}^{\mathrm{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove 
\begin_inset Formula 
\begin{equation}
TV_{l_{1}}(\boldsymbol{h})=\sum_{i,j}\bigparallel\partial_{x}\boldsymbol{h}(i,j)\bigparallel+\bigparallel\partial_{y}\boldsymbol{h}(i,j)\bigparallel
\end{equation}

\end_inset

è la 
\series bold
regolarizzazione Anistropic Total Variation
\series default
 usata per preservare i bordi della heatmap target ad alta risoluzione.
 Questo viene risolto tramite 
\series bold
Subgradient Descent
\series default
 (
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://en.wikipedia.org/wiki/Subderivative#The_subgradient
\end_layout

\end_inset

) dato che la funzione è convessa e non-smooth (non differenziabile e discontinu
a).
\end_layout

\begin_layout Section
Localizzazione
\end_layout

\begin_layout Subsection
R-CNN
\end_layout

\begin_layout Standard
Per quanto riguarda il task del riconoscimento degli oggetti (
\series bold
object detection
\series default
), una possibile soluzione potrebbe essere quella di usare una finestra
 scorrevole (
\series bold
sliding window
\series default
) che ha un’etichetta assegnata al pixel centrale e consente di analizzare
 un’immagine di dimensioni variabili a piccoli pezzi di dimensioni fisse.
 Tali “pezzi”, cioè regioni di immagine, vengono poi date in pasto a un
 modello preallenato per effettuare predizioni sulle stesse.
 Questo approccio ha, però, vari contro, tra cui il fatto che non riutilizza
 features condivise fra regioni sovrapposte.
 Una possibile idea per risolvere questo problema è usare un algoritmo a
 proposta di regione (
\series bold
region proposal algorithm
\series default
), che classifica tramite una CNN l’immagine dentro ogni regione proposta.
 Tale rete prende quindi il nome di R-CNN (dove R sta per 
\series bold
regioni
\series default
).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename rcnn.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Funzionamento R-CNN
\end_layout

\end_inset


\end_layout

\end_inset

Lo step 4 viene performato da un regressore 
\series bold
Support Vector Machine (SVM)
\series default
, allenato per minimizzare l’errore di classificazione sulla regione di
 interesse (Region of Interest, 
\series bold
ROI
\series default
) estratta e da un regressore 
\series bold
Bounding Box (BB)
\series default
, che rifinisce le regioni correggendo la stima del bounding box dall’algoritmo
 di estrazione delle ROI.
 La CNN pre allenata, invece, è fine-tunata sulle classi da rilevare inserendo
 un FC layer dopo l’estrazione delle feature.
 Per scartare le regioni che non corrispondono a nessun oggetto, inoltre,
 va inclusa la classe di background.
\end_layout

\begin_layout Subsection
Fast R-CNN
\end_layout

\begin_layout Standard
Le R-CNN hanno delle limitazioni, fra cui la lentezza computazionale, perciò
 sono state introdotte le Fast R-CNN, nelle quali:
\end_layout

\begin_layout Enumerate
Le immagini intere sono date in pasto alla CNN che estrae le feature maps.
\end_layout

\begin_layout Enumerate
Le proposte di regione sono identificate a partire dall’immagine e proiettate
 nelle feature maps.
 Inoltre, le regioni sono direttamente ritagliate dalle feature maps anziché
 dall’immagine (si riusa la computazione convoluzionale).
\end_layout

\begin_layout Enumerate
Una dimensione fissata è comunque richiesta per poter dare i dati in pasto
 a un FC layer.
 Dopodichè i layer di ROI pooling estraggono un vettore di feature di dimensione
 fissa H x W da ogni regione proposta.
 In pratica ogni ROI nelle feature maps è diviso in una griglia H x W su
 cui viene fatto un maxpooling per ottenere il feature vector (vettore di
 feature).
\end_layout

\begin_layout Enumerate
I layer FC stimano sia le classi che la posizione dei bounding box (ovvero,
 come nel regressore BB).
 Viene poi usata una combinazione convessa (cioè una combinazione lineare
 con termini aventi coefficienti non-negativi la cui somma è 1) dei due
 come loss multitask da ottimizzare (come nelle R-CNN, ma senza l’uso di
 regressori SVM).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fast-r-cnn.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Funzionamento Fast R-CNN
\end_layout

\end_inset


\end_layout

\end_inset

In questa architettura è quindi possibile retro-propagare l’errore attraverso
 l’intera rete, in modo che essa venga allenata in maniera end-to-end.
 Dal momento che le convoluzioni non sono ripetute su aree sovrapposte,
 la stragrande maggioranza del tempo di test è speso sull’estrazione delle
 ROI.
\end_layout

\begin_layout Subsection
Faster R-CNN
\end_layout

\begin_layout Standard
Invece dell’algoritmo di estrazione delle ROI, le 
\series bold
Faster R-CNN
\series default
 allenano una 
\series bold
Region Proposal Network (RPN)
\series default
, la quale è una Fully-CNN.
 La RPN opera sulle stesse feature maps usate per la classificazione, cioè
 negli ultimi layer convoluzionali.
 La RPN può quindi essere vista come un modulo aggiuntivo che migliora l’efficie
nza e concentra la Fast R-CNN sulle regioni più promettenti per il riconosciment
o degli oggetti.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename faster-r-cnn.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RPN
\end_layout

\end_inset


\end_layout

\end_inset

Il suo scopo è quello di associare a ogni locazione spaziale k box “ancora”,
 ovvero ROI aventi scale e ratio differenti.
 Tale rete produrrà in output delle ancore candidate di dimensione H x W
 x k insieme a dei punteggi di stima per ogni ancora.
 Il layer intermedio di una RPN è un layer convoluzionale standard che prende
 come input l’ultimo layer della rete di estrazione delle feature e usa
 un filtro di dimensione 
\begin_inset Formula $3\times3\times256$
\end_inset

.
 Tale layer è usato per ridurre la dimensionalità della feature map, cioè
 mappa una certa regione a un vettore di dimensione inferiore 
\begin_inset Formula $H\times W\times256$
\end_inset

.
 Vi sono poi 2 layer, cioè:
\end_layout

\begin_layout Itemize
La 
\series bold
rete di classificazione
\series default
: allenata per predire la 
\series bold
probabilità dell’oggetto
\series default
, cioè la probabilità che una certa ancora contenga un oggetto (dovrà quindi
 produrre 2k punteggi in output, ovvero k coppie con probabilità <contiene,
 non contiene>).
 Essa è fatta da uno stack di layer convoluzionali di dimensione 1x1.
 Ognuna delle k coppie di probabilità corrisponde a un’ancora specifica
 ed esprime la probabilità che quell’ancora, in quella locazione spaziale,
 contenga un oggetto.
\end_layout

\begin_layout Itemize
La 
\series bold
rete di regressione
\series default
: allenata per aggiustare ognuna delle k ancore predette (deve quindi fare
 4k stime, ovvero una stima per ognuna delle 4 coordinate del bounding box
 e per ogni bounding box).
\end_layout

\begin_layout Standard
Abbiamo detto che la RPN restituisce in outut 
\begin_inset Formula $H\times W\times k$
\end_inset

 proposte di regioni, quindi rimpiazza l’algoritmo di proposta di regioni
 (
\series bold
selective search
\series default
).
 Nelle Faster R-CNN, dopo la RPN vi è una soppressione dei non-massimi basata
 su punteggi di oggettività.
 Le proposte rimanenti sono quindi date in pasto al layer di ROI pooling
 e infine classificate dall’architettura Fast R-CNN standard.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename faster-r-cnn_2.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Faster R-CNN
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

La procedura di training è la seguente:
\end_layout

\begin_layout Enumerate
Allenare la RPN mantenendo congelata la rete strutturale (Deep ConvNet)
 e allenando solo i layer RPN.
\end_layout

\begin_layout Enumerate
Si allena la Fast R-CNN usando le proposte di regioni restituite dalla RPN
 allenata nello step precedente, cioè si fa fine-tuning dell’intera Fast
 R-CNN includendo la Deep ConvNet.
\end_layout

\begin_layout Enumerate
Si fa fine-tuning in cascata della RPN.
\end_layout

\begin_layout Enumerate
Si congela la Deep ConvNet e si fa fine-tuning solo degli ultimi layer della
 Faster R-CNN.
\end_layout

\begin_layout Section
Altri esempi di architetture
\end_layout

\begin_layout Subsection
YOLO (You Only Look Once)
\end_layout

\begin_layout Standard
YOLO è un metodo 
\shape italic
region-free
\shape default
, in cui riformuliamo il riconoscimento degli oggetti come un problema di
 regressione singolo che parte dai pixel dell’immagine alle coordinate dei
 bounding box e alle probabilità delle classi.
 Esso risolve questi problemi di regressione in una volta sola, tramite
 una CNN larga.
\end_layout

\begin_layout Standard
Gli step su cui si basa YOLO sono: 
\end_layout

\begin_layout Enumerate
Dividere l’immagine in una griglia grossolana (di dimensione N x M).
 
\end_layout

\begin_layout Enumerate
Ogni griglia contiene B ancore (
\series bold
bounding box base
\series default
) associate.
 
\end_layout

\begin_layout Enumerate
Per ogni cella e per ogni ancora prediciamo:
\end_layout

\begin_deeper
\begin_layout Enumerate
l’offset del bounding box base: (
\shape italic
dx, dy, dh, dw, punteggio dell’oggettività
\shape default
).
 
\end_layout

\begin_layout Enumerate
il punteggio di classificazione del bounding box base sulle C categorie
 considerate (includendo lo sfondo).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename yolo.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset

L’output della rete avrà quindi dimensione: 
\begin_inset Formula $N\times M\times B\times(5+C)$
\end_inset

.
 L’intera predizione è fatta in un singolo passo di forward sull’immagine
 e da una singola rete convoluzionale.
 Ciò lo rende molto veloce, ma meno accurato rispetto alle R-CNN.
\end_layout

\begin_layout Subsection
Mask R-CNN
\end_layout

\begin_layout Standard
La segmentazione d’istanza combina le sfide di rilevamento degli oggetti
 (istanze multiple presenti nell’immagine) e di segmentazione semantica
 (etichette associate a ogni pixel).
 Un’architettura che risolve tale problema è la Mask R-CNN.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename mask_rccn.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Mask R-CNN
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Come nelle Fast R-CNN, viene classificata l’intera ROI e viene fatta una
 regressione sul bounding box (possibilmente stimando la posa degli oggetti).
 Dentro ogni ROI viene poi fatta, per ogni classe, una segmentazione semantica
 tramite la stima di una maschera (vedi figura sotto).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename out_mask_rcnn.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Output mask R-CNN
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Reti Ricorrenti
\end_layout

\begin_layout Section
Modellazione sequenziale
\end_layout

\begin_layout Standard
Finora abbiamo considerato solo dataset 
\begin_inset Quotes fld
\end_inset

statici
\begin_inset Quotes frd
\end_inset

.
 Ora invece ci concentreremo su dataset 
\begin_inset Quotes fld
\end_inset


\shape italic
dinamici
\shape default

\begin_inset Quotes frd
\end_inset

 in due modi:
\end_layout

\begin_layout Itemize

\series bold
modelli senza memoria
\series default
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
modelli autoregressivi
\series default
: predicono il prossimo input a partire dal precedente sfruttando dei 
\begin_inset Quotes fld
\end_inset

ritardi
\begin_inset Quotes frd
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Feed Forward Neural Networks
\series default
: generalizzano i modelli autoregressivi usando layer nascosti non-lineari
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename autoregressive.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Modello autoregressivo (sopra), FFNN (sotto)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
modelli con memoria
\series default
 (sistemi dinamici lineari, Hidden Markov models, Recurrent Neural Networks)
\end_layout

\begin_layout Standard
I 
\series bold
sistemi dinamici
\series default
 sono modelli generativi con uno stato nascosto che non può essere osservato
 direttamente.
 Tale stato ha delle dinamiche che possono essere affette da rumore e produce
 l'output.
\end_layout

\begin_layout Subsection
Sistemi dinamici lineari
\end_layout

\begin_layout Standard
\noindent
Nei 
\series bold
sistemi dinamici lineari
\series default
 ciò si traduce nell'avere uno stato continuo con incertezza Gaussiana che
 può essere stimato usando il 
\series bold
Kalman filtering
\series default
.
 Le trasformazioni sono assunte essere lineari.
\end_layout

\begin_layout Subsection
Hidden Markov models
\end_layout

\begin_layout Standard
Negli 
\series bold
HMM
\series default
 lo stato è assunto come discreto e può essere stimato tramite 
\series bold
l'algoritmo di Viterbi
\series default
, mentre le transizioni sono 
\shape italic
stocastiche
\shape default
 (si usa la cosiddetta ''matrice di transizione'').
 L'output è una funzione stocastica degli stati nascosti.
\end_layout

\begin_layout Subsection
Recurrent Neural Networks
\end_layout

\begin_layout Standard
Le 
\series bold
reti neurali ricorrenti
\series default
 implementano la memoria tramite delle connessioni chiamate 
\series bold
connessioni ricorrenti
\series default
.
 Inoltre, gli stati distribuiti consentono di salvare efficentemente le
 informazioni, mentre le dinamiche non-lineari consentono di aggiornare
 stati nascosti complessi.
 
\end_layout

\begin_layout Standard
\noindent

\emph on
''Con abbastanza neuroni e tempo, le RNN possono calcolare qualsiasi cosa
 che possa essere calcolata da un computer.'' 
\emph default
(Computation Beyond the Turing Limit, Hava T.
 Siegelmann, 1995)
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename RNN(1).png
	lyxscale 120

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Rete neurale ricorrente
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Nella figura sopra sono raffigurati due tipi di neuroni: quelli grigi, che
 dipendono dall'
\series bold
input corrente
\series default
, e quelli blu, che dipendono dalla 
\series bold
storia della rete
\series default
.
 Inoltre abbiamo dei parametri nuovi chiamati:
\end_layout

\begin_layout Itemize
\begin_inset Formula $V$
\end_inset

 rappresenta i pesi tra neuroni blu e neuroni grigi
\end_layout

\begin_layout Itemize
\begin_inset Formula $V_{B}$
\end_inset

 rappresenta i pesi tra i neuroni blu
\end_layout

\begin_layout Itemize
\begin_inset Formula $W$
\end_inset

 rappresenta i pesi tra neuroni grigi
\end_layout

\begin_layout Itemize
\begin_inset Formula $W_{B}$
\end_inset

 rappresenta i pesi tra neuroni grigi e neuroni blu
\end_layout

\begin_layout Standard
Le funzioni di output dei neuroni sono
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{equation}
h_{j}^{t}(\cdot)=h_{j}^{t}\left(\sum_{j=0}^{J}w_{ji}^{(1)}\cdot x_{i,n}+\sum_{b=0}^{B}v_{jb}^{(1)}\cdot c_{b}^{t-1}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{equation}
c_{b}^{t}(\cdot)=c_{b}^{t}\left(\sum_{j=0}^{J}w_{bi}^{(1)}\cdot x_{i,n}+\sum_{b'=0}^{B}v_{bb'}^{(1)}\cdot c_{b'}^{t-1}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
mentre la funzione di output della rete è
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g^{t}(x_{n}|w)=g\left(\sum_{j=0}^{J}w_{1j}^{(2)}\cdot h_{j}^{t}(\cdot)+\sum_{b=0}^{B}v_{1b}^{(2)}\cdot c_{b}^{t}(\cdot)\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
La rete nascosta è chiamata 
\series bold
context network
\series default
 e contiene 
\emph on
B
\emph default
 neuroni, mentre la rete ''visibile'' ne ha 
\emph on
J
\emph default
.
 La context network è la parte più difficile da allenare in quanto non è
 una FFNN.
 
\end_layout

\begin_layout Standard
\noindent
La tecnica usata per allenarla è chiamata 
\series bold
Backpropagation Through Time
\series default
.
 Gli step che la compongono sono:
\end_layout

\begin_layout Enumerate
dispiegare (unfold) la RNN per 
\emph on
U
\emph default
 step temporali, ottenendo una FFNN: sia 
\emph on
N 
\emph default
una RNN che deve apprendere un task temporale a partire dal tempo 
\emph on
t-u 
\emph default
fino al tempo 
\emph on
t
\emph default
, e sia 
\emph on
N* 
\emph default
la FFNN che risulta dall'unfold della rete 
\emph on
N:
\end_layout

\begin_deeper
\begin_layout Itemize
per ogni istante di tempo nell'intervallo 
\begin_inset Formula $(t-u,t]$
\end_inset

 la rete 
\emph on
N* 
\emph default
ha un layer contenente 
\emph on
k
\emph default
 neuroni, dove 
\emph on
k
\emph default
 è il numero di neuroni di 
\emph on
N
\end_layout

\begin_layout Itemize
per ogni layer di 
\emph on
N*
\emph default
 c'è una copia di ogni neurone in 
\emph on
N
\end_layout

\begin_layout Itemize
per ogni istante di tempo 
\begin_inset Formula $\tau\in(t-u,t]$
\end_inset

 il peso sinaptico dal neurone 
\emph on
i 
\emph default
nel layer 
\emph on

\begin_inset Formula $\tau$
\end_inset

 
\emph default
al neurone 
\emph on
j 
\emph default
nel layer 
\emph on

\begin_inset Formula $\tau+1$
\end_inset

 
\emph default
di 
\emph on
N* 
\emph default
è una copia della connessione sinaptica dal neurone 
\emph on
i 
\emph default
al neurone 
\emph on
j
\emph default
 in 
\emph on
N
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename backprop_through_time.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Unfolding della rete
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
inizializzare le repliche di 
\begin_inset Formula $W_{B}$
\end_inset

e 
\begin_inset Formula $V_{B}$
\end_inset

in modo da renderle uguali
\end_layout

\begin_layout Enumerate
fare una backpropagation su tutta la nuova rete: tutti i pesi sono allenati
 col gradient descent, cioè considerando un istante di tempo generico 
\begin_inset Formula $\tau$
\end_inset

.
 Applicando il solito metodo di aggiornamento, avremo valori diversi per
 gli stessi pesi in diversi istanti di tempo.
 Ciò non è conveniente perchè di fatto sono gli stessi pesi, quindi poi
 se ne fa la media dopo averli aggiornati sulla base del gradient descent.
 Una tecnica alternativa possibile è quella di aggiornare i pesi con la
 media del gradiente, ottenendo quindi gli stessi risultati ma il processo
 sarà più efficiente.
 La formula di aggiornamento per i pesi da fornire alla context network
 è:
\begin_inset Formula 
\begin{equation}
W_{B}=W_{B}-\eta\cdot\frac{1}{U}\sum_{0}^{U-1}\frac{\partial E}{\partial W_{B}^{t-u}}
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Mentre per mantenere la memoria del contesto si usa:
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
V_{B}=V_{B}-\eta\cdot\frac{1}{U}\sum_{0}^{U-1}\frac{\partial E^{t}}{\partial V_{B}^{t-u}}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Vanishing Gradient
\end_layout

\begin_layout Standard
Le RNN non riescono ad andare indietro nel passato per più di 10 step a
 causa del cosiddetto effetto del 
\series bold
Vanishing Gradient
\series default
.
 Per spiegarlo proviamo a semplificare la RNN come nella figura sotto.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename RNN_simple.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
La backpropagation su un'intera sequenza 
\emph on
S
\emph default
 è calcolata come 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial E}{\partial w}=\sum_{t=1}^{S}\frac{\partial E^{t}}{\partial w}=\sum_{t=1}^{S}\frac{\partial E^{t}}{\partial y^{t}}\frac{\partial y^{t}}{\partial h^{t}}\frac{\partial h^{t}}{\partial h^{k}}\frac{\partial h^{k}}{\partial w}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial h^{t}}{\partial h^{k}}=\prod_{i=k+1}^{t}\frac{\partial h_{i}}{\partial h_{i-1}}=\prod_{i=k+1}^{t}v^{(1)}g'\left(h^{i-1}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Considerando la norma di questi termini
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\Vert\frac{\partial h_{i}}{\partial h_{i-1}}\Vert=\parallel v^{(1)}\Vert\Vert g'\left(h^{i-1}\right)\Vert
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
otteniamo che
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\Vert\frac{\partial h^{t}}{\partial h^{k}}\Vert\leq\left(\gamma_{v}\gamma_{g}^{'}\right)^{t-k}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove se 
\begin_inset Formula $\left(\gamma_{v}\gamma_{g}^{'}\right)<1$
\end_inset

 allora la norma converge a 0.
 In particolare 
\begin_inset Formula $\gamma_{v}$
\end_inset

 è la norma dei pesi e 
\begin_inset Formula $\gamma_{g}^{'}$
\end_inset

 è la norma della derivata 
\begin_inset Formula $g^{'}$
\end_inset

 e vale 1 se usiamo una ReLU.
 Usando sigmoide o Tanh otteniamo il cosiddetto 
\series bold
Vanishing Gradient
\series default
, che dimostra che più si va indietro nel passato, più il gradiente tenderà
 a 0 indipendentemente dall'errore.
 Basandosi sullo stesso principio, se 
\begin_inset Formula $v^{(1)}>1$
\end_inset

 il gradiente incrementerà drammaticamente fino a risultare in un'
\begin_inset Quotes fld
\end_inset


\shape italic
esplosione
\shape default

\begin_inset Quotes frd
\end_inset

 che colpirà la capacità di apprendimento della rete.
\end_layout

\begin_layout Standard
\noindent
Per risolvere questo problema si può usare la ReLU, che forza tutti i gradienti
 ad essere 0 o 1.
 Infatti ricordiamo che:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
ReLU(x)=f(x)=max(0,x)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f'(x)=1_{x>0}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Il trick sta quindi nel costruire la RNN usando piccoli moduli fatti apposta
 per ricordare valori per tanto tempo.
\end_layout

\begin_layout Section
Long Short-Term Memories
\end_layout

\begin_layout Standard
Nel 1997, Hochreiter&Schmidhuber hanno risolto il problema del vanishing
 gradient disegnando una 
\series bold
cella di memoria
\series default
 che usa unità logistiche e lineari con interazioni moltiplicative.
 L'informazione entra nella cella quando il suo gate di 
\emph on
''write''
\emph default
 è on, rimane nella cella fintantochè il suo gate di 
\emph on
''keep'' 
\emph default
è on, e viene infine letta dalla cella mettendo a on il suo gate
\emph on
 
\emph default
di 
\emph on
''read''
\emph default
.
 
\end_layout

\begin_layout Standard
\noindent
Il problema viene quindi risolto poichè il loop ha un peso fissato, cioè
 non vanno imparati i pesi della parte di rete ''unfoldata'' dato che valgono
 1.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename LSTM.png
	lyxscale 80
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
La 
\series bold
LSTM
\series default
 è composta da:
\end_layout

\begin_layout Itemize
un 
\series bold
input gate
\series default
 (ci dice quanto teniamo in memoria)
\begin_inset Formula 
\begin{equation}
i_{t}=\sigma\left(W_{i}\cdot\left[h_{t-1},x_{t}\right]+b_{i}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\tilde{C}_{t}=tanh\left(W_{C}\cdot\left[h_{t-1},x_{t}\right]+b_{C}\right)
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename input_gate.png
	scale 90

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Input gate
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
un 
\series bold
forget gate
\series default
 
\begin_inset Formula 
\begin{equation}
f_{t}=\sigma\left(W_{f}\cdot\left[h_{t-1},x_{t}\right]+b_{f}\right)
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename forget_gate.png
	scale 90

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Forget gate
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
un 
\series bold
memory gate
\series default

\begin_inset Formula 
\begin{equation}
C_{t}=f_{t}\ast C_{t-1}+i_{t}\ast\tilde{C_{t}}
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename memory_gate.png
	scale 90

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Memory gate
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
un 
\series bold
output gate
\series default

\begin_inset Formula 
\begin{equation}
o_{t}=\sigma\left(W_{o}\cdot\left[h_{t-1},x_{t}\right]+b_{o}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
h_{t}=o_{t}\ast tanh(C_{t})
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename output_gate.png
	scale 90

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Output gate
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Possiamo quindi costruire un grafo computazionale con trasformazioni continue
 come nella figura sotto.
 In pratica le LSTM funzionano come i layer convoluzionali nelle CNN perchè
 sono una sorta di 
\shape italic
estrattori di features
\shape default
.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename LSTM(1).png

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
LSTM in sequenza (le parti nei riquadri neri sono i layer nascosti)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Un altro approccio possibile consiste nell'usare reti 
\series bold
LSTM bidirezionali
\series default
 che sfruttano l'intera sequenza in input.
 Sono composte da una RNN che attraversa l'intera sequenza da sinistra a
 destra e da una RNN che attraversa l'intera sequenza da destra a sinistra.
 Queste due RNN vengono poi concatenate per essere usate come rappresentazione
 delle features.
 Per inizializzarle bisogna specificare lo stato iniziale trattandolo come
 parametro da apprendere.
 L'inizializzazione può essere fatta a 0 oppure a con valori randomici.
 Dopodichè si inizia a predirre randomicamente i valori dello stato iniziale,
 si backpropaga l'errore di predizione nel tempo fino allo stato iniziale
 e si calcola il gradiente dell'errore rispetto ai nuovi valori dello stato
 iniziale.
\end_layout

\begin_layout Paragraph
Gated Recurrent Unit
\end_layout

\begin_layout Standard
La 
\series bold
GRU
\series default
 combina i gate di input e forget in un unico ''
\series bold
update gate
\series default
'' e fa un merge tra lo stato della cella e lo stato nascosto (più altri
 cambiamenti).
 Le parti che caratterizzano tale unità sono:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
z_{t}=\sigma\left(W_{z}\cdot\left[h_{t-1},x_{t}\right]\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
r_{t}=\sigma\left(W_{r}\cdot\left[h_{t-1},x_{t}\right]\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\tilde{h_{t}}=tanh\left(W\cdot\left[r_{t}\ast h_{t-1},x_{t}\right]\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h_{t}=(1-z_{t})\ast h_{t-1}+z_{t}\ast\tilde{h_{t}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename GRU.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
GRU
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
La GRU può essere usata in alternativa alla LSTM.
\end_layout

\begin_layout Section
Modellazione Sequence to Sequence
\end_layout

\begin_layout Standard
Il 
\series bold
sequence modeling
\series default
 risolve vari problemi in base all'architettura usata.
 Le architetture possibili sono:
\end_layout

\begin_layout Itemize

\series bold
uno a uno
\series default
: un input di dimensione fissa viene trasformato in un output di dimensione
 fissata (es.: image classification)
\end_layout

\begin_layout Itemize

\series bold
uno a molti
\series default
: output sequenziale (es.: captioning delle immagini che prende un'immafine
 in input e produce in output una sequenza di parole)
\end_layout

\begin_layout Itemize

\series bold
molti a uno
\series default
: input sequenziale (es.: analisi dei sentimenti dove una data frase viene
 classificata sulla base della positività o negatività dei sentimenti che
 esprime)
\end_layout

\begin_layout Itemize

\series bold
molti a molti
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
input e output sequenziali (es.: traduttori, cioè una RNN che legge una frase
 in inglese e ne produce in output la traduzione francese)
\end_layout

\begin_layout Itemize
input e output sequenziali sincronizzati (es.: classificazione dei video,
 dei quali vogliamo etichettare ogni frame)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename seq2seq_architectures.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architetture seq2seq
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Il modello seq2seq segue la classica 
\series bold
architettura encoder-decoder
\series default
, in cui durante la fase di inferenza (e non durante il training) il decoder
 dà in pasto l'output di ogni istante di tempo all'input successivo.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename enc-dec.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architettura Encoder-Decoder
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Il processo di 
\shape italic
training
\shape default
 è caratterizzato da una 
\series bold
trasformazione
\series default
 delle parole a identificativi e da una fase di 
\series bold
embedding
\series default
.
 Il processo di inferenza (testing) è caratterizzato anch'esso da una prima
 fase di 
\series bold
embedding
\series default
 e da una 
\series bold
trasformazione
\series default
 di identificativi in parole.
 La fase di embedding consiste nella rappresentazione delle parole usando
 un vettore di parole (vocabolario) denso.
 L'encoder può essere una RNN così come una CNN, ad esempio nel caso di
 image captioning.
\end_layout

\begin_layout Standard
\noindent
Vi sono inoltre dei 
\series bold
caratteri speciali
\series default
 che vengono dati in input al decoder:
\end_layout

\begin_layout Itemize
<PAD>: durante il training gli esempi sono dati in pasto alla rete in batch,
 i cui input devono essere della stessa larghezza.
 Questo carattere viene usato per ''gonfiare'' input più corti rendendoli
 della stessa dimensione del batch
\end_layout

\begin_layout Itemize
<EOS>: necessario per il batching nel decoder, ovvero indica al decoder
 dove finisce la frase e gli consente di indicare la stessa cosa nel suo
 output
\end_layout

\begin_layout Itemize
<UNK>: su dati reali può ampiamente migliorare l'efficienza delle risorse
 per ignorare le parole che non si presentano abbastanza spesso nel vocabolario
 (rimpiazzandole quindi con questo carattere)
\end_layout

\begin_layout Itemize
<SOS>/<GO>: questo è l'input del decoder nel primo istante di tempo per
 consentirgli di capire quando iniziare a generare l'output
\end_layout

\begin_layout Standard
La 
\series bold
preparazione dei batch
\series default
 si compone dei seguenti step:
\end_layout

\begin_layout Enumerate
prendere un campione di coppie <source_sequence, target_sequence> di dimensione
 batch_size
\end_layout

\begin_layout Enumerate
appendere <EOS> a source_sequence
\end_layout

\begin_layout Enumerate
prependere <SOS> a target_sequence per ottenere la target_input_sequence
 e appendere <EOS> per ottenere la target_output_sequence
\end_layout

\begin_layout Enumerate
''gonfiare'' le frasi fino alla max_input_length (o max_target_length) all'inter
no dello stesso batch usando il token <PAD>
\end_layout

\begin_layout Enumerate
codificare i token basandosi sul vocabolario (embedding)
\end_layout

\begin_layout Enumerate
sostituire i token OOV (out of vocabulary) con <UNK> e calcolare la lunghezza
 di ogni sequenza di input e target nel batch
\end_layout

\begin_layout Standard
Il 
\series bold
modello seq2seq
\series default
 ci dice quindi che, data una coppia <S,T>, legge S e restituisce in output
 T' che corrisponda a T.
 In pratica il problema riguarda la probabilità di ottenere una sequenza
 in output data una sequenza in input:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(y_{1},...,y_{T^{'}}|x_{1},...,x_{T})=\prod_{t=1}^{T^{'}}p(y_{t}|v,y_{1},...,y_{t-1})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove 
\emph on
v 
\emph default
è lo stato nascosto.
 Quindi, tale problema si riconduce alla 
\series bold
massimizzazione della crossentropy media
\series default
:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{1}{|S|}\sum_{(T,S)\in\mathcal{S}}\mathrm{log}p(T|S)
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Macchine neurali di Turing
\end_layout

\begin_layout Standard
Le 
\series bold
Neural Turing Machines
\series default
 combinano una RNN con una banca di memoria esterna, cioè un array di vettori.
 La rete principale scrive su e legge da questa memoria a ogni step.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename neural_turing_machine.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Neural Turing Machine
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
La sfida è quella di imparare cosa scrivere/leggere e dove scrivere/leggere.
 La soluzione è quella di, a ogni step, leggere e scrivere ovunque ma con
 diversa misura.
 Tale meccanismo è chiamato 
\series bold
attention mechanism
\series default
 e si basa sul porre l'
\series bold
attenzione
\series default
 su una specifica parte della memoria per produrre l'output.
 In pratica la RNN dà una 
\series bold
distribuzione di attenzione
\series default
 (dettata dalla softmax) che descrive come distribuiamo in memoria ciò che
 ci interessa (vedi figura sotto).
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename attention.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Attention mechanism in lettura
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Il risultato della lettura è una somma pesata differenziabile:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
r\leftarrow\sum_{i}a_{i}M_{i}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In fase di scrittura, invece, anzichè scrivere in una posizione specifica,
 scriviamo ovunque ma con diversa misura.
 La RNN dà una distribuzione di attenzione, descrivendo quanto dovremmo
 cambiare ogni posizione della memoria nella direzione del valore di scrittura.
 Sotto vi è una figura che spiega questo passaggio.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename attention_write.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Attention mechanism in scrittura
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
L'aggiornamento della memoria si basa sulla formula seguente:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
M_{i}\leftarrow a_{i}w+(1-a_{i})M_{i}
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Meccanismo di attenzione
\end_layout

\begin_layout Standard
\noindent
Vi sono due componenti principali in una Neural Turing Machine: l'
\series bold
attention mechanism
\series default
 e il 
\series bold
RNN controller
\series default
.
 L'architettura è inoltre divisa in due parti: 
\end_layout

\begin_layout Itemize

\series bold
content-based attention
\series default
: cerca in memoria e si concentra su posti che corrispondono a ciò che si
 sta cercando
\end_layout

\begin_layout Itemize

\series bold
location-based attention
\series default
: consente il movimento relativo nella memoria, abilitando la NTM a ciclare
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename ntm_attention.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Attention in una NMT
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Innanzitutto, il controller dà un 
\series bold
query vector
\series default
 (output della RNN, usato come meccanismo di indirizzamento) all'attention
 mechanism e a ogni entry in memoria viene assegnato un punteggio basandosi
 sulla similarità con il vettore.
 I punteggi vengono quindi convertiti in una 
\series bold
distribuzione
\series default
 di probabilità usando la softmax.
 Poi si 
\series bold
interpola
\series default
 il risultato della softmax con l'attention generata dallo step precedente,
 tenendo conto di un 
\series bold
interpolation amount
\series default
 che rappresenta la velocità di 
\shape italic
interpolazione
\shape default
, che è un parametro predetto dalla RNN (vedi figura 5.17: se è 0 si prende
 solo A, se è 1 si prende solo B).
 Fatto ciò, viene effettuata una convoluzione tra l'attenzione e un filtro
 di shift, che consente al controller di spostare il focus dell'attention
 mechanism su una particolare zona.
 Infine, si affina la 
\series bold
distribuzione dell'attention
\series default
, che viene poi data in pasto all'operazione di read o write.
\end_layout

\begin_layout Standard
\noindent
Consideriamo il seguente dataset sequenziale:
\begin_inset Formula $\left\{ \left(\left(x_{1},...,x_{n}\right),\left(y_{1},...,y_{n}\right)\right)\right\} _{i=1}^{N}$
\end_inset

.
\end_layout

\begin_layout Standard
\noindent
Il ruolo del decoder è quello di modellare la probabilità generativa 
\begin_inset Formula $P(y_{1},$
\end_inset

...,y
\begin_inset Formula $_{m}|x)$
\end_inset

.
 Nei modelli seq2seq 
\shape italic
''vanilla''
\shape default
 (convenzionali), il decoder viene condizionato inizializzando lo stato
 iniziale con l'ultimo stato dell'encoder.
 Ciò funziona bene per frasi medio-corte, mentre per frasi lunghe ciò diventa
 un 
\shape italic
bottleneck
\shape default
.
\end_layout

\begin_layout Standard
\noindent
La 
\series bold
funzione di attention
\series default
 mappa il query vector e l'insieme di coppie chiave-valore a un output,
 che è calcolato come la somma pesata dei valori, dove il peso assegnato
 a ogni valore è calcolato da una 
\series bold
funzione di compatibilità
\series default
.
 Tale funzione:
\end_layout

\begin_layout Enumerate
compara lo stato nascosto corrente 
\begin_inset Formula $h_{t},$
\end_inset

 con stati di origine h
\begin_inset Formula $_{s}$
\end_inset

 per derivare l'attention usando una funzione di scoring tra le due seguenti:
 
\begin_inset Formula 
\begin{equation}
score(\boldsymbol{h_{t}},\boldsymbol{\bar{h_{s}}})=\begin{cases}
\boldsymbol{h_{t}^{T}}\boldsymbol{W\bar{h_{s}}}\\
\boldsymbol{v_{a}^{T}}tanh\left(\boldsymbol{W_{1}h_{t}}+\boldsymbol{W_{2}\bar{h_{s}}}\right)
\end{cases}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
applica la softmax sugli score e calcola i pesi dell'attention, uno per
 ogni token dell'encoder, nel modo seguente:
\begin_inset Formula 
\begin{equation}
\alpha_{ts}=\frac{e^{score(\boldsymbol{h_{t}},\boldsymbol{\bar{h_{s}}})}}{\sum_{s'=1}^{S}e^{score(\boldsymbol{h_{t}},\boldsymbol{\bar{h_{s'}}})}}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
calcola il 
\series bold
context vector
\series default
 come media pesata degli stati di origine:
\begin_inset Formula 
\begin{equation}
\boldsymbol{c_{t}=}\sum_{s}\alpha_{ts}\boldsymbol{\bar{h_{s}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
combina il context vector con lo stato nascosto target corrente per produrre
 il vettore di attention finale:
\begin_inset Formula 
\begin{equation}
\boldsymbol{a_{i}=}f\left(c_{t},\boldsymbol{h_{i}}\right)=tanh\left(\boldsymbol{W_{c}}[\boldsymbol{c_{t}};\boldsymbol{h_{t}}]\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename attention_function.png
	lyxscale 70
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Esempio di funzione di compatibilità
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Per visualizzare i pesi dell'attention tra le frasi di 
\color red
source
\color inherit
 e 
\color blue
target 
\color inherit
si può usare la cosiddetta 
\series bold
alignment matrix
\series default
.
 Per ogni passo di decodifica (cioè per ogni token target generato) descrive
 quali sono i token di origine che sono più presenti nella somma pesata
 e che hanno quindi condizionato la decodifica.
 L'attention è quindi uno strumento che, durante la decodifica, consente
 alla rete di prestare più attenzione a varie parti della frase originale.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename attention_visual.png
	lyxscale 50
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Esempio di alignment matrix
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Tra le possibili applicazioni del meccanismo di attention, oltre alle traduzioni
, troviamo il riconoscimento vocale, l'image captioning e la generazione
 automatica di risposte (es.: chatbot).
\end_layout

\begin_layout Paragraph
Chatbot
\end_layout

\begin_layout Standard
I 
\series bold
chatbot
\series default
 possono essere definiti lungo due dimensioni:
\end_layout

\begin_layout Itemize
l'
\series bold
algoritmo core
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
generativo
\series default
: codifica la domanda in un context vector e genera la risposta parola per
 parola usando la distribuzione della probabilità condizionata sul vocabolario
 della risposta (modello encoder-decoder) 
\end_layout

\begin_layout Itemize

\series bold
retrieval
\series default
: si basa sulla conoscenza di base delle coppie domanda-risposta, cioè quando
 viene fatta una nuova domanda, la fase di inferenza la codifica in un context
 vector e recupera, usando misure di similarità, i top-k nearest neighbours
 basandosi sulla conoscenza di base posseduta
\end_layout

\end_deeper
\begin_layout Itemize
la 
\series bold
gestione del contesto
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
single-turn
\series default
: costruisce il vettore di input considerando la domanda in arrivo (può
 perdere informazioni importanti riguardanti la storia della conversazione
 e generare, quindi, risposte irrilevanti, ma è più facile da implementare
 e allenare)
\begin_inset Formula 
\[
\left\{ \left(\boldsymbol{q}_{i},\boldsymbol{a}_{i}\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
multi-turn
\series default
: costruisce il vettore di input considerando un contesto di conversazione
 ''multi-turn'', cioè basandosi sulla domanda in arrivo ma anche sulle domande
 precedenti
\begin_inset Formula 
\[
\left\{ \left(\left[\boldsymbol{q}_{i-2};\boldsymbol{a}_{i-2};\boldsymbol{q}_{i-1};\boldsymbol{a}_{i-1};\boldsymbol{q}_{i}\right],\boldsymbol{a}_{i}\right)\right\} 
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
I chatbot generativi, in particolare, usano una RNN e la trainano per mappare
 ciò che viene detto dalla prima persona a ciò che viene risposto dal bot
 basandosi sempre sull'attention mechanism.
 Tale meccanismo di risposta single-turn è stato esteso nel 2017 in un 
\series bold
attention mechanism gerarchico
\series default
.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename hier_gen_chatbot.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Chatbot gerarchico multi-turn generativo
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
In tale modello si usano le seguenti formule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{U}=\left(u_{1},...,u_{m}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{u}_{i}=\left(w_{i,1},...,w_{i,T_{i}}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{h}_{i,k}=concat\left(\overrightarrow{\boldsymbol{h}_{i,k}},...,\overleftarrow{\boldsymbol{h}_{i,k}}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{r}_{i,t}=\sum_{j=1}^{T_{i}}\boldsymbol{\alpha}_{i,t,j}\boldsymbol{h}_{i,j}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
e_{i,t,j}=\eta\left(\boldsymbol{s}_{t-1},\boldsymbol{1}_{i+1,t},\boldsymbol{h}_{i,j}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{\alpha}_{i,y,j}=\frac{exp\left(e_{i,t,j}\right)}{\sum_{k=1}^{T_{i}}exp\left(e_{i,t,j}\right)}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\left(\boldsymbol{1}_{i,t},...,\boldsymbol{1}_{m,t}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\boldsymbol{c}_{t}=\sum_{i=1}^{m}\beta_{i,t}\boldsymbol{1}_{i,t}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove 
\begin_inset Formula $\boldsymbol{\alpha}_{i,y,j}$
\end_inset

 è la 
\series bold
softmax
\series default
.
 Le reti di attenzione gerarchiche sono state usate anche per la classificazione
 degli argomenti (es.: dataset di Yahoo Answers), così come per l'analisi
 dei sentimenti.
 
\end_layout

\begin_layout Section
Trasformatore
\end_layout

\begin_layout Standard
Un modello 
\series bold
trasformatore
\series default
 è caratterizzato da uno stack di encoder che codificano la sequenza in
 input e da uno stack di decoder che producono l'output.
 Ogni encoder e decoder hanno una struttura interna che consente di non
 usare un modello ricorrente.
 Ciò consente di allenare il modello con una quantità molto vasta di parole.
 L'elemento più importante che li caratterizza è la self-attention, che
 presta attenzione a una parola basandosi sulle parole che sono connesse
 ad essa.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename enc-dec_transformer.png
	lyxscale 70
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Encoder e decoder di un transformer
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Andando più nel dettaglio, gli elementi che compongono un trasformatore
 sono:
\end_layout

\begin_layout Itemize

\series bold
dot-product attention scalata
\end_layout

\begin_layout Itemize

\series bold
multi-head attention
\end_layout

\begin_layout Itemize

\series bold
FFNN posizionale
\end_layout

\begin_layout Itemize

\series bold
embedding e softmax
\end_layout

\begin_layout Itemize

\series bold
codifica posizionale
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename transformer.png
	lyxscale 90

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architettura del Transformer
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
L'attention qui è definita come
\begin_inset Formula 
\begin{equation}
Attention(Q,K,V)=softmax\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In particolare, Q, K e V sono delle matrici che indicano rispettivamente
 l'encoding delle parole in input (
\series bold
query
\series default
), la trasformazione delle query in chiavi (
\series bold
key
\series default
) e il valore di queste parole trasformate in chiavi (
\series bold
value
\series default
).
 Nella dot-product 
\series bold
attention scalata
\series default
, a ogni step viene paragonato ogni valore di query 
\begin_inset Formula $q_{i}$
\end_inset

 con ogni altro valore chiave 
\begin_inset Formula $k_{j}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\noindent
Per quanto riguarda la multi-head attention, si cerca di calcolare la dot-produc
t attention più volte in parallelo e poi concatenare i risultati.
 
\end_layout

\begin_layout Standard
\noindent
Dopodichè si ha una 
\series bold
FFNN
\series default
 che, per ogni parola in input, calcola la 
\series bold
multi-head attention
\series default
 per quella parola e poi calcola una funzione non-lineare (sempre per la
 stessa parola):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
FFNN\left(x\right)=max\left(0,xW_{1}+b_{1}\right)W_{2}+b_{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Vengono quindi ripetute le stesse operazioni sul decoder in modo tale da
 ottenere in output, per ogni parola in input, la probabilità di generare
 la 'giusta' parola (ad esempio, se in input ho ''A B C'' e in output voglio
 avere ''D E F'', il decoder mi calcolerà qual è la probabilità di generare
 D al primo step, E al secondo e F al terzo).
\end_layout

\begin_layout Standard
\noindent
Infine, per tenere traccia della posizione delle parole in input si aggiunge
 un 
\series bold
encoding posizionale
\series default
, che assegna una sorta di timestamp alle parole.
 Ci sono due tipi di encoding posizionale, cioè il 
\series bold
learned positional embedding
\series default
 e la 
\series bold
sinusoide
\series default
.
 Quest'ultima può essere:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
PE_{(pos,2i)}=sin\left(\frac{pos}{1000^{2i/d_{model}}}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
oppure:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
PE_{(pos,2i+1)}=cos\left(\frac{pos}{1000^{2i/d_{model}}}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Word embedding
\end_layout

\begin_layout Standard
La performance di un'applicazione nel mondo reale (es.: chatbot, classificatori
 di documenti, sistemi di recupero delle informazioni) dipende dalla codifica
 dell'input, che può essere basata su:
\end_layout

\begin_layout Itemize
rappresentazioni locali
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
N-gram
\end_layout

\begin_layout Itemize

\series bold
bag-of-words
\end_layout

\begin_layout Itemize

\series bold
codifica 1-di-N
\series default
 
\end_layout

\end_deeper
\begin_layout Itemize
rappresentazioni continue
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Latent Semantic Analysis
\end_layout

\begin_layout Itemize

\series bold
Latent Dirichlet Allocation
\end_layout

\begin_layout Itemize

\series bold
rappresentazioni distribuite
\end_layout

\end_deeper
\begin_layout Standard
In particolare nella 
\series bold
rappresentazione N-gram
\series default
 si cerca di determinare la probabilità 
\begin_inset Formula $P(s=w_{1},...,w_{k})$
\end_inset

 di una frase per capire qual è il modello sottostante di un certo documento.
 Questo viene ottenuto tramite il calcolo della probabilità congiunta
\begin_inset Formula 
\begin{equation}
P(s_{k})=\prod_{i}^{k}P(w_{i}|w_{1},...,w_{i-1})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Dal momento che nei modelli di lingua tradizionali n-gram ''
\shape italic
la probabilità di una parola dipende solo dal contesto delle n-1 parole
 precedenti
\shape default
'', la probabilità diventa
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{P}(s_{k})=\prod_{i}^{k}P(w_{i}|w_{i-n+1},...,w_{i-1})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Un processo tipico di apprendimento 
\shape italic
ML-smoothing
\shape default
 effettua una levigazione della probabilità per evitare di avere probabilità
 pari a 0.
 Per fare ciò calcola la probabilità
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{P}(w_{i}|w_{i-n+1},...,w_{i-1})=\frac{\#w_{i-n+1},...,w_{i-1},w_{i}}{\#w_{i-n+1},...,w_{i-1}}
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Neural Net Language Model
\end_layout

\begin_layout Standard
\noindent
Si definisce 
\series bold
embedding
\series default
 una tecnica che mappa una parola (o frase) dal suo spazio di input alto-dimensi
onale (il corpo di tutte le parole) a uno spazio vettoriale numerico di
 dimensione inferiore.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename embedding_esempi.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Esempi di embedding
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Ogni parola unica 
\series bold
\emph on
w
\series default
\emph default
 in un vocabolario V (con dimensione tipicamente nell'ordine di
\begin_inset Formula $10^{6}$
\end_inset

) viene quindi mappata in uno spazio m-dimensionale continuo (con dimensione
 tipicamente compresa nel range 100 < m < 500).
 Un modello tipico per il 
\shape italic
word embedding
\shape default
 è il
\series bold
 Neural Net Language Model
\series default
.
 Per ogni sequenza di training, si ha che l'input è costituito da una coppia
 <contesto, target> fatta così: <
\begin_inset Formula $w_{t-n+1}...w_{t-1},w_{t}$
\end_inset

>.
 L'obiettivo, inoltre, è quello di minimizzare la funzione di errore
\begin_inset Formula 
\begin{equation}
E=-log\hat{P}\left(w_{t}|w_{t-n+1}...w_{t-1}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename neural_net_language_model.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Neural Net Language Model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Il 
\series bold
layer di proiezione
\series default
 contiene i vettori in 
\begin_inset Formula $C_{|V|,m}$
\end_inset

.
 La softmax è usata per produrre in output una 
\series bold
distribuzione multinomiale
\series default
 con probabilità
\begin_inset Formula 
\begin{equation}
\hat{P}(w_{i}=w_{t}|w_{t-n+1}...w_{t-1})=\frac{e^{y_{w_{i}}}}{\sum_{i'}^{|V|}e^{y_{w_{i'}}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
dove:
\end_layout

\begin_layout Itemize
\begin_inset Formula $y=b+U\cdot tanh(d+Hx)$
\end_inset


\end_layout

\begin_layout Itemize

\emph on
x
\emph default
 è la concatenazione 
\emph on
C(w) 
\emph default
del contesto dei vettori dei pesi
\end_layout

\begin_layout Itemize

\emph on
d
\emph default
 e 
\emph on
b
\emph default
 sono dei bias, rispettivamente degli elementi 
\emph on
h
\emph default
 e 
\emph on
|V|
\end_layout

\begin_layout Itemize

\emph on
U 
\emph default
è la matrice di dimensione 
\begin_inset Formula $|V|\times h$
\end_inset

 dei pesi tra il layer nascosto e il layer di output
\end_layout

\begin_layout Itemize

\emph on
H
\emph default
 è la matrice di dimensione 
\begin_inset Formula $h\times(n-1)\cdot m$
\end_inset

 dei pesi tra il layer di proiezione e il layer nascosto
\end_layout

\begin_layout Paragraph
Word2vec di Google
\end_layout

\begin_layout Standard
L'idea è quella di ottenere una performance che consente il training di
 un modello meno profondo su una quantità di dati più grande.
 In questo modello non ci sono layer nascosti, il layer di proiezione è
 condiviso (non solo la matrice dei pesi) e il contesto contiene le parole
 provenienti sia dalla storia che dal futuro.
\end_layout

\begin_layout Standard
\noindent
Le due architetture possibili sono l'
\series bold
architettura skip-gram
\series default
 e l'
\series bold
architettura bag-of-words continua
\series default
 (vedi figura sotto).
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename word2vec.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architettura skip-gram (a sinistra) e architettura bag-of-words (a destra)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
In particolare, nell'architettura bag-of-words continua (
\series bold
CBOW
\series default
), per ogni sequenza di training, si ha che l'input è costituito da una
 coppia <contesto, target> fatta così: <
\begin_inset Formula $w_{t-n+1}...w_{t-1}w_{t+1}...w_{t+\frac{n}{2}},w_{t}$
\end_inset

>.
 L'obiettivo, inoltre, è quello di minimizzare la funzione di errore
\begin_inset Formula 
\begin{equation}
E=-log\hat{P}\left(w_{t}|w_{t-\frac{n}{2}}...w_{t-1}w_{t+1}...w_{t+\frac{n}{2}}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename cbow.png
	lyxscale 125
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
CBOW
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
In pratica, per ogni coppia <contesto, target> solo le parole di contesto
 vengono aggiornate.
 Se la probabilità 
\begin_inset Formula $\hat{P}(w_{i}=w_{t}|context)$
\end_inset

 è sovrastimata, una certa porzione di 
\begin_inset Formula $C'(w_{i})$
\end_inset

 viene sottratta dai vettori delle parole contestuali in 
\begin_inset Formula $C_{|V|,m}$
\end_inset

; se invece è sottostimata, la porzione viene aggiunta ai vettori delle
 parole contestuali in 
\begin_inset Formula $C_{|V|,m}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\noindent
Applicazioni di 
\series bold
word2vec
\series default
 riguardano la classificazione di documenti e l'analisi sentimentale.
\end_layout

\begin_layout Paragraph
GloVe
\end_layout

\begin_layout Standard

\series bold
GloVe
\series default
 fa esplicitamente ciò che word2vec fa implicitamente, ovvero codifica il
 significato come 
\series bold
vettore di offset
\series default
 in uno spazio di embedding.
 Tale significato viene codificato da dei rapporti di probabilità di co-occorren
za.
 L'allenamento avviene con una 
\shape italic
least squares pesata
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J=\sum_{i,j=1}^{V}f\left(X_{ij}\right)\left(w_{i}^{T}\tilde{w_{j}}+b_{i}+\tilde{b_{j}}-logX_{ij}\right)^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In pratica questo metodo si basa sull'individuazione dei 
\series bold
nearest neighbours
\series default
 di una certa parola.
\end_layout

\begin_layout Chapter
Autoencoder
\end_layout

\begin_layout Section
Struttura
\end_layout

\begin_layout Standard
Un' 
\series bold
autoencoder
\series default
 è un tipo di rete neurale che nella sua formulazione più semplice cerca
 di apprendere la 
\series bold
funzione identità
\series default
.
 In altre parole impara a produrre una copia dell'input.
 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Autoencoder_schema.png
	scale 110

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Schema generale di un autoencoder
\end_layout

\end_inset


\end_layout

\end_inset

È composto principalmente da due parti.
\end_layout

\begin_layout Enumerate

\series bold
Encoder
\series default
, la cui funzione è mappare l'input in una rappresentazione latente di dimension
e minore ed eventualmente con altri vincoli.
 
\end_layout

\begin_layout Enumerate

\series bold
Decoder
\series default
 che mappa la rappresentazione latente nell'output.
\end_layout

\begin_layout Standard
Formalmente possiamo descrivere un autoencoder attraverso tre funzioni matematic
he:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\displaystyle \phi:{\mathcal{X}}\rightarrow{\mathcal{F}}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\displaystyle \psi:{\mathcal{F}}\rightarrow{\mathcal{X}}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
{\displaystyle \phi,\psi={\underset{\phi,\psi}{\operatorname{arg\,min}}}\,\|X-(\psi\circ\phi)X\|^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $X\in\mathbb{R}^{d}$
\end_inset

 e 
\begin_inset Formula $F\in\mathbb{R}^{n}$
\end_inset

 con 
\begin_inset Formula $n\ll d$
\end_inset

.
 Le features 
\begin_inset Formula $F$
\end_inset

 vengono solitamente chiamate 
\series bold
rappresentazione latente
\series default
.
 Lo schema di un modello di autoencoder è abbastanza generale ed è quindi
 implementabile tramite varie architetture come 
\shape italic
feedforward, convoluzionali, ricorrenti 
\shape default
e con varie profondità della rete, andando incontro al ben noto 
\shape italic
trade-off
\shape default
 tra capacità di approssimazione e sovradattamento.

\shape italic
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename autoencoder.png
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Autoencoder feedforward
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Utilizzo delle reti autoencoder
\end_layout

\begin_layout Subsection
Riduzione dimensionale
\end_layout

\begin_layout Standard
Una rete autoencoder è usata principalmente come algoritmo di 
\series bold
riduzione dimensionale
\series default
 (o compressione) con determinate proprietà:
\end_layout

\begin_layout Itemize

\series bold
Data specificità
\series default
: l'autoencoder è adatta a comprimere solo dati simili a quelli su cui è
 allenata, o addirittura solo questi ultimi.
 Apprende features specifiche, quindi un' autoencoder allenata a comprimere
 immagini di caratteri scritti a mano non sarà adatta a comprimere foto
 generiche.
\end_layout

\begin_layout Itemize

\series bold
Lossy
\series default
: l'output non sarà esattamente identico all'input, si ha quindi una degradazion
e.
\end_layout

\begin_layout Itemize

\series bold
Self-supervized
\series default
.
\end_layout

\begin_layout Standard
Una caratteristica spesso cercata nella rappresentazione latente è la 
\series bold
sparsità.
 
\series default
Questa proprietà può essere raggiunta aggiungendo un termine di penalità
 proporzionale alla rappresentazione latente.
 Tipicamente viene utilizzata la norma L1.
 Riprendendo la notazione formale precedente la loss function diventa:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L=\|X-(\psi\circ\phi)X\|^{2}+\lambda\mathop{\sum_{i}|f_{i}|}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Minimizzando il secondo termine imponiamo che la rappresentazione latente
 abbia la somma dei valori assoluti delle singole componenti minima ma contempor
aneamente minimizzando il primo termine imponiamo che la rappresentazione
 latente sia in grado di separare significativamente le features.
 Idealmente, se non considerirassimo la dimensionalità, una rappresentazione
 efficacie a minimizzare questa loss function sarebbe la 
\shape italic
one-hot encode.

\shape default
 Questa considerazione suggerisce che la rete tenderà comunque ad una rappresent
azione in cui la maggior parte delle componenti 
\begin_inset Formula $f_{i}$
\end_inset

 siano tendenti a zero e solo il minimo necessario di componenti siano con
 modulo significativamente diversi da zero.
\end_layout

\begin_layout Subsection
Eliminazione del rumore
\end_layout

\begin_layout Standard
È possibile usare le reti autoencoder per rimuovere e/o attenuare il rumore
 nei dati.
 Per far ciò, durante il training, ai dati di input viene aggiunto del 
\series bold
rumore
\series default
 e la rete tenterà di ricostruire l'input originale.
 Oltre a poter utilizzare l'autoencoder per rimuovere il rumore dai dati,
 aggiungere del rumore in input permette alla rete di apprendere una 
\series bold
rappresentazione latente
\series default
 più robusta, in quanto è forzata ad estrarre features idealmente indipendenti
 dalle fluttuazioni dovute al rumore.
 Questo può migliorare le prestazioni anche negli altri casi d'uso.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename denoising_autoencoder.png
	scale 12

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Schema denoising autoencoder
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Inizializzatore di pesi
\end_layout

\begin_layout Standard
Gli autoencoder possono essere utilizzate per 
\series bold
inizializzare
\series default
 reti neurali con altri compiti, come per esempio un classificatore.
 In particolare quando si hanno pochi dati già classificati per il training.
 Per far questo si procede in questo modo:
\end_layout

\begin_layout Enumerate
Si progetta la rete per classificare normalmente.
 Ricordando la struttura delle reti CNN (senza perdere generalità) per la
 classificazione ricordiamo che essa è strutturata in due parti.
 La prima parte è incaricata di estrarre le features mentre la seconda usa
 le features per classificare effettivamente.
 Sostituiamo la seconda parte della rete con il simmetrico della prima parte.
 In questo modo abbiamo una struttura che produce un output della stessa
 dimensione dell'input.
 Di fatto abbiamo ottenuto una rete autoencoder.
 Alleniamo l'autoencoder ottenuto con i dati privi di etichette.
\end_layout

\begin_layout Enumerate
Scartiamo il decoder e manteniamo l'encoder con i pesi ottenuti dall'allenamento.
\end_layout

\begin_layout Enumerate
Connettiamo la parte precendentemente sostituita dal decoder (classificatore).
\end_layout

\begin_layout Enumerate
Usiamo la tecnica del fine tuning per completare l'allenamento con i pochi
 dati etichettati.
\end_layout

\begin_layout Standard
Le reti autoencoder forniscono una buona inizializzazione, riducendo anche
 il rischio di overfitting, perchè dovendo approssimare la funzione identità
 apprendono una rappresentazione latente su dati simili ma non uguali a
 quelli effettivamente usati nel training.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename classificatore_autoencoder.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Classificatore inizializzato con autoencoder
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Autoencoder generativi
\end_layout

\begin_layout Standard
I 
\series bold
Generative Models
\series default
 o modelli generativi permettono di creare dati come foto, film, musica,
 testi e usarli negli ambiti più diversi come 
\series bold
data augmentation
\series default
, simulazioni e pianificazioni.
 È possibile tecnicamente usare autoencoder per generare nuovi dati, ma
 non senza qualche criticità.
 Un opzione per generare nuovi dati potrebbe essere la seguente:
\end_layout

\begin_layout Enumerate
Alleniamo l'autoencoder su un insieme di dati 
\begin_inset Formula $X$
\end_inset

 in modo da costruire una rappresentazione latente 
\begin_inset Formula $F$
\end_inset

.
\end_layout

\begin_layout Enumerate
Scartiamo l'encoder
\end_layout

\begin_layout Enumerate
Generiamo dei vettori casuali nello spazio 
\begin_inset Formula $F$
\end_inset

 e li usiamo come input del decoder che produrra in output un nuovo dato.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Autoencoder_sampling.png
	scale 45

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Generazione con autoencoder
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Mentre i punti 1 e 2 non creano eccessivi problemi, il punto 3 è critico.
 Non sappiamo nulla a priori della distibuzione che assume lo spazio latente
 
\begin_inset Formula $F$
\end_inset

 quindi non siamo in grado di generare con alta probabilità dei vettori
 di input per il decoder che producano un output sensato.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename autoencoder_generativo.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Spazio latente irregolare
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Osservando l'esempio qui sopra vediamo una rete autoencoder allenata a rappresen
tare figure geometriche chiuse.
 Nel momento in cui diamo in input un vettore casuale (punto viola) notiamo
 che il decoder genera una figura senza significato per il contesto.
 Questo avviene perchè non campioniamo con la funzione di probabilità costruita
 dalla rete.
 Questa distribuzione (
\begin_inset Formula $\phi_{f}$
\end_inset

) è estremamente difficile da stimare a priori e devono essere applicate
 tecniche sofisticate come i 
\series bold
variational autoencoder
\series default
.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"
literal "false"

\end_inset

Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L.
 (2017).
 The Expressive Power of Neural Networks: A View from the Width.
 Neural Information Processing Systems, 6231-6239.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"
literal "false"

\end_inset

Slides delle lezioni
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"
literal "false"

\end_inset

Appunti di PoliMI Data Scientists del corso di Soft Computing
\end_layout

\end_body
\end_document
